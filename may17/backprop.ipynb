{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X --> y1 --> y\n",
    "\n",
    "\n",
    "# y1 = w01*x + w00 -> X1*[w01 w00].T\n",
    "# y = w11*y1 + w10 -> \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def insert1(X):\n",
    "    m, _ = X.shape\n",
    "    _1 = np.ones((m,1))\n",
    "    return np.concatenate((X, _1), axis=1)\n",
    "# def activation():\n",
    "    # if X > 0:\n",
    "    #     return X\n",
    "    # return 0\n",
    "def predict(X, W):\n",
    "    w1, w2 = W[0], W[1]\n",
    "    X1 = insert1(X)\n",
    "    y1 = insert1(X1.dot(w1.T).reshape(-1, 1))\n",
    "    # y1 = activation(y1)\n",
    "    yp = y1.dot(w2.T)\n",
    "    return yp\n",
    "\n",
    "def loss(X, y, W): \n",
    "    yp = predict(X, W)\n",
    "    sqdiff = (y - yp)**2\n",
    "    return np.mean(sqdiff)\n",
    "\n",
    "EP = 1e-8\n",
    "\n",
    "def incrL(f, X, y, W, i, j):\n",
    "    epm = np.zeros(W.shape)\n",
    "    epm[i][j] = EP\n",
    "    W1 = W + epm\n",
    "    l1 = f(X, y,W1)\n",
    "    return l1\n",
    "\n",
    "def diff(f, X, y, W):\n",
    "    R, C = W.shape\n",
    "    gradients = np.zeros(W.shape)\n",
    "    loss = f(X, y, W)\n",
    "    for i in range(R):\n",
    "        for j in range(C):\n",
    "            newloss = incrL(f, X, y, W, i, j)\n",
    "            g = (newloss - loss)/EP\n",
    "            gradients[i][j] = g\n",
    "    return gradients\n",
    "\n",
    "def fit(X, y, X_test, y_test, epochs=10, eta=0.001, W=None):\n",
    "    if W is None:\n",
    "        W = np.random.random((2, 2))\n",
    "        W[:, 1] = 0\n",
    "    loss_train = loss(X, y, W)\n",
    "    loss_test = loss(X_test, y_test, W)\n",
    "    print(f\" - Loss_train: {loss_train}, Loss_test: {loss_test}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        grads = diff(loss, X, y, W)\n",
    "        W = W - eta * grads\n",
    "        loss_train = loss(X, y, W)\n",
    "        loss_test = loss(X_test, y_test, W)\n",
    "        print(f\"{epoch} - Loss_train: {loss_train}, Loss_test: {loss_test}\")\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64031bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-91],\n",
       "        [ 25],\n",
       "        [-85],\n",
       "        [-36],\n",
       "        [ 13]]),\n",
       " array([-905.24880388,  255.00149299, -844.72479665, -354.69806286,\n",
       "         134.73946929]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generate_data(m, seed=10):\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.randint(-100, 100, size=m)\n",
    "    y  = 10 * x + 5 + (0.5 - np.random.random(m))\n",
    "    return x.reshape(-1, 1), y\n",
    "generate_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee72b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Loss_train: 0.08375882825301191, Loss_test: 0.0817691299827\n",
      "0 - Loss_train: 0.0837584422231939, Loss_test: 0.08176927661699641\n",
      "1 - Loss_train: 0.08375805639455063, Loss_test: 0.08176942330460561\n",
      "2 - Loss_train: 0.08375767076623644, Loss_test: 0.0817695700834703\n",
      "3 - Loss_train: 0.08375728533833175, Loss_test: 0.08176971689156495\n",
      "4 - Loss_train: 0.0837569001108802, Loss_test: 0.08176986377978263\n",
      "5 - Loss_train: 0.08375651508442147, Loss_test: 0.0817700107374031\n",
      "6 - Loss_train: 0.08375613025749938, Loss_test: 0.08177015773143267\n",
      "7 - Loss_train: 0.08375574563119606, Loss_test: 0.08177030481829048\n",
      "8 - Loss_train: 0.08375536120439585, Loss_test: 0.08177045193929175\n",
      "9 - Loss_train: 0.08375497697755102, Loss_test: 0.08177059914553618\n",
      "10 - Loss_train: 0.08375459295104362, Loss_test: 0.08177074639401548\n",
      "11 - Loss_train: 0.08375420912421049, Loss_test: 0.08177089373299755\n",
      "12 - Loss_train: 0.08375382549692705, Loss_test: 0.08177104111162822\n",
      "13 - Loss_train: 0.08375344206874731, Loss_test: 0.08177118854141781\n",
      "14 - Loss_train: 0.08375305883990809, Loss_test: 0.08177133608533974\n",
      "15 - Loss_train: 0.08375267581029681, Loss_test: 0.0817714836311935\n",
      "16 - Loss_train: 0.08375229297990877, Loss_test: 0.08177163126886507\n",
      "17 - Loss_train: 0.08375191034849275, Loss_test: 0.08177177896151677\n",
      "18 - Loss_train: 0.08375152791668063, Loss_test: 0.08177192672988669\n",
      "19 - Loss_train: 0.08375114568326622, Loss_test: 0.08177207455058186\n",
      "20 - Loss_train: 0.08375076364874535, Loss_test: 0.08177222243232686\n",
      "21 - Loss_train: 0.08375038181325009, Loss_test: 0.08177237037784507\n",
      "22 - Loss_train: 0.0837500001757318, Loss_test: 0.08177251836595538\n",
      "23 - Loss_train: 0.08374961873748529, Loss_test: 0.08177266645448335\n",
      "24 - Loss_train: 0.08374923749732932, Loss_test: 0.08177281456541426\n",
      "25 - Loss_train: 0.08374885645534057, Loss_test: 0.08177296275506989\n",
      "26 - Loss_train: 0.08374847561116705, Loss_test: 0.08177311101755264\n",
      "27 - Loss_train: 0.0837480949650326, Loss_test: 0.0817732593357271\n",
      "28 - Loss_train: 0.08374771451722492, Loss_test: 0.08177340768496814\n",
      "29 - Loss_train: 0.08374733426672142, Loss_test: 0.08177355614907031\n",
      "30 - Loss_train: 0.08374695421448812, Loss_test: 0.08177370462545952\n",
      "31 - Loss_train: 0.08374657435932575, Loss_test: 0.08177385318535504\n",
      "32 - Loss_train: 0.08374619470170264, Loss_test: 0.08177400179783875\n",
      "33 - Loss_train: 0.08374581524174396, Loss_test: 0.08177415048186498\n",
      "34 - Loss_train: 0.08374543597902998, Loss_test: 0.08177429922565577\n",
      "35 - Loss_train: 0.08374505691354633, Loss_test: 0.08177444801893488\n",
      "36 - Loss_train: 0.08374467804577519, Loss_test: 0.0817745968878269\n",
      "37 - Loss_train: 0.08374429937503654, Loss_test: 0.08177474580658624\n",
      "38 - Loss_train: 0.08374392090064578, Loss_test: 0.08177489479245356\n",
      "39 - Loss_train: 0.0837435426230827, Loss_test: 0.08177504383361461\n",
      "40 - Loss_train: 0.08374316454261817, Loss_test: 0.08177519293426375\n",
      "41 - Loss_train: 0.08374278665840723, Loss_test: 0.08177534210187139\n",
      "42 - Loss_train: 0.0837424089712406, Loss_test: 0.08177549131752081\n",
      "43 - Loss_train: 0.0837420314799341, Loss_test: 0.08177564060526138\n",
      "44 - Loss_train: 0.08374165418483788, Loss_test: 0.08177578995303585\n",
      "45 - Loss_train: 0.08374127708646824, Loss_test: 0.08177593936973152\n",
      "46 - Loss_train: 0.08374090018371463, Loss_test: 0.0817760888247385\n",
      "47 - Loss_train: 0.08374052347735662, Loss_test: 0.08177623834448075\n",
      "48 - Loss_train: 0.08374014696693514, Loss_test: 0.08177638793774511\n",
      "49 - Loss_train: 0.08373977065245515, Loss_test: 0.08177653758226328\n",
      "50 - Loss_train: 0.08373939453332764, Loss_test: 0.081776687278596\n",
      "51 - Loss_train: 0.08373901860963205, Loss_test: 0.08177683704968923\n",
      "52 - Loss_train: 0.08373864288153132, Loss_test: 0.08177698686361165\n",
      "53 - Loss_train: 0.08373826734876696, Loss_test: 0.08177713675512387\n",
      "54 - Loss_train: 0.0837378920116796, Loss_test: 0.08177728669106772\n",
      "55 - Loss_train: 0.08373751686976867, Loss_test: 0.0817774367003275\n",
      "56 - Loss_train: 0.0837371419224909, Loss_test: 0.08177758676824157\n",
      "57 - Loss_train: 0.08373676717015387, Loss_test: 0.08177773688295781\n",
      "58 - Loss_train: 0.08373639261315122, Loss_test: 0.08177788706739292\n",
      "59 - Loss_train: 0.08373601825064514, Loss_test: 0.08177803729077397\n",
      "60 - Loss_train: 0.08373564408267671, Loss_test: 0.08177818760860683\n",
      "61 - Loss_train: 0.08373527010929878, Loss_test: 0.08177833795933306\n",
      "62 - Loss_train: 0.08373489633045857, Loss_test: 0.08177848837982309\n",
      "63 - Loss_train: 0.08373452274600665, Loss_test: 0.08177863884610641\n",
      "64 - Loss_train: 0.08373414935577969, Loss_test: 0.08177878937249906\n",
      "65 - Loss_train: 0.08373377615959728, Loss_test: 0.08177893997804091\n",
      "66 - Loss_train: 0.08373340315761724, Loss_test: 0.0817790906330984\n",
      "67 - Loss_train: 0.0837330303495605, Loss_test: 0.08177924133005086\n",
      "68 - Loss_train: 0.08373265773547436, Loss_test: 0.08177939211902138\n",
      "69 - Loss_train: 0.08373228531521108, Loss_test: 0.08177954293045152\n",
      "70 - Loss_train: 0.0837319130884742, Loss_test: 0.08177969383730199\n",
      "71 - Loss_train: 0.08373154105580058, Loss_test: 0.0817798447598352\n",
      "72 - Loss_train: 0.08373116921618537, Loss_test: 0.0817799957735525\n",
      "73 - Loss_train: 0.08373079756982904, Loss_test: 0.0817801468243469\n",
      "74 - Loss_train: 0.0837304261168441, Loss_test: 0.08178029795501499\n",
      "75 - Loss_train: 0.08373005485696546, Loss_test: 0.08178044913410996\n",
      "76 - Loss_train: 0.08372968379024252, Loss_test: 0.08178060036360282\n",
      "77 - Loss_train: 0.08372931291652141, Loss_test: 0.08178075166848651\n",
      "78 - Loss_train: 0.08372894223604076, Loss_test: 0.08178090300980496\n",
      "79 - Loss_train: 0.0837285717478768, Loss_test: 0.08178105442420085\n",
      "80 - Loss_train: 0.08372820145231336, Loss_test: 0.0817812058818762\n",
      "81 - Loss_train: 0.08372783134944527, Loss_test: 0.08178135741886146\n",
      "82 - Loss_train: 0.0837274614390209, Loss_test: 0.08178150899329197\n",
      "83 - Loss_train: 0.08372709172097582, Loss_test: 0.08178166063951126\n",
      "84 - Loss_train: 0.08372672219512271, Loss_test: 0.08178181234282618\n",
      "85 - Loss_train: 0.08372635286145483, Loss_test: 0.0817819640634686\n",
      "86 - Loss_train: 0.0837259837199218, Loss_test: 0.08178211589641739\n",
      "87 - Loss_train: 0.08372561477081772, Loss_test: 0.08178226777212048\n",
      "88 - Loss_train: 0.08372524601345156, Loss_test: 0.08178241970096303\n",
      "89 - Loss_train: 0.08372487744741469, Loss_test: 0.08178257166755573\n",
      "90 - Loss_train: 0.08372450907300896, Loss_test: 0.08178272372667277\n",
      "91 - Loss_train: 0.08372414089032637, Loss_test: 0.08178287581831142\n",
      "92 - Loss_train: 0.08372377289920967, Loss_test: 0.08178302797925911\n",
      "93 - Loss_train: 0.08372340509982104, Loss_test: 0.0817831802059768\n",
      "94 - Loss_train: 0.08372303749134041, Loss_test: 0.08178333245933554\n",
      "95 - Loss_train: 0.08372267007398261, Loss_test: 0.08178348478065134\n",
      "96 - Loss_train: 0.08372230284766027, Loss_test: 0.08178363717722593\n",
      "97 - Loss_train: 0.08372193581271953, Loss_test: 0.0817837895988109\n",
      "98 - Loss_train: 0.0837215689682173, Loss_test: 0.08178394210024145\n",
      "99 - Loss_train: 0.08372120231498836, Loss_test: 0.08178409465836652\n",
      "100 - Loss_train: 0.0837208358519931, Loss_test: 0.08178424727226435\n",
      "101 - Loss_train: 0.08372046957962298, Loss_test: 0.08178439993317936\n",
      "102 - Loss_train: 0.08372010349757113, Loss_test: 0.08178455264551865\n",
      "103 - Loss_train: 0.08371973760594187, Loss_test: 0.08178470544746051\n",
      "104 - Loss_train: 0.083719371904773, Loss_test: 0.08178485826536408\n",
      "105 - Loss_train: 0.08371900639412619, Loss_test: 0.08178501115089863\n",
      "106 - Loss_train: 0.08371864107312925, Loss_test: 0.08178516411042333\n",
      "107 - Loss_train: 0.08371827594220521, Loss_test: 0.08178531710756286\n",
      "108 - Loss_train: 0.08371791100109423, Loss_test: 0.08178547016980145\n",
      "109 - Loss_train: 0.08371754625013283, Loss_test: 0.08178562328199918\n",
      "110 - Loss_train: 0.08371718168890468, Loss_test: 0.08178577644291378\n",
      "111 - Loss_train: 0.0837168173167246, Loss_test: 0.08178592969043236\n",
      "112 - Loss_train: 0.08371645313463336, Loss_test: 0.08178608295111622\n",
      "113 - Loss_train: 0.08371608914148636, Loss_test: 0.08178623631273658\n",
      "114 - Loss_train: 0.08371572533782073, Loss_test: 0.0817863896894522\n",
      "115 - Loss_train: 0.08371536172311621, Loss_test: 0.08178654314792709\n",
      "116 - Loss_train: 0.08371499829749172, Loss_test: 0.08178669664421676\n",
      "117 - Loss_train: 0.08371463506137006, Loss_test: 0.08178685020098175\n",
      "118 - Loss_train: 0.08371427201372282, Loss_test: 0.0817870038226256\n",
      "119 - Loss_train: 0.08371390915483722, Loss_test: 0.08178715749468493\n",
      "120 - Loss_train: 0.08371354648511345, Loss_test: 0.08178731121520463\n",
      "121 - Loss_train: 0.08371318400334488, Loss_test: 0.08178746499348487\n",
      "122 - Loss_train: 0.0837128217106924, Loss_test: 0.08178761884488765\n",
      "123 - Loss_train: 0.08371245960665258, Loss_test: 0.08178777272113208\n",
      "124 - Loss_train: 0.08371209769082648, Loss_test: 0.08178792667175948\n",
      "125 - Loss_train: 0.08371173596282305, Loss_test: 0.08178808068170502\n",
      "126 - Loss_train: 0.08371137442339302, Loss_test: 0.08178823472096176\n",
      "127 - Loss_train: 0.08371101307192323, Loss_test: 0.0817883888429663\n",
      "128 - Loss_train: 0.08371065190783446, Loss_test: 0.08178854300522521\n",
      "129 - Loss_train: 0.08371029093153265, Loss_test: 0.08178869722568609\n",
      "130 - Loss_train: 0.08370993014328672, Loss_test: 0.08178885149946546\n",
      "131 - Loss_train: 0.08370956954235778, Loss_test: 0.08178900582205761\n",
      "132 - Loss_train: 0.08370920912962236, Loss_test: 0.08178916021907369\n",
      "133 - Loss_train: 0.08370884890372789, Loss_test: 0.08178931463870164\n",
      "134 - Loss_train: 0.08370848886501225, Loss_test: 0.08178946914143494\n",
      "135 - Loss_train: 0.08370812901348358, Loss_test: 0.08178962367947844\n",
      "136 - Loss_train: 0.08370776934986368, Loss_test: 0.08178977829527932\n",
      "137 - Loss_train: 0.08370740987271988, Loss_test: 0.0817899329362028\n",
      "138 - Loss_train: 0.08370705058265848, Loss_test: 0.08179008766811414\n",
      "139 - Loss_train: 0.08370669147974649, Loss_test: 0.08179024241027037\n",
      "140 - Loss_train: 0.08370633256326403, Loss_test: 0.0817903972328666\n",
      "141 - Loss_train: 0.08370597383331137, Loss_test: 0.08179055210633371\n",
      "142 - Loss_train: 0.0837056152904687, Loss_test: 0.0817907070219242\n",
      "143 - Loss_train: 0.0837052569340518, Loss_test: 0.08179086200464834\n",
      "144 - Loss_train: 0.08370489876346254, Loss_test: 0.0817910170271866\n",
      "145 - Loss_train: 0.08370454077919302, Loss_test: 0.081791172122424\n",
      "146 - Loss_train: 0.08370418298172319, Loss_test: 0.08179132726033114\n",
      "147 - Loss_train: 0.08370382536981835, Loss_test: 0.08179148245711013\n",
      "148 - Loss_train: 0.08370346794369134, Loss_test: 0.08179163769267049\n",
      "149 - Loss_train: 0.08370311070356522, Loss_test: 0.08179179299790493\n",
      "150 - Loss_train: 0.08370275364919863, Loss_test: 0.08179194835231167\n",
      "151 - Loss_train: 0.08370239678062122, Loss_test: 0.08179210376300956\n",
      "152 - Loss_train: 0.0837020400976455, Loss_test: 0.08179225920981828\n",
      "153 - Loss_train: 0.08370168360039677, Loss_test: 0.08179241472777822\n",
      "154 - Loss_train: 0.08370132728839279, Loss_test: 0.08179257030237175\n",
      "155 - Loss_train: 0.08370097116152227, Loss_test: 0.08179272591099966\n",
      "156 - Loss_train: 0.08370061522047083, Loss_test: 0.08179288158340077\n",
      "157 - Loss_train: 0.08370025946395986, Loss_test: 0.08179303731194106\n",
      "158 - Loss_train: 0.08369990389243127, Loss_test: 0.08179319309090458\n",
      "159 - Loss_train: 0.08369954850593075, Loss_test: 0.08179334890193728\n",
      "160 - Loss_train: 0.08369919330425327, Loss_test: 0.08179350479502472\n",
      "161 - Loss_train: 0.08369883828710507, Loss_test: 0.08179366072783731\n",
      "162 - Loss_train: 0.08369848345510822, Loss_test: 0.08179381671600172\n",
      "163 - Loss_train: 0.08369812880739315, Loss_test: 0.08179397276489447\n",
      "164 - Loss_train: 0.08369777434469608, Loss_test: 0.0817941288466674\n",
      "165 - Loss_train: 0.08369742006588572, Loss_test: 0.08179428498887571\n",
      "166 - Loss_train: 0.08369706597159435, Loss_test: 0.081794441190232\n",
      "167 - Loss_train: 0.08369671206089106, Loss_test: 0.08179459744095453\n",
      "168 - Loss_train: 0.0836963583344045, Loss_test: 0.08179475374321425\n",
      "169 - Loss_train: 0.08369600479178575, Loss_test: 0.08179491008992533\n",
      "170 - Loss_train: 0.08369565143299959, Loss_test: 0.08179506649481418\n",
      "171 - Loss_train: 0.0836952982579849, Loss_test: 0.08179522296824902\n",
      "172 - Loss_train: 0.08369494526677607, Loss_test: 0.08179537947918052\n",
      "173 - Loss_train: 0.08369459245954436, Loss_test: 0.08179553602131623\n",
      "174 - Loss_train: 0.08369423983589049, Loss_test: 0.08179569265151816\n",
      "175 - Loss_train: 0.08369388739557915, Loss_test: 0.08179584930633226\n",
      "176 - Loss_train: 0.08369353513853267, Loss_test: 0.08179600602464643\n",
      "177 - Loss_train: 0.08369318306419737, Loss_test: 0.08179616279662126\n",
      "178 - Loss_train: 0.08369283117342054, Loss_test: 0.08179631961571288\n",
      "179 - Loss_train: 0.08369247946519112, Loss_test: 0.08179647648989398\n",
      "180 - Loss_train: 0.08369212793994277, Loss_test: 0.08179663342149586\n",
      "181 - Loss_train: 0.0836917765974152, Loss_test: 0.08179679037474695\n",
      "182 - Loss_train: 0.0836914254381552, Loss_test: 0.08179694742688563\n",
      "183 - Loss_train: 0.08369107446102032, Loss_test: 0.08179710449145196\n",
      "184 - Loss_train: 0.0836907236664385, Loss_test: 0.08179726161241058\n",
      "185 - Loss_train: 0.08369037305476854, Loss_test: 0.0817974188055031\n",
      "186 - Loss_train: 0.08369002262489991, Loss_test: 0.08179757602874553\n",
      "187 - Loss_train: 0.08368967237781501, Loss_test: 0.08179773331833924\n",
      "188 - Loss_train: 0.08368932231247657, Loss_test: 0.08179789063539875\n",
      "189 - Loss_train: 0.08368897242966661, Loss_test: 0.08179804802233542\n",
      "190 - Loss_train: 0.08368862272816355, Loss_test: 0.08179820545346711\n",
      "191 - Loss_train: 0.0836882732086633, Loss_test: 0.08179836294936792\n",
      "192 - Loss_train: 0.08368792387084883, Loss_test: 0.08179852048481556\n",
      "193 - Loss_train: 0.08368757471471655, Loss_test: 0.0817986780608109\n",
      "194 - Loss_train: 0.08368722574031626, Loss_test: 0.08179883570933794\n",
      "195 - Loss_train: 0.08368687694776777, Loss_test: 0.08179899339478088\n",
      "196 - Loss_train: 0.08368652833675963, Loss_test: 0.0817991511263582\n",
      "197 - Loss_train: 0.08368617990660468, Loss_test: 0.08179930892734\n",
      "198 - Loss_train: 0.08368583165757845, Loss_test: 0.08179946675918431\n",
      "199 - Loss_train: 0.08368548358988409, Loss_test: 0.08179962466137447\n",
      "200 - Loss_train: 0.08368513570350349, Loss_test: 0.08179978258686915\n",
      "201 - Loss_train: 0.08368478799740173, Loss_test: 0.0817999405715285\n",
      "202 - Loss_train: 0.08368444047207466, Loss_test: 0.08180009861096231\n",
      "203 - Loss_train: 0.08368409312776609, Loss_test: 0.0818002567134068\n",
      "204 - Loss_train: 0.08368374596411866, Loss_test: 0.08180041485220882\n",
      "205 - Loss_train: 0.0836833989810584, Loss_test: 0.08180057304076423\n",
      "206 - Loss_train: 0.0836830521787045, Loss_test: 0.08180073128341853\n",
      "207 - Loss_train: 0.08368270555680407, Loss_test: 0.08180088957004132\n",
      "208 - Loss_train: 0.08368235911475651, Loss_test: 0.08180104790097717\n",
      "209 - Loss_train: 0.08368201285294817, Loss_test: 0.08180120629912904\n",
      "210 - Loss_train: 0.08368166677155092, Loss_test: 0.08180136473562906\n",
      "211 - Loss_train: 0.08368132086998122, Loss_test: 0.08180152321847121\n",
      "212 - Loss_train: 0.08368097514796688, Loss_test: 0.0818016817548548\n",
      "213 - Loss_train: 0.08368062960585576, Loss_test: 0.08180184034339082\n",
      "214 - Loss_train: 0.0836802842434607, Loss_test: 0.0818019989907427\n",
      "215 - Loss_train: 0.08367993906062655, Loss_test: 0.0818021576585259\n",
      "216 - Loss_train: 0.0836795940573181, Loss_test: 0.08180231639772956\n",
      "217 - Loss_train: 0.0836792492335566, Loss_test: 0.0818024751940853\n",
      "218 - Loss_train: 0.08367890458965432, Loss_test: 0.08180263401256455\n",
      "219 - Loss_train: 0.0836785601246158, Loss_test: 0.0818027929047125\n",
      "220 - Loss_train: 0.08367821583852894, Loss_test: 0.08180295182869987\n",
      "221 - Loss_train: 0.08367787173153761, Loss_test: 0.08180311082232979\n",
      "222 - Loss_train: 0.08367752780349032, Loss_test: 0.08180326984028023\n",
      "223 - Loss_train: 0.08367718405427792, Loss_test: 0.08180342892896837\n",
      "224 - Loss_train: 0.08367684048380818, Loss_test: 0.0818035880456362\n",
      "225 - Loss_train: 0.08367649709201294, Loss_test: 0.08180374723129644\n",
      "226 - Loss_train: 0.0836761538788929, Loss_test: 0.08180390644537251\n",
      "227 - Loss_train: 0.08367581084435374, Loss_test: 0.08180406573263845\n",
      "228 - Loss_train: 0.08367546798810674, Loss_test: 0.08180422504656713\n",
      "229 - Loss_train: 0.08367512531096416, Loss_test: 0.08180438441226486\n",
      "230 - Loss_train: 0.0836747828112889, Loss_test: 0.08180454384082307\n",
      "231 - Loss_train: 0.08367444049009257, Loss_test: 0.08180470328760805\n",
      "232 - Loss_train: 0.08367409834658163, Loss_test: 0.0818048628347488\n",
      "233 - Loss_train: 0.08367375638107762, Loss_test: 0.08180502236982722\n",
      "234 - Loss_train: 0.08367341459342086, Loss_test: 0.0818051820024142\n",
      "235 - Loss_train: 0.08367307298365967, Loss_test: 0.0818053416443686\n",
      "236 - Loss_train: 0.08367273155134801, Loss_test: 0.08180550135231612\n",
      "237 - Loss_train: 0.08367239029658319, Loss_test: 0.08180566111225258\n",
      "238 - Loss_train: 0.08367204921989828, Loss_test: 0.08180582093038317\n",
      "239 - Loss_train: 0.0836717083200005, Loss_test: 0.08180598076707044\n",
      "240 - Loss_train: 0.08367136759792408, Loss_test: 0.08180614067209242\n",
      "241 - Loss_train: 0.08367102705259374, Loss_test: 0.08180630061892877\n",
      "242 - Loss_train: 0.08367068668447783, Loss_test: 0.08180646062572075\n",
      "243 - Loss_train: 0.08367034649336691, Loss_test: 0.08180662065694096\n",
      "244 - Loss_train: 0.08367000647920202, Loss_test: 0.08180678074366404\n",
      "245 - Loss_train: 0.08366966664216198, Loss_test: 0.08180694089686638\n",
      "246 - Loss_train: 0.08366932698204096, Loss_test: 0.08180710106765321\n",
      "247 - Loss_train: 0.0836689874982973, Loss_test: 0.08180726131875994\n",
      "248 - Loss_train: 0.08366864819085996, Loss_test: 0.08180742158649985\n",
      "249 - Loss_train: 0.08366830905991364, Loss_test: 0.08180758192206747\n",
      "250 - Loss_train: 0.08366797010528836, Loss_test: 0.08180774228274214\n",
      "251 - Loss_train: 0.08366763132699846, Loss_test: 0.08180790271939545\n",
      "252 - Loss_train: 0.08366729272488231, Loss_test: 0.0818080631807698\n",
      "253 - Loss_train: 0.08366695429909221, Loss_test: 0.08180822371240513\n",
      "254 - Loss_train: 0.08366661604910551, Loss_test: 0.08180838426418106\n",
      "255 - Loss_train: 0.08366627797510648, Loss_test: 0.08180854487641265\n",
      "256 - Loss_train: 0.0836659400774962, Loss_test: 0.08180870551790244\n",
      "257 - Loss_train: 0.08366560235492151, Loss_test: 0.08180886624241886\n",
      "258 - Loss_train: 0.08366526480851096, Loss_test: 0.08180902697492679\n",
      "259 - Loss_train: 0.08366492743732444, Loss_test: 0.08180918778821439\n",
      "260 - Loss_train: 0.08366459024149389, Loss_test: 0.08180934862141905\n",
      "261 - Loss_train: 0.08366425322124896, Loss_test: 0.08180950952002236\n",
      "262 - Loss_train: 0.08366391637613177, Loss_test: 0.08180967045887988\n",
      "263 - Loss_train: 0.08366357970697692, Loss_test: 0.08180983144048139\n",
      "264 - Loss_train: 0.08366324321208181, Loss_test: 0.08180999246512405\n",
      "265 - Loss_train: 0.08366290689215373, Loss_test: 0.08181015355084302\n",
      "266 - Loss_train: 0.08366257074768926, Loss_test: 0.08181031467284625\n",
      "267 - Loss_train: 0.08366223477817193, Loss_test: 0.08181047582103763\n",
      "268 - Loss_train: 0.08366189898285689, Loss_test: 0.08181063706641942\n",
      "269 - Loss_train: 0.08366156336215175, Loss_test: 0.081810798323764\n",
      "270 - Loss_train: 0.08366122791602616, Loss_test: 0.08181095961069147\n",
      "271 - Loss_train: 0.08366089264471534, Loss_test: 0.08181112097636759\n",
      "272 - Loss_train: 0.08366055754743533, Loss_test: 0.08181128237982555\n",
      "273 - Loss_train: 0.08366022262451714, Loss_test: 0.08181144383341009\n",
      "274 - Loss_train: 0.08365988787615541, Loss_test: 0.08181160531169292\n",
      "275 - Loss_train: 0.08365955330150364, Loss_test: 0.08181176685654197\n",
      "276 - Loss_train: 0.08365921890085178, Loss_test: 0.08181192844166305\n",
      "277 - Loss_train: 0.08365888467428141, Loss_test: 0.0818120900607642\n",
      "278 - Loss_train: 0.08365855062131412, Loss_test: 0.08181225175147673\n",
      "279 - Loss_train: 0.0836582167421717, Loss_test: 0.08181241346300373\n",
      "280 - Loss_train: 0.08365788303663585, Loss_test: 0.08181257523157401\n",
      "281 - Loss_train: 0.08365754950471059, Loss_test: 0.08181273703590883\n",
      "282 - Loss_train: 0.08365721614630298, Loss_test: 0.08181289890871238\n",
      "283 - Loss_train: 0.08365688296133, Loss_test: 0.08181306080080226\n",
      "284 - Loss_train: 0.08365654994940433, Loss_test: 0.08181322273725343\n",
      "285 - Loss_train: 0.08365621711072788, Loss_test: 0.08181338474442615\n",
      "286 - Loss_train: 0.08365588444533667, Loss_test: 0.08181354677329539\n",
      "287 - Loss_train: 0.0836555519528167, Loss_test: 0.08181370885973474\n",
      "288 - Loss_train: 0.08365521963319536, Loss_test: 0.08181387098321302\n",
      "289 - Loss_train: 0.08365488748708248, Loss_test: 0.08181403316528951\n",
      "290 - Loss_train: 0.08365455551302348, Loss_test: 0.08181419537448752\n",
      "291 - Loss_train: 0.08365422371163495, Loss_test: 0.08181435763933294\n",
      "292 - Loss_train: 0.08365389208304017, Loss_test: 0.08181451995005437\n",
      "293 - Loss_train: 0.08365356062738745, Loss_test: 0.08181468230463596\n",
      "294 - Loss_train: 0.08365322934357723, Loss_test: 0.08181484470305683\n",
      "295 - Loss_train: 0.08365289823199423, Loss_test: 0.08181500713891852\n",
      "296 - Loss_train: 0.08365256729269818, Loss_test: 0.08181516962792225\n",
      "297 - Loss_train: 0.0836522365256556, Loss_test: 0.08181533215237835\n",
      "298 - Loss_train: 0.0836519059305317, Loss_test: 0.08181549473880717\n",
      "299 - Loss_train: 0.08365157550736356, Loss_test: 0.08181565735760382\n",
      "300 - Loss_train: 0.08365124525624668, Loss_test: 0.08181582001141809\n",
      "301 - Loss_train: 0.08365091517678944, Loss_test: 0.08181598272433012\n",
      "302 - Loss_train: 0.0836505852689194, Loss_test: 0.08181614547936064\n",
      "303 - Loss_train: 0.08365025553260323, Loss_test: 0.08181630827016173\n",
      "304 - Loss_train: 0.08364992596790453, Loss_test: 0.08181647112487817\n",
      "305 - Loss_train: 0.08364959657453579, Loss_test: 0.08181663398669384\n",
      "306 - Loss_train: 0.0836492673529881, Loss_test: 0.08181679693696309\n",
      "307 - Loss_train: 0.08364893830268064, Loss_test: 0.0818169599018035\n",
      "308 - Loss_train: 0.08364860942308365, Loss_test: 0.08181712292696003\n",
      "309 - Loss_train: 0.08364828071439692, Loss_test: 0.08181728598105044\n",
      "310 - Loss_train: 0.08364795217677558, Loss_test: 0.08181744908893719\n",
      "311 - Loss_train: 0.08364762381000178, Loss_test: 0.08181761223428186\n",
      "312 - Loss_train: 0.08364729561405465, Loss_test: 0.08181777543050534\n",
      "313 - Loss_train: 0.0836469675887109, Loss_test: 0.08181793866487974\n",
      "314 - Loss_train: 0.0836466397340661, Loss_test: 0.08181810194143288\n",
      "315 - Loss_train: 0.08364631204987268, Loss_test: 0.08181826527125795\n",
      "316 - Loss_train: 0.08364598453662682, Loss_test: 0.08181842862641674\n",
      "317 - Loss_train: 0.08364565719336033, Loss_test: 0.08181859204439877\n",
      "318 - Loss_train: 0.08364533002022656, Loss_test: 0.08181875550664214\n",
      "319 - Loss_train: 0.08364500301740942, Loss_test: 0.08181891899567166\n",
      "320 - Loss_train: 0.0836446761848812, Loss_test: 0.08181908254009489\n",
      "321 - Loss_train: 0.08364434952207218, Loss_test: 0.08181924610590181\n",
      "322 - Loss_train: 0.08364402302938931, Loss_test: 0.0818194097469595\n",
      "323 - Loss_train: 0.08364369670636812, Loss_test: 0.08181957341526978\n",
      "324 - Loss_train: 0.0836433705529676, Loss_test: 0.08181973713127029\n",
      "325 - Loss_train: 0.08364304456980774, Loss_test: 0.08181990088334716\n",
      "326 - Loss_train: 0.08364271875585981, Loss_test: 0.08182006468008426\n",
      "327 - Loss_train: 0.08364239311155237, Loss_test: 0.08182022852718934\n",
      "328 - Loss_train: 0.08364206763614447, Loss_test: 0.08182039240061713\n",
      "329 - Loss_train: 0.08364174233007798, Loss_test: 0.08182055633652707\n",
      "330 - Loss_train: 0.0836414171931893, Loss_test: 0.08182072030957568\n",
      "331 - Loss_train: 0.08364109222573099, Loss_test: 0.08182088430555925\n",
      "332 - Loss_train: 0.08364076742734651, Loss_test: 0.081821048366927\n",
      "333 - Loss_train: 0.08364044279812197, Loss_test: 0.08182121246045754\n",
      "334 - Loss_train: 0.08364011833722321, Loss_test: 0.0818213766114074\n",
      "335 - Loss_train: 0.08363979404494698, Loss_test: 0.08182154078170388\n",
      "336 - Loss_train: 0.08363946992142385, Loss_test: 0.08182170502097677\n",
      "337 - Loss_train: 0.08363914596649144, Loss_test: 0.08182186928228928\n",
      "338 - Loss_train: 0.08363882218044164, Loss_test: 0.08182203358853377\n",
      "339 - Loss_train: 0.08363849856218929, Loss_test: 0.08182219792545885\n",
      "340 - Loss_train: 0.08363817511280991, Loss_test: 0.08182236232508719\n",
      "341 - Loss_train: 0.08363785183119123, Loss_test: 0.08182252675872169\n",
      "342 - Loss_train: 0.08363752871830515, Loss_test: 0.08182269124720404\n",
      "343 - Loss_train: 0.08363720577295308, Loss_test: 0.0818228557567164\n",
      "344 - Loss_train: 0.08363688299557007, Loss_test: 0.081823020336106\n",
      "345 - Loss_train: 0.0836365603862041, Loss_test: 0.08182318493015005\n",
      "346 - Loss_train: 0.08363623794452582, Loss_test: 0.08182334958828327\n",
      "347 - Loss_train: 0.08363591567064518, Loss_test: 0.08182351426963091\n",
      "348 - Loss_train: 0.08363559356448637, Loss_test: 0.08182367899597331\n",
      "349 - Loss_train: 0.08363527162580484, Loss_test: 0.08182384375992341\n",
      "350 - Loss_train: 0.08363494985437021, Loss_test: 0.08182400856432132\n",
      "351 - Loss_train: 0.08363462825040105, Loss_test: 0.08182417342602938\n",
      "352 - Loss_train: 0.08363430681412429, Loss_test: 0.08182433832041071\n",
      "353 - Loss_train: 0.08363398554467497, Loss_test: 0.08182450325634673\n",
      "354 - Loss_train: 0.08363366444269897, Loss_test: 0.0818246682418418\n",
      "355 - Loss_train: 0.0836333435072752, Loss_test: 0.0818248332525351\n",
      "356 - Loss_train: 0.08363302273877923, Loss_test: 0.08182499832506458\n",
      "357 - Loss_train: 0.08363270213725925, Loss_test: 0.08182516342413995\n",
      "358 - Loss_train: 0.08363238170248116, Loss_test: 0.08182532855759232\n",
      "359 - Loss_train: 0.08363206143438356, Loss_test: 0.08182549374358379\n",
      "360 - Loss_train: 0.0836317413328388, Loss_test: 0.0818256589601051\n",
      "361 - Loss_train: 0.08363142139774048, Loss_test: 0.08182582424254183\n",
      "362 - Loss_train: 0.08363110162914725, Loss_test: 0.08182598952997018\n",
      "363 - Loss_train: 0.08363078202725778, Loss_test: 0.08182615489340504\n",
      "364 - Loss_train: 0.0836304625915949, Loss_test: 0.08182632027380672\n",
      "365 - Loss_train: 0.08363014332215728, Loss_test: 0.08182648570697049\n",
      "366 - Loss_train: 0.08362982421865359, Loss_test: 0.08182665118712781\n",
      "367 - Loss_train: 0.08362950528059221, Loss_test: 0.08182681667784847\n",
      "368 - Loss_train: 0.08362918650907868, Loss_test: 0.08182698224468313\n",
      "369 - Loss_train: 0.08362886790300128, Loss_test: 0.08182714783851215\n",
      "370 - Loss_train: 0.08362854946315688, Loss_test: 0.08182731345529927\n",
      "371 - Loss_train: 0.08362823118833763, Loss_test: 0.08182747914965267\n",
      "372 - Loss_train: 0.08362791307971724, Loss_test: 0.0818276448625679\n",
      "373 - Loss_train: 0.08362759513583576, Loss_test: 0.08182781059778226\n",
      "374 - Loss_train: 0.08362727735779085, Loss_test: 0.08182797639447792\n",
      "375 - Loss_train: 0.083626959744596, Loss_test: 0.08182814222793626\n",
      "376 - Loss_train: 0.08362664229700577, Loss_test: 0.08182830810956138\n",
      "377 - Loss_train: 0.08362632501470309, Loss_test: 0.08182847402550518\n",
      "378 - Loss_train: 0.08362600789692612, Loss_test: 0.08182863997699898\n",
      "379 - Loss_train: 0.08362569094397027, Loss_test: 0.08182880596169362\n",
      "380 - Loss_train: 0.08362537415599824, Loss_test: 0.08182897200235946\n",
      "381 - Loss_train: 0.08362505753268758, Loss_test: 0.0818291380737832\n",
      "382 - Loss_train: 0.08362474107399519, Loss_test: 0.08182930418537442\n",
      "383 - Loss_train: 0.08362442477999983, Loss_test: 0.08182947035408067\n",
      "384 - Loss_train: 0.0836241086509712, Loss_test: 0.08182963654450734\n",
      "385 - Loss_train: 0.08362379268637002, Loss_test: 0.0818298027682516\n",
      "386 - Loss_train: 0.08362347688618399, Loss_test: 0.08182996906124719\n",
      "387 - Loss_train: 0.08362316124985159, Loss_test: 0.08183013535175464\n",
      "388 - Loss_train: 0.083622845777618, Loss_test: 0.08183030171546052\n",
      "389 - Loss_train: 0.08362253046968428, Loss_test: 0.08183046809293693\n",
      "390 - Loss_train: 0.08362221532561244, Loss_test: 0.08183063453896171\n",
      "391 - Loss_train: 0.08362190034580033, Loss_test: 0.08183080100786327\n",
      "392 - Loss_train: 0.08362158552943251, Loss_test: 0.08183096751137642\n",
      "393 - Loss_train: 0.08362127087677634, Loss_test: 0.08183113406305292\n",
      "394 - Loss_train: 0.08362095638784635, Loss_test: 0.08183130064661004\n",
      "395 - Loss_train: 0.0836206420629097, Loss_test: 0.08183146729016685\n",
      "396 - Loss_train: 0.08362032790096484, Loss_test: 0.08183163394474029\n",
      "397 - Loss_train: 0.0836200139024616, Loss_test: 0.08183180065694154\n",
      "398 - Loss_train: 0.08361970006717805, Loss_test: 0.08183196740426146\n",
      "399 - Loss_train: 0.08361938639519596, Loss_test: 0.08183213419189132\n",
      "400 - Loss_train: 0.08361907288694413, Loss_test: 0.08183230100856907\n",
      "401 - Loss_train: 0.0836187595412478, Loss_test: 0.08183246787456869\n",
      "402 - Loss_train: 0.08361844635834338, Loss_test: 0.08183263475574457\n",
      "403 - Loss_train: 0.08361813333855682, Loss_test: 0.08183280171449528\n",
      "404 - Loss_train: 0.08361782048204376, Loss_test: 0.08183296868133723\n",
      "405 - Loss_train: 0.08361750778771941, Loss_test: 0.08183313570896207\n",
      "406 - Loss_train: 0.08361719525606562, Loss_test: 0.08183330276599446\n",
      "407 - Loss_train: 0.08361688288702976, Loss_test: 0.08183346984111324\n",
      "408 - Loss_train: 0.08361657068051838, Loss_test: 0.08183363697675756\n",
      "409 - Loss_train: 0.08361625863638597, Loss_test: 0.0818338041621963\n",
      "410 - Loss_train: 0.08361594675502464, Loss_test: 0.081833971372604\n",
      "411 - Loss_train: 0.08361563503564225, Loss_test: 0.08183413861645374\n",
      "412 - Loss_train: 0.08361532347806129, Loss_test: 0.0818343058878901\n",
      "413 - Loss_train: 0.08361501208254363, Loss_test: 0.0818344732295145\n",
      "414 - Loss_train: 0.08361470084943844, Loss_test: 0.08183464058875389\n",
      "415 - Loss_train: 0.0836143897778315, Loss_test: 0.08183480798706232\n",
      "416 - Loss_train: 0.08361407886838657, Loss_test: 0.08183497542513582\n",
      "417 - Loss_train: 0.08361376812027059, Loss_test: 0.08183514289322874\n",
      "418 - Loss_train: 0.08361345753385958, Loss_test: 0.0818353104033832\n",
      "419 - Loss_train: 0.08361314710897409, Loss_test: 0.08183547797323511\n",
      "420 - Loss_train: 0.08361283684551078, Loss_test: 0.08183564555277427\n",
      "421 - Loss_train: 0.08361252674348155, Loss_test: 0.0818358131840316\n",
      "422 - Loss_train: 0.08361221680312395, Loss_test: 0.08183598085298445\n",
      "423 - Loss_train: 0.08361190702397678, Loss_test: 0.08183614854968844\n",
      "424 - Loss_train: 0.08361159740552264, Loss_test: 0.08183631627605299\n",
      "425 - Loss_train: 0.08361128794827737, Loss_test: 0.08183648407982336\n",
      "426 - Loss_train: 0.08361097865234854, Loss_test: 0.08183665188580685\n",
      "427 - Loss_train: 0.08361066951732132, Loss_test: 0.08183681973834381\n",
      "428 - Loss_train: 0.08361036054268711, Loss_test: 0.08183698763407167\n",
      "429 - Loss_train: 0.08361005172854045, Loss_test: 0.08183715553960486\n",
      "430 - Loss_train: 0.08360974307523031, Loss_test: 0.08183732353336357\n",
      "431 - Loss_train: 0.08360943458249392, Loss_test: 0.08183749153035964\n",
      "432 - Loss_train: 0.08360912625021072, Loss_test: 0.08183765958241022\n",
      "433 - Loss_train: 0.08360881807816245, Loss_test: 0.08183782764027057\n",
      "434 - Loss_train: 0.08360851006657848, Loss_test: 0.08183799576695866\n",
      "435 - Loss_train: 0.08360820221539127, Loss_test: 0.08183816391139345\n",
      "436 - Loss_train: 0.08360789452408185, Loss_test: 0.08183833209476457\n",
      "437 - Loss_train: 0.08360758699295295, Loss_test: 0.08183850034160368\n",
      "438 - Loss_train: 0.08360727962168271, Loss_test: 0.08183866859170125\n",
      "439 - Loss_train: 0.08360697241059488, Loss_test: 0.08183883688020657\n",
      "440 - Loss_train: 0.08360666535896867, Loss_test: 0.08183900522752158\n",
      "441 - Loss_train: 0.08360635846710318, Loss_test: 0.08183917358779047\n",
      "442 - Loss_train: 0.08360605173492745, Loss_test: 0.08183934200529755\n",
      "443 - Loss_train: 0.08360574516265644, Loss_test: 0.08183951044397948\n",
      "444 - Loss_train: 0.08360543874929216, Loss_test: 0.08183967893835681\n",
      "445 - Loss_train: 0.08360513249583844, Loss_test: 0.08183984744563233\n",
      "446 - Loss_train: 0.08360482640165653, Loss_test: 0.08184001601313476\n",
      "447 - Loss_train: 0.08360452046669707, Loss_test: 0.0818401846061707\n",
      "448 - Loss_train: 0.08360421469045375, Loss_test: 0.0818403532293376\n",
      "449 - Loss_train: 0.08360390907326964, Loss_test: 0.08184052189450494\n",
      "450 - Loss_train: 0.08360360361508286, Loss_test: 0.08184069059162644\n",
      "451 - Loss_train: 0.08360329831575739, Loss_test: 0.08184085932300313\n",
      "452 - Loss_train: 0.08360299317576446, Loss_test: 0.08184102810969467\n",
      "453 - Loss_train: 0.08360268819391849, Loss_test: 0.08184119692525638\n",
      "454 - Loss_train: 0.08360238337078042, Loss_test: 0.0818413657670904\n",
      "455 - Loss_train: 0.08360207870628109, Loss_test: 0.0818415346530176\n",
      "456 - Loss_train: 0.08360177420023064, Loss_test: 0.0818417035641568\n",
      "457 - Loss_train: 0.08360146985288919, Loss_test: 0.08184187252335344\n",
      "458 - Loss_train: 0.08360116566341569, Loss_test: 0.08184204150950641\n",
      "459 - Loss_train: 0.08360086163253716, Loss_test: 0.08184221053242562\n",
      "460 - Loss_train: 0.083600557759591, Loss_test: 0.08184237959063745\n",
      "461 - Loss_train: 0.08360025404470754, Loss_test: 0.0818425486913612\n",
      "462 - Loss_train: 0.08359995048786037, Loss_test: 0.0818427178317224\n",
      "463 - Loss_train: 0.08359964708871348, Loss_test: 0.08184288699683175\n",
      "464 - Loss_train: 0.08359934384733969, Loss_test: 0.08184305620315664\n",
      "465 - Loss_train: 0.08359904076370646, Loss_test: 0.08184322543491752\n",
      "466 - Loss_train: 0.08359873783780247, Loss_test: 0.0818433947240724\n",
      "467 - Loss_train: 0.08359843506947562, Loss_test: 0.08184356403634375\n",
      "468 - Loss_train: 0.08359813245905512, Loss_test: 0.08184373337317759\n",
      "469 - Loss_train: 0.08359783000610875, Loss_test: 0.0818439027561916\n",
      "470 - Loss_train: 0.08359752771017856, Loss_test: 0.08184407218138383\n",
      "471 - Loss_train: 0.08359722557135929, Loss_test: 0.08184424162418497\n",
      "472 - Loss_train: 0.08359692358982948, Loss_test: 0.08184441110820222\n",
      "473 - Loss_train: 0.0835966217651816, Loss_test: 0.08184458064371744\n",
      "474 - Loss_train: 0.08359632009796919, Loss_test: 0.08184475018909465\n",
      "475 - Loss_train: 0.08359601858743576, Loss_test: 0.08184491978869637\n",
      "476 - Loss_train: 0.08359571723363536, Loss_test: 0.0818450894183151\n",
      "477 - Loss_train: 0.08359541603665609, Loss_test: 0.08184525907390423\n",
      "478 - Loss_train: 0.08359511499625964, Loss_test: 0.08184542876710647\n",
      "479 - Loss_train: 0.0835948141124685, Loss_test: 0.08184559850517362\n",
      "480 - Loss_train: 0.08359451338524464, Loss_test: 0.08184576828559315\n",
      "481 - Loss_train: 0.08359421281442446, Loss_test: 0.08184593806759821\n",
      "482 - Loss_train: 0.08359391240036868, Loss_test: 0.08184610791579812\n",
      "483 - Loss_train: 0.08359361214255046, Loss_test: 0.08184627778103364\n",
      "484 - Loss_train: 0.08359331204063902, Loss_test: 0.08184644770031457\n",
      "485 - Loss_train: 0.08359301209469955, Loss_test: 0.08184661762573225\n",
      "486 - Loss_train: 0.08359271230474087, Loss_test: 0.08184678760797309\n",
      "487 - Loss_train: 0.08359241267108589, Loss_test: 0.0818469576080928\n",
      "488 - Loss_train: 0.08359211319294661, Loss_test: 0.08184712765906822\n",
      "489 - Loss_train: 0.08359181387095399, Loss_test: 0.08184729773054888\n",
      "490 - Loss_train: 0.08359151470431173, Loss_test: 0.08184746784635884\n",
      "491 - Loss_train: 0.08359121569321357, Loss_test: 0.08184763799528572\n",
      "492 - Loss_train: 0.0835909168377482, Loss_test: 0.0818478081694687\n",
      "493 - Loss_train: 0.0835906181376314, Loss_test: 0.08184797838151407\n",
      "494 - Loss_train: 0.08359031959332933, Loss_test: 0.08184814863010174\n",
      "495 - Loss_train: 0.0835900212040673, Loss_test: 0.08184831891520233\n",
      "496 - Loss_train: 0.0835897229705893, Loss_test: 0.08184848923739693\n",
      "497 - Loss_train: 0.08358942489155448, Loss_test: 0.08184865957652676\n",
      "498 - Loss_train: 0.0835891269679085, Loss_test: 0.08184882997224766\n",
      "499 - Loss_train: 0.08358882919892777, Loss_test: 0.08184900037244329\n",
      "500 - Loss_train: 0.08358853158490347, Loss_test: 0.081849170842656\n",
      "501 - Loss_train: 0.08358823412574674, Loss_test: 0.0818493413310346\n",
      "502 - Loss_train: 0.08358793682152083, Loss_test: 0.08184951183576744\n",
      "503 - Loss_train: 0.08358763967149936, Loss_test: 0.0818496823986297\n",
      "504 - Loss_train: 0.08358734267653226, Loss_test: 0.08184985298130844\n",
      "505 - Loss_train: 0.08358704583572478, Loss_test: 0.08185002361496167\n",
      "506 - Loss_train: 0.08358674914933804, Loss_test: 0.0818501942586401\n",
      "507 - Loss_train: 0.0835864526176212, Loss_test: 0.08185036495228454\n",
      "508 - Loss_train: 0.0835861562398265, Loss_test: 0.08185053566387887\n",
      "509 - Loss_train: 0.08358586001627474, Loss_test: 0.08185070643400127\n",
      "510 - Loss_train: 0.0835855639467401, Loss_test: 0.08185087720185394\n",
      "511 - Loss_train: 0.0835852680311608, Loss_test: 0.081851048034805\n",
      "512 - Loss_train: 0.08358497226994256, Loss_test: 0.08185121888786336\n",
      "513 - Loss_train: 0.08358467666256092, Loss_test: 0.08185138977459204\n",
      "514 - Loss_train: 0.08358438120889737, Loss_test: 0.08185156068111633\n",
      "515 - Loss_train: 0.0835840859086095, Loss_test: 0.08185173165214853\n",
      "516 - Loss_train: 0.08358379076187203, Loss_test: 0.08185190263205287\n",
      "517 - Loss_train: 0.08358349576899743, Loss_test: 0.08185207365768807\n",
      "518 - Loss_train: 0.08358320092928677, Loss_test: 0.08185224470639196\n",
      "519 - Loss_train: 0.08358290624290123, Loss_test: 0.08185241579587292\n",
      "520 - Loss_train: 0.08358261170972157, Loss_test: 0.08185258689766414\n",
      "521 - Loss_train: 0.08358231733027514, Loss_test: 0.08185275804844666\n",
      "522 - Loss_train: 0.0835820231038054, Loss_test: 0.08185292924362146\n",
      "523 - Loss_train: 0.08358172903002278, Loss_test: 0.0818531004561223\n",
      "524 - Loss_train: 0.08358143510938558, Loss_test: 0.08185327171251913\n",
      "525 - Loss_train: 0.08358114134162654, Loss_test: 0.08185344298232215\n",
      "526 - Loss_train: 0.08358084772657541, Loss_test: 0.0818536142899618\n",
      "527 - Loss_train: 0.08358055426443418, Loss_test: 0.08185378565120528\n",
      "528 - Loss_train: 0.08358026095523172, Loss_test: 0.08185395702847292\n",
      "529 - Loss_train: 0.08357996779809862, Loss_test: 0.08185412843684609\n",
      "530 - Loss_train: 0.08357967479419473, Loss_test: 0.08185429987011617\n",
      "531 - Loss_train: 0.08357938194215885, Loss_test: 0.08185447135016542\n",
      "532 - Loss_train: 0.08357908924240554, Loss_test: 0.0818546428592869\n",
      "533 - Loss_train: 0.0835787966948767, Loss_test: 0.08185481440354908\n",
      "534 - Loss_train: 0.0835785042999823, Loss_test: 0.08185498597309346\n",
      "535 - Loss_train: 0.08357821205681842, Loss_test: 0.08185515757264007\n",
      "536 - Loss_train: 0.08357791996545313, Loss_test: 0.08185532919908331\n",
      "537 - Loss_train: 0.08357762802660762, Loss_test: 0.08185550088464386\n",
      "538 - Loss_train: 0.08357733623917865, Loss_test: 0.08185567258129399\n",
      "539 - Loss_train: 0.08357704460351179, Loss_test: 0.08185584431400647\n",
      "540 - Loss_train: 0.08357675311994288, Loss_test: 0.0818560160720319\n",
      "541 - Loss_train: 0.08357646178766534, Loss_test: 0.0818561878757565\n",
      "542 - Loss_train: 0.0835761706069502, Loss_test: 0.08185635970420005\n",
      "543 - Loss_train: 0.08357587957758224, Loss_test: 0.08185653155675482\n",
      "544 - Loss_train: 0.08357558869971299, Loss_test: 0.08185670345423088\n",
      "545 - Loss_train: 0.08357529797308809, Loss_test: 0.08185687535848597\n",
      "546 - Loss_train: 0.08357500739816515, Loss_test: 0.08185704733396239\n",
      "547 - Loss_train: 0.08357471697401625, Loss_test: 0.08185721931250976\n",
      "548 - Loss_train: 0.08357442670132323, Loss_test: 0.08185739132184318\n",
      "549 - Loss_train: 0.08357413657956198, Loss_test: 0.08185756336351624\n",
      "550 - Loss_train: 0.08357384660836632, Loss_test: 0.08185773544910338\n",
      "551 - Loss_train: 0.08357355678804945, Loss_test: 0.08185790757311485\n",
      "552 - Loss_train: 0.08357326711850738, Loss_test: 0.08185807969595606\n",
      "553 - Loss_train: 0.08357297759959707, Loss_test: 0.08185825188048183\n",
      "554 - Loss_train: 0.08357268823138903, Loss_test: 0.08185842409433487\n",
      "555 - Loss_train: 0.08357239901364008, Loss_test: 0.08185859630927472\n",
      "556 - Loss_train: 0.0835721099467266, Loss_test: 0.08185876858648433\n",
      "557 - Loss_train: 0.083571821029677, Loss_test: 0.0818589408833641\n",
      "558 - Loss_train: 0.08357153226305916, Loss_test: 0.08185911321253408\n",
      "559 - Loss_train: 0.08357124364703018, Loss_test: 0.08185928558084388\n",
      "560 - Loss_train: 0.08357095518069166, Loss_test: 0.08185945796548896\n",
      "561 - Loss_train: 0.08357066686443201, Loss_test: 0.0818596303739907\n",
      "562 - Loss_train: 0.08357037869812489, Loss_test: 0.08185980283822392\n",
      "563 - Loss_train: 0.08357009068173897, Loss_test: 0.08185997531500717\n",
      "564 - Loss_train: 0.0835698028151945, Loss_test: 0.08186014783403837\n",
      "565 - Loss_train: 0.08356951509850825, Loss_test: 0.08186032038079977\n",
      "566 - Loss_train: 0.08356922753176804, Loss_test: 0.08186049295322519\n",
      "567 - Loss_train: 0.08356894011428949, Loss_test: 0.08186066556725519\n",
      "568 - Loss_train: 0.08356865284635433, Loss_test: 0.08186083819441285\n",
      "569 - Loss_train: 0.08356836572779582, Loss_test: 0.08186101085487162\n",
      "570 - Loss_train: 0.08356807875868091, Loss_test: 0.08186118355583939\n",
      "571 - Loss_train: 0.08356779193883068, Loss_test: 0.08186135628897473\n",
      "572 - Loss_train: 0.08356750526806543, Loss_test: 0.08186152905215284\n",
      "573 - Loss_train: 0.08356721874695792, Loss_test: 0.08186170183547362\n",
      "574 - Loss_train: 0.0835669323744026, Loss_test: 0.08186187464982077\n",
      "575 - Loss_train: 0.08356664615116938, Loss_test: 0.08186204749152536\n",
      "576 - Loss_train: 0.08356636007635365, Loss_test: 0.08186222038261524\n",
      "577 - Loss_train: 0.0835660741504571, Loss_test: 0.08186239328175\n",
      "578 - Loss_train: 0.08356578837332695, Loss_test: 0.08186256621451043\n",
      "579 - Loss_train: 0.08356550274479371, Loss_test: 0.08186273918258444\n",
      "580 - Loss_train: 0.08356521726528819, Loss_test: 0.0818629121847944\n",
      "581 - Loss_train: 0.08356493193422311, Loss_test: 0.08186308521299328\n",
      "582 - Loss_train: 0.0835646467516659, Loss_test: 0.08186325827423753\n",
      "583 - Loss_train: 0.08356436171688847, Loss_test: 0.0818634313523946\n",
      "584 - Loss_train: 0.08356407683056082, Loss_test: 0.08186360447370179\n",
      "585 - Loss_train: 0.0835637920927195, Loss_test: 0.08186377763650177\n",
      "586 - Loss_train: 0.08356350750268196, Loss_test: 0.08186395080283479\n",
      "587 - Loss_train: 0.08356322306063216, Loss_test: 0.08186412400995001\n",
      "588 - Loss_train: 0.08356293876660074, Loss_test: 0.08186429724667985\n",
      "589 - Loss_train: 0.0835626546205508, Loss_test: 0.08186447051352035\n",
      "590 - Loss_train: 0.08356237062276362, Loss_test: 0.08186464381180208\n",
      "591 - Loss_train: 0.08356208677256231, Loss_test: 0.08186481712571368\n",
      "592 - Loss_train: 0.08356180306958873, Loss_test: 0.08186499048773334\n",
      "593 - Loss_train: 0.08356151951422888, Loss_test: 0.08186516386811743\n",
      "594 - Loss_train: 0.08356123610654288, Loss_test: 0.08186533728123495\n",
      "595 - Loss_train: 0.08356095284659737, Loss_test: 0.08186551072611613\n",
      "596 - Loss_train: 0.08356066973384013, Loss_test: 0.08186568419913279\n",
      "597 - Loss_train: 0.08356038676813911, Loss_test: 0.08186585769332874\n",
      "598 - Loss_train: 0.08356010394959133, Loss_test: 0.08186603123196873\n",
      "599 - Loss_train: 0.08355982127807725, Loss_test: 0.08186620478413291\n",
      "600 - Loss_train: 0.08355953875361506, Loss_test: 0.08186637837281639\n",
      "601 - Loss_train: 0.08355925637655137, Loss_test: 0.08186655197743826\n",
      "602 - Loss_train: 0.08355897414599393, Loss_test: 0.08186672563938285\n",
      "603 - Loss_train: 0.08355869206241184, Loss_test: 0.08186689930860037\n",
      "604 - Loss_train: 0.08355841012551396, Loss_test: 0.0818670730132514\n",
      "605 - Loss_train: 0.08355812833525092, Loss_test: 0.08186724673306207\n",
      "606 - Loss_train: 0.0835578466915906, Loss_test: 0.08186742050160133\n",
      "607 - Loss_train: 0.08355756519484274, Loss_test: 0.0818675942981267\n",
      "608 - Loss_train: 0.08355728384441875, Loss_test: 0.08186776811237773\n",
      "609 - Loss_train: 0.08355700264033095, Loss_test: 0.08186794196567063\n",
      "610 - Loss_train: 0.08355672158215642, Loss_test: 0.0818681158334018\n",
      "611 - Loss_train: 0.08355644067012043, Loss_test: 0.08186828973183764\n",
      "612 - Loss_train: 0.08355615990435317, Loss_test: 0.08186846366655796\n",
      "613 - Loss_train: 0.08355587928495385, Loss_test: 0.08186863762409179\n",
      "614 - Loss_train: 0.08355559881150268, Loss_test: 0.08186881161132986\n",
      "615 - Loss_train: 0.08355531848380185, Loss_test: 0.08186898562885712\n",
      "616 - Loss_train: 0.08355503830181674, Loss_test: 0.08186915969209642\n",
      "617 - Loss_train: 0.08355475826562381, Loss_test: 0.08186933375065436\n",
      "618 - Loss_train: 0.08355447837556654, Loss_test: 0.08186950785971753\n",
      "619 - Loss_train: 0.08355419863060708, Loss_test: 0.08186968198104959\n",
      "620 - Loss_train: 0.08355391903123045, Loss_test: 0.08186985613939392\n",
      "621 - Loss_train: 0.08355363957741063, Loss_test: 0.08187003034524402\n",
      "622 - Loss_train: 0.08355336026910952, Loss_test: 0.08187020454360129\n",
      "623 - Loss_train: 0.08355308110581638, Loss_test: 0.08187037877808798\n",
      "624 - Loss_train: 0.08355280208768699, Loss_test: 0.08187055305353445\n",
      "625 - Loss_train: 0.08355252321515229, Loss_test: 0.08187072736040683\n",
      "626 - Loss_train: 0.08355224448777679, Loss_test: 0.0818709016787587\n",
      "627 - Loss_train: 0.08355196590493948, Loss_test: 0.08187107604172546\n",
      "628 - Loss_train: 0.08355168746711425, Loss_test: 0.08187125040670022\n",
      "629 - Loss_train: 0.0835514091740907, Loss_test: 0.0818714248352326\n",
      "630 - Loss_train: 0.08355113102616644, Loss_test: 0.08187159924552731\n",
      "631 - Loss_train: 0.0835508530227574, Loss_test: 0.08187177374241619\n",
      "632 - Loss_train: 0.08355057516382436, Loss_test: 0.08187194822565984\n",
      "633 - Loss_train: 0.08355029744939202, Loss_test: 0.08187212274507552\n",
      "634 - Loss_train: 0.08355001987979044, Loss_test: 0.08187229727399326\n",
      "635 - Loss_train: 0.08354974245414505, Loss_test: 0.08187247185157177\n",
      "636 - Loss_train: 0.08354946517287824, Loss_test: 0.08187264646400885\n",
      "637 - Loss_train: 0.0835491880358959, Loss_test: 0.08187282108791326\n",
      "638 - Loss_train: 0.08354891104295688, Loss_test: 0.08187299574038347\n",
      "639 - Loss_train: 0.08354863419420926, Loss_test: 0.08187317042207595\n",
      "640 - Loss_train: 0.08354835748943423, Loss_test: 0.08187334513469978\n",
      "641 - Loss_train: 0.08354808092875278, Loss_test: 0.08187351988063096\n",
      "642 - Loss_train: 0.08354780451184048, Loss_test: 0.08187369464076447\n",
      "643 - Loss_train: 0.0835475282386888, Loss_test: 0.08187386942988079\n",
      "644 - Loss_train: 0.08354725210968467, Loss_test: 0.08187404424969663\n",
      "645 - Loss_train: 0.08354697612394417, Loss_test: 0.08187421911196942\n",
      "646 - Loss_train: 0.08354670028215369, Loss_test: 0.08187439395449085\n",
      "647 - Loss_train: 0.08354642458350217, Loss_test: 0.08187456888060854\n",
      "648 - Loss_train: 0.08354614902815514, Loss_test: 0.08187474377780013\n",
      "649 - Loss_train: 0.08354587361634579, Loss_test: 0.08187491876261019\n",
      "650 - Loss_train: 0.08354559834812406, Loss_test: 0.08187509371797924\n",
      "651 - Loss_train: 0.08354532322277593, Loss_test: 0.0818752687511913\n",
      "652 - Loss_train: 0.08354504824086977, Loss_test: 0.08187544375274053\n",
      "653 - Loss_train: 0.0835447734019305, Loss_test: 0.08187561883055648\n",
      "654 - Loss_train: 0.08354449870559727, Loss_test: 0.08187579390523521\n",
      "655 - Loss_train: 0.0835442241523085, Loss_test: 0.08187596902782493\n",
      "656 - Loss_train: 0.08354394974188767, Loss_test: 0.08187614418555254\n",
      "657 - Loss_train: 0.08354367547417853, Loss_test: 0.08187631934104063\n",
      "658 - Loss_train: 0.0835434013489981, Loss_test: 0.08187649451960288\n",
      "659 - Loss_train: 0.08354312736656094, Loss_test: 0.08187666974838637\n",
      "660 - Loss_train: 0.0835428535269562, Loss_test: 0.0818768449778967\n",
      "661 - Loss_train: 0.08354257982952458, Loss_test: 0.08187702027799634\n",
      "662 - Loss_train: 0.0835423062743589, Loss_test: 0.08187719553777047\n",
      "663 - Loss_train: 0.0835420328614866, Loss_test: 0.08187737089159378\n",
      "664 - Loss_train: 0.08354175959084653, Loss_test: 0.08187754622660133\n",
      "665 - Loss_train: 0.08354148646236109, Loss_test: 0.08187772160560741\n",
      "666 - Loss_train: 0.08354121347639955, Loss_test: 0.08187789701108353\n",
      "667 - Loss_train: 0.08354094063246958, Loss_test: 0.08187807243780622\n",
      "668 - Loss_train: 0.08354066793053738, Loss_test: 0.0818782478934972\n",
      "669 - Loss_train: 0.08354039536997646, Loss_test: 0.0818784233753293\n",
      "670 - Loss_train: 0.08354012295128463, Loss_test: 0.0818785988795195\n",
      "671 - Loss_train: 0.08353985067474734, Loss_test: 0.08187877441251931\n",
      "672 - Loss_train: 0.08353957853938383, Loss_test: 0.08187894997476156\n",
      "673 - Loss_train: 0.08353930654561223, Loss_test: 0.08187912555098238\n",
      "674 - Loss_train: 0.08353903469384179, Loss_test: 0.08187930116867446\n",
      "675 - Loss_train: 0.08353876298331657, Loss_test: 0.08187947679487205\n",
      "676 - Loss_train: 0.08353849141385922, Loss_test: 0.081879652465371\n",
      "677 - Loss_train: 0.08353821998602508, Loss_test: 0.08187982812729339\n",
      "678 - Loss_train: 0.08353794869902191, Loss_test: 0.08188000386091575\n",
      "679 - Loss_train: 0.08353767755312738, Loss_test: 0.08188017957474593\n",
      "680 - Loss_train: 0.08353740654833719, Loss_test: 0.08188035535661527\n",
      "681 - Loss_train: 0.0835371356848532, Loss_test: 0.08188053113972021\n",
      "682 - Loss_train: 0.08353686496237682, Loss_test: 0.08188070695751126\n",
      "683 - Loss_train: 0.08353659438065904, Loss_test: 0.08188088277613623\n",
      "684 - Loss_train: 0.08353632393929007, Loss_test: 0.08188105865302103\n",
      "685 - Loss_train: 0.08353605363860976, Loss_test: 0.08188123455557474\n",
      "686 - Loss_train: 0.08353578347852768, Loss_test: 0.08188141044524257\n",
      "687 - Loss_train: 0.08353551345907512, Loss_test: 0.08188158639781326\n",
      "688 - Loss_train: 0.08353524358041556, Loss_test: 0.08188176235043895\n",
      "689 - Loss_train: 0.08353497384210068, Loss_test: 0.08188193833814347\n",
      "690 - Loss_train: 0.08353470424392087, Loss_test: 0.0818821143668213\n",
      "691 - Loss_train: 0.0835344347859014, Loss_test: 0.08188229039736125\n",
      "692 - Loss_train: 0.08353416546799897, Loss_test: 0.0818824664464633\n",
      "693 - Loss_train: 0.08353389629030937, Loss_test: 0.08188264255487437\n",
      "694 - Loss_train: 0.0835336272526379, Loss_test: 0.081882818662951\n",
      "695 - Loss_train: 0.08353335835487512, Loss_test: 0.08188299479265003\n",
      "696 - Loss_train: 0.08353308959746933, Loss_test: 0.08188317097537087\n",
      "697 - Loss_train: 0.08353282097936438, Loss_test: 0.08188334713847345\n",
      "698 - Loss_train: 0.08353255250118105, Loss_test: 0.08188352337288872\n",
      "699 - Loss_train: 0.08353228416294126, Loss_test: 0.08188369960886539\n",
      "700 - Loss_train: 0.08353201596384553, Loss_test: 0.08188387586336932\n",
      "701 - Loss_train: 0.0835317479043874, Loss_test: 0.08188405214956478\n",
      "702 - Loss_train: 0.0835314799843569, Loss_test: 0.08188422845045323\n",
      "703 - Loss_train: 0.08353121220379871, Loss_test: 0.08188440479737956\n",
      "704 - Loss_train: 0.08353094456275625, Loss_test: 0.08188458115194625\n",
      "705 - Loss_train: 0.08353067706070239, Loss_test: 0.08188475753333237\n",
      "706 - Loss_train: 0.08353040969779374, Loss_test: 0.08188493394932105\n",
      "707 - Loss_train: 0.08353014247399611, Loss_test: 0.08188511038169244\n",
      "708 - Loss_train: 0.08352987538909308, Loss_test: 0.08188528683069268\n",
      "709 - Loss_train: 0.08352960844319497, Loss_test: 0.08188546331055317\n",
      "710 - Loss_train: 0.08352934163619623, Loss_test: 0.08188563980870261\n",
      "711 - Loss_train: 0.08352907496826277, Loss_test: 0.08188581635097321\n",
      "712 - Loss_train: 0.08352880843878809, Loss_test: 0.0818859928957119\n",
      "713 - Loss_train: 0.08352854204800321, Loss_test: 0.08188616948708421\n",
      "714 - Loss_train: 0.08352827579581597, Loss_test: 0.08188634606875195\n",
      "715 - Loss_train: 0.08352800968220982, Loss_test: 0.08188652270077768\n",
      "716 - Loss_train: 0.08352774370695745, Loss_test: 0.0818866993618575\n",
      "717 - Loss_train: 0.0835274778705102, Loss_test: 0.08188687602228949\n",
      "718 - Loss_train: 0.08352721217187897, Loss_test: 0.08188705271869297\n",
      "719 - Loss_train: 0.08352694661201325, Loss_test: 0.08188722945267882\n",
      "720 - Loss_train: 0.0835266811898994, Loss_test: 0.08188740618442837\n",
      "721 - Loss_train: 0.08352641590567461, Loss_test: 0.08188758294937205\n",
      "722 - Loss_train: 0.0835261507595706, Loss_test: 0.08188775974595165\n",
      "723 - Loss_train: 0.08352588575139325, Loss_test: 0.08188793655333784\n",
      "724 - Loss_train: 0.08352562088161279, Loss_test: 0.08188811339634024\n",
      "725 - Loss_train: 0.08352535614908298, Loss_test: 0.08188829026195185\n",
      "726 - Loss_train: 0.08352509155425696, Loss_test: 0.0818884671432432\n",
      "727 - Loss_train: 0.08352482709719895, Loss_test: 0.08188864404990084\n",
      "728 - Loss_train: 0.08352456277771696, Loss_test: 0.08188882098564437\n",
      "729 - Loss_train: 0.08352429859601783, Loss_test: 0.08188899794341037\n",
      "730 - Loss_train: 0.08352403455183519, Loss_test: 0.0818891749209911\n",
      "731 - Loss_train: 0.08352377064457946, Loss_test: 0.08188935191781424\n",
      "732 - Loss_train: 0.0835235068745901, Loss_test: 0.08188952893309599\n",
      "733 - Loss_train: 0.08352324324189615, Loss_test: 0.08188970599291034\n",
      "734 - Loss_train: 0.08352297974680183, Loss_test: 0.0818898830642626\n",
      "735 - Loss_train: 0.083522716388285, Loss_test: 0.08189006014719848\n",
      "736 - Loss_train: 0.08352245316674729, Loss_test: 0.08189023727292895\n",
      "737 - Loss_train: 0.08352219008231655, Loss_test: 0.08189041442100974\n",
      "738 - Loss_train: 0.08352192713506272, Loss_test: 0.08189059157291678\n",
      "739 - Loss_train: 0.0835216643244797, Loss_test: 0.08189076875426096\n",
      "740 - Loss_train: 0.08352140165036649, Loss_test: 0.08189094595850892\n",
      "741 - Loss_train: 0.08352113911296055, Loss_test: 0.08189112321024515\n",
      "742 - Loss_train: 0.08352087671198757, Loss_test: 0.08189130044980134\n",
      "743 - Loss_train: 0.08352061444805219, Loss_test: 0.08189147773915417\n",
      "744 - Loss_train: 0.08352035232010485, Loss_test: 0.08189165502465995\n",
      "745 - Loss_train: 0.08352009032861331, Loss_test: 0.08189183235418143\n",
      "746 - Loss_train: 0.0835198284734545, Loss_test: 0.08189200971108837\n",
      "747 - Loss_train: 0.08351956675431665, Loss_test: 0.0818921870645215\n",
      "748 - Loss_train: 0.08351930517179962, Loss_test: 0.08189236445823303\n",
      "749 - Loss_train: 0.08351904372502206, Loss_test: 0.08189254187532242\n",
      "750 - Loss_train: 0.08351878241412783, Loss_test: 0.08189271930970722\n",
      "751 - Loss_train: 0.0835185212393582, Loss_test: 0.08189289676705304\n",
      "752 - Loss_train: 0.08351826020044141, Loss_test: 0.08189307423543556\n",
      "753 - Loss_train: 0.08351799929742543, Loss_test: 0.08189325174818123\n",
      "754 - Loss_train: 0.08351773853024752, Loss_test: 0.0818934292595365\n",
      "755 - Loss_train: 0.08351747789851813, Loss_test: 0.08189360681957815\n",
      "756 - Loss_train: 0.08351721740249615, Loss_test: 0.08189378438903688\n",
      "757 - Loss_train: 0.08351695704182836, Loss_test: 0.08189396196924398\n",
      "758 - Loss_train: 0.08351669681705809, Loss_test: 0.08189413958344373\n",
      "759 - Loss_train: 0.08351643672735377, Loss_test: 0.08189431723672336\n",
      "760 - Loss_train: 0.0835161767728609, Loss_test: 0.08189449488363236\n",
      "761 - Loss_train: 0.08351591695391516, Loss_test: 0.08189467255921254\n",
      "762 - Loss_train: 0.08351565726972839, Loss_test: 0.08189485027118514\n",
      "763 - Loss_train: 0.08351539772079296, Loss_test: 0.08189502798807073\n",
      "764 - Loss_train: 0.08351513830684228, Loss_test: 0.08189520573172089\n",
      "765 - Loss_train: 0.08351487902821529, Loss_test: 0.08189538349324701\n",
      "766 - Loss_train: 0.08351461988418417, Loss_test: 0.08189556128871693\n",
      "767 - Loss_train: 0.08351436087489962, Loss_test: 0.0818957390941328\n",
      "768 - Loss_train: 0.08351410200068345, Loss_test: 0.08189591692580452\n",
      "769 - Loss_train: 0.08351384326128086, Loss_test: 0.08189609477397403\n",
      "770 - Loss_train: 0.08351358465646591, Loss_test: 0.08189627265883902\n",
      "771 - Loss_train: 0.08351332618614862, Loss_test: 0.08189645055535533\n",
      "772 - Loss_train: 0.08351306785033384, Loss_test: 0.08189662847566628\n",
      "773 - Loss_train: 0.08351280964888692, Loss_test: 0.08189680641593026\n",
      "774 - Loss_train: 0.08351255158136187, Loss_test: 0.08189698437952185\n",
      "775 - Loss_train: 0.0835122936481275, Loss_test: 0.08189716236006644\n",
      "776 - Loss_train: 0.08351203584916955, Loss_test: 0.08189734036444485\n",
      "777 - Loss_train: 0.08351177818472219, Loss_test: 0.08189751838729324\n",
      "778 - Loss_train: 0.08351152065387604, Loss_test: 0.08189769644180733\n",
      "779 - Loss_train: 0.0835112632570185, Loss_test: 0.08189787450631102\n",
      "780 - Loss_train: 0.08351100599410931, Loss_test: 0.08189805259405546\n",
      "781 - Loss_train: 0.08351074886502913, Loss_test: 0.08189823069200397\n",
      "782 - Loss_train: 0.08351049186963655, Loss_test: 0.08189840883394053\n",
      "783 - Loss_train: 0.08351023500846616, Loss_test: 0.08189858699515298\n",
      "784 - Loss_train: 0.08350997828083993, Loss_test: 0.08189876517459518\n",
      "785 - Loss_train: 0.08350972168638428, Loss_test: 0.08189894335263587\n",
      "786 - Loss_train: 0.08350946522542167, Loss_test: 0.08189912156126923\n",
      "787 - Loss_train: 0.08350920889796626, Loss_test: 0.08189929981636014\n",
      "788 - Loss_train: 0.08350895270383989, Loss_test: 0.08189947805816261\n",
      "789 - Loss_train: 0.08350869664291881, Loss_test: 0.08189965634174558\n",
      "790 - Loss_train: 0.08350844071580031, Loss_test: 0.08189983465112423\n",
      "791 - Loss_train: 0.08350818492129028, Loss_test: 0.08190001296367116\n",
      "792 - Loss_train: 0.0835079292598318, Loss_test: 0.08190019130020865\n",
      "793 - Loss_train: 0.08350767373147207, Loss_test: 0.08190036967169528\n",
      "794 - Loss_train: 0.08350741833598069, Loss_test: 0.08190054804982302\n",
      "795 - Loss_train: 0.08350716307330246, Loss_test: 0.08190072645136624\n",
      "796 - Loss_train: 0.08350690794355795, Loss_test: 0.08190090487215712\n",
      "797 - Loss_train: 0.08350665294646245, Loss_test: 0.0819010833077035\n",
      "798 - Loss_train: 0.08350639808208686, Loss_test: 0.08190126179287825\n",
      "799 - Loss_train: 0.08350614335037522, Loss_test: 0.08190144027104224\n",
      "800 - Loss_train: 0.08350588875110654, Loss_test: 0.08190161876093431\n",
      "801 - Loss_train: 0.08350563428439547, Loss_test: 0.08190179730072952\n",
      "802 - Loss_train: 0.08350537995006932, Loss_test: 0.08190197584789781\n",
      "803 - Loss_train: 0.08350512574836447, Loss_test: 0.08190215441647837\n",
      "804 - Loss_train: 0.08350487167846313, Loss_test: 0.08190233300755236\n",
      "805 - Loss_train: 0.08350461774081536, Loss_test: 0.08190251160202477\n",
      "806 - Loss_train: 0.08350436393512191, Loss_test: 0.08190269022491972\n",
      "807 - Loss_train: 0.08350411026199145, Loss_test: 0.08190286887422166\n",
      "808 - Loss_train: 0.08350385672084838, Loss_test: 0.08190304754399032\n",
      "809 - Loss_train: 0.08350360331162207, Loss_test: 0.08190322623309731\n",
      "810 - Loss_train: 0.08350335003381562, Loss_test: 0.08190340493782725\n",
      "811 - Loss_train: 0.08350309688783618, Loss_test: 0.08190358366891184\n",
      "812 - Loss_train: 0.0835028438738789, Loss_test: 0.08190376240602423\n",
      "813 - Loss_train: 0.08350259099126452, Loss_test: 0.08190394118257627\n",
      "814 - Loss_train: 0.0835023382401902, Loss_test: 0.08190411995900596\n",
      "815 - Loss_train: 0.08350208562056198, Loss_test: 0.08190429876804492\n",
      "816 - Loss_train: 0.08350183313237267, Loss_test: 0.08190447760967592\n",
      "817 - Loss_train: 0.08350158077591106, Loss_test: 0.08190465643185557\n",
      "818 - Loss_train: 0.08350132855034793, Loss_test: 0.08190483531059194\n",
      "819 - Loss_train: 0.0835010764559994, Loss_test: 0.08190501419500382\n",
      "820 - Loss_train: 0.08350082449276928, Loss_test: 0.08190519309537553\n",
      "821 - Loss_train: 0.08350057266107097, Loss_test: 0.08190537202202991\n",
      "822 - Loss_train: 0.083500320960462, Loss_test: 0.08190555097376695\n",
      "823 - Loss_train: 0.0835000693905171, Loss_test: 0.08190572994222588\n",
      "824 - Loss_train: 0.08349981795136051, Loss_test: 0.08190590891459346\n",
      "825 - Loss_train: 0.0834995666430382, Loss_test: 0.08190608792113938\n",
      "826 - Loss_train: 0.0834993154655105, Loss_test: 0.08190626694214093\n",
      "827 - Loss_train: 0.08349906441885928, Loss_test: 0.08190644597415309\n",
      "828 - Loss_train: 0.08349881350245475, Loss_test: 0.08190662504917133\n",
      "829 - Loss_train: 0.08349856271695878, Loss_test: 0.08190680411365005\n",
      "830 - Loss_train: 0.08349831206172001, Loss_test: 0.08190698321283062\n",
      "831 - Loss_train: 0.08349806153693509, Loss_test: 0.08190716234252587\n",
      "832 - Loss_train: 0.08349781114286474, Loss_test: 0.08190734148293158\n",
      "833 - Loss_train: 0.08349756087896246, Loss_test: 0.0819075206190954\n",
      "834 - Loss_train: 0.08349731074483344, Loss_test: 0.08190769981395193\n",
      "835 - Loss_train: 0.08349706074093277, Loss_test: 0.08190787900311924\n",
      "836 - Loss_train: 0.08349681086699945, Loss_test: 0.08190805820993723\n",
      "837 - Loss_train: 0.08349656112306034, Loss_test: 0.08190823744513458\n",
      "838 - Loss_train: 0.08349631150948526, Loss_test: 0.08190841669174817\n",
      "839 - Loss_train: 0.0834960620254217, Loss_test: 0.0819085959740931\n",
      "840 - Loss_train: 0.08349581267114513, Loss_test: 0.08190877524230628\n",
      "841 - Loss_train: 0.08349556344688444, Loss_test: 0.08190895456174653\n",
      "842 - Loss_train: 0.08349531435205929, Loss_test: 0.08190913389836563\n",
      "843 - Loss_train: 0.08349506538675783, Loss_test: 0.08190931323115368\n",
      "844 - Loss_train: 0.08349481655097042, Loss_test: 0.0819094925990081\n",
      "845 - Loss_train: 0.08349456784469912, Loss_test: 0.08190967198125929\n",
      "846 - Loss_train: 0.08349431926786419, Loss_test: 0.08190985137380931\n",
      "847 - Loss_train: 0.08349407082063882, Loss_test: 0.0819100308092521\n",
      "848 - Loss_train: 0.08349382250233923, Loss_test: 0.08191021023596724\n",
      "849 - Loss_train: 0.08349357431333186, Loss_test: 0.08191038970054841\n",
      "850 - Loss_train: 0.08349332625338156, Loss_test: 0.08191056917025087\n",
      "851 - Loss_train: 0.08349307832256009, Loss_test: 0.08191074867106271\n",
      "852 - Loss_train: 0.08349283052106603, Loss_test: 0.08191092816559445\n",
      "853 - Loss_train: 0.08349258284847708, Loss_test: 0.08191110770402438\n",
      "854 - Loss_train: 0.08349233530430793, Loss_test: 0.08191128726492926\n",
      "855 - Loss_train: 0.0834920878889047, Loss_test: 0.08191146681814773\n",
      "856 - Loss_train: 0.08349184060241967, Loss_test: 0.0819116464102943\n",
      "857 - Loss_train: 0.08349159344443852, Loss_test: 0.08191182601453509\n",
      "858 - Loss_train: 0.08349134641512883, Loss_test: 0.08191200562746305\n",
      "859 - Loss_train: 0.0834910995143686, Loss_test: 0.08191218527353396\n",
      "860 - Loss_train: 0.08349085274248126, Loss_test: 0.08191236492275573\n",
      "861 - Loss_train: 0.08349060609851174, Loss_test: 0.08191254461693165\n",
      "862 - Loss_train: 0.0834903595833814, Loss_test: 0.08191272429768481\n",
      "863 - Loss_train: 0.0834901131959706, Loss_test: 0.08191290400847469\n",
      "864 - Loss_train: 0.08348986693680424, Loss_test: 0.08191308373449192\n",
      "865 - Loss_train: 0.08348962080580791, Loss_test: 0.08191326349049756\n",
      "866 - Loss_train: 0.08348937480323357, Loss_test: 0.08191344325514255\n",
      "867 - Loss_train: 0.0834891289286895, Loss_test: 0.0819136230275973\n",
      "868 - Loss_train: 0.08348888318165032, Loss_test: 0.0819138028305446\n",
      "869 - Loss_train: 0.08348863756273772, Loss_test: 0.08191398263972824\n",
      "870 - Loss_train: 0.08348839207164596, Loss_test: 0.08191416249904317\n",
      "871 - Loss_train: 0.08348814670828335, Loss_test: 0.08191434232917126\n",
      "872 - Loss_train: 0.08348790147222238, Loss_test: 0.08191452220092967\n",
      "873 - Loss_train: 0.08348765636376126, Loss_test: 0.08191470209433158\n",
      "874 - Loss_train: 0.08348741138282462, Loss_test: 0.08191488201246372\n",
      "875 - Loss_train: 0.083487166529372, Loss_test: 0.08191506191965417\n",
      "876 - Loss_train: 0.08348692180322356, Loss_test: 0.08191524185595905\n",
      "877 - Loss_train: 0.08348667720447595, Loss_test: 0.08191542182944725\n",
      "878 - Loss_train: 0.08348643273301366, Loss_test: 0.08191560179169823\n",
      "879 - Loss_train: 0.08348618838871935, Loss_test: 0.08191578178659473\n",
      "880 - Loss_train: 0.08348594417189936, Loss_test: 0.08191596180493096\n",
      "881 - Loss_train: 0.08348570008173681, Loss_test: 0.08191614183516192\n",
      "882 - Loss_train: 0.08348545611911097, Loss_test: 0.08191632188730752\n",
      "883 - Loss_train: 0.08348521228295429, Loss_test: 0.08191650193620748\n",
      "884 - Loss_train: 0.08348496857360065, Loss_test: 0.08191668201924583\n",
      "885 - Loss_train: 0.0834847249909924, Loss_test: 0.0819168620963704\n",
      "886 - Loss_train: 0.08348448153518141, Loss_test: 0.08191704222326984\n",
      "887 - Loss_train: 0.08348423820598605, Loss_test: 0.08191722235992041\n",
      "888 - Loss_train: 0.08348399500345056, Loss_test: 0.08191740249769025\n",
      "889 - Loss_train: 0.08348375192781149, Loss_test: 0.08191758266716773\n",
      "890 - Loss_train: 0.08348350897832593, Loss_test: 0.08191776284718495\n",
      "891 - Loss_train: 0.08348326615516133, Loss_test: 0.08191794303677932\n",
      "892 - Loss_train: 0.08348302345833271, Loss_test: 0.08191812325827647\n",
      "893 - Loss_train: 0.08348278088776755, Loss_test: 0.08191830348042144\n",
      "894 - Loss_train: 0.08348253844333203, Loss_test: 0.08191848373366989\n",
      "895 - Loss_train: 0.08348229612511443, Loss_test: 0.08191866399367521\n",
      "896 - Loss_train: 0.08348205393313313, Loss_test: 0.08191884430016597\n",
      "897 - Loss_train: 0.08348181186717378, Loss_test: 0.081919024587723\n",
      "898 - Loss_train: 0.08348156992712184, Loss_test: 0.08191920490071672\n",
      "899 - Loss_train: 0.08348132811279516, Loss_test: 0.08191938521915151\n",
      "900 - Loss_train: 0.08348108642447352, Loss_test: 0.08191956557861754\n",
      "901 - Loss_train: 0.08348084486180858, Loss_test: 0.08191974594310918\n",
      "902 - Loss_train: 0.08348060342469744, Loss_test: 0.08191992633004425\n",
      "903 - Loss_train: 0.08348036211362901, Loss_test: 0.08192010672168865\n",
      "904 - Loss_train: 0.083480120928011, Loss_test: 0.0819202871470164\n",
      "905 - Loss_train: 0.08347987986773293, Loss_test: 0.08192046756924225\n",
      "906 - Loss_train: 0.08347963893284142, Loss_test: 0.0819206480089157\n",
      "907 - Loss_train: 0.08347939812322284, Loss_test: 0.08192082848751053\n",
      "908 - Loss_train: 0.08347915743893525, Loss_test: 0.08192100895022843\n",
      "909 - Loss_train: 0.08347891687993574, Loss_test: 0.08192118946501777\n",
      "910 - Loss_train: 0.08347867644636076, Loss_test: 0.08192136996032723\n",
      "911 - Loss_train: 0.08347843613792984, Loss_test: 0.08192155048585013\n",
      "912 - Loss_train: 0.08347819595411493, Loss_test: 0.08192173102836067\n",
      "913 - Loss_train: 0.08347795589562415, Loss_test: 0.08192191159572121\n",
      "914 - Loss_train: 0.08347771596175126, Loss_test: 0.08192209216734436\n",
      "915 - Loss_train: 0.08347747615268303, Loss_test: 0.081922272755168\n",
      "916 - Loss_train: 0.08347723646841834, Loss_test: 0.08192245335968817\n",
      "917 - Loss_train: 0.0834769969088094, Loss_test: 0.08192263399550802\n",
      "918 - Loss_train: 0.0834767574741729, Loss_test: 0.08192281462629898\n",
      "919 - Loss_train: 0.08347651816377609, Loss_test: 0.08192299528730203\n",
      "920 - Loss_train: 0.08347627897792416, Loss_test: 0.08192317595464596\n",
      "921 - Loss_train: 0.08347603991650285, Loss_test: 0.08192335665103073\n",
      "922 - Loss_train: 0.08347580097979, Loss_test: 0.08192353733389648\n",
      "923 - Loss_train: 0.08347556216718105, Loss_test: 0.08192371806568854\n",
      "924 - Loss_train: 0.08347532347914198, Loss_test: 0.08192389879045976\n",
      "925 - Loss_train: 0.08347508491491322, Loss_test: 0.08192407955422924\n",
      "926 - Loss_train: 0.08347484647485393, Loss_test: 0.08192426030809248\n",
      "927 - Loss_train: 0.0834746081587326, Loss_test: 0.08192444109288514\n",
      "928 - Loss_train: 0.08347436996667569, Loss_test: 0.08192462187782715\n",
      "929 - Loss_train: 0.08347413189854075, Loss_test: 0.0819248026981047\n",
      "930 - Loss_train: 0.0834738939542309, Loss_test: 0.0819249835232733\n",
      "931 - Loss_train: 0.0834736561336427, Loss_test: 0.0819251643614365\n",
      "932 - Loss_train: 0.08347341843687571, Loss_test: 0.08192534523102342\n",
      "933 - Loss_train: 0.08347318086388177, Loss_test: 0.08192552610482254\n",
      "934 - Loss_train: 0.08347294341442073, Loss_test: 0.08192570698970572\n",
      "935 - Loss_train: 0.08347270608881331, Loss_test: 0.08192588789018677\n",
      "936 - Loss_train: 0.08347246888660313, Loss_test: 0.0819260688152394\n",
      "937 - Loss_train: 0.08347223180790758, Loss_test: 0.08192624974894017\n",
      "938 - Loss_train: 0.08347199485277909, Loss_test: 0.0819264307009254\n",
      "939 - Loss_train: 0.0834717580205428, Loss_test: 0.08192661165938389\n",
      "940 - Loss_train: 0.08347152131163764, Loss_test: 0.08192679262921565\n",
      "941 - Loss_train: 0.08347128472589482, Loss_test: 0.0819269736369468\n",
      "942 - Loss_train: 0.0834710482629382, Loss_test: 0.08192715463579678\n",
      "943 - Loss_train: 0.08347081192310031, Loss_test: 0.08192733567041388\n",
      "944 - Loss_train: 0.0834705757061488, Loss_test: 0.08192751669390773\n",
      "945 - Loss_train: 0.08347033961213246, Loss_test: 0.08192769776451733\n",
      "946 - Loss_train: 0.08347010364099205, Loss_test: 0.08192787883237257\n",
      "947 - Loss_train: 0.08346986779295049, Loss_test: 0.08192805989655191\n",
      "948 - Loss_train: 0.08346963206762198, Loss_test: 0.08192824101867281\n",
      "949 - Loss_train: 0.08346939646457752, Loss_test: 0.08192842211417435\n",
      "950 - Loss_train: 0.08346916098451956, Loss_test: 0.08192860325959286\n",
      "951 - Loss_train: 0.08346892562662303, Loss_test: 0.08192878436775546\n",
      "952 - Loss_train: 0.08346869039114786, Loss_test: 0.08192896554912352\n",
      "953 - Loss_train: 0.08346845527818407, Loss_test: 0.08192914672274489\n",
      "954 - Loss_train: 0.08346822028737237, Loss_test: 0.08192932791014806\n",
      "955 - Loss_train: 0.08346798541904293, Loss_test: 0.08192950910846314\n",
      "956 - Loss_train: 0.08346775067281358, Loss_test: 0.08192969032659538\n",
      "957 - Loss_train: 0.0834675160486647, Loss_test: 0.08192987155692555\n",
      "958 - Loss_train: 0.08346728154656896, Loss_test: 0.08193005279730507\n",
      "959 - Loss_train: 0.08346704716649801, Loss_test: 0.08193023406371464\n",
      "960 - Loss_train: 0.08346681290877686, Loss_test: 0.08193041534089403\n",
      "961 - Loss_train: 0.08346657877249036, Loss_test: 0.08193059661697416\n",
      "962 - Loss_train: 0.083466344758414, Loss_test: 0.08193077791681601\n",
      "963 - Loss_train: 0.08346611086602532, Loss_test: 0.08193095925119445\n",
      "964 - Loss_train: 0.08346587709526505, Loss_test: 0.0819311405620126\n",
      "965 - Loss_train: 0.08346564344606738, Loss_test: 0.0819313219116324\n",
      "966 - Loss_train: 0.08346540991844838, Loss_test: 0.08193150326161763\n",
      "967 - Loss_train: 0.08346517651223592, Loss_test: 0.08193168464895932\n",
      "968 - Loss_train: 0.08346494322790345, Loss_test: 0.0819318660244674\n",
      "969 - Loss_train: 0.08346471006450719, Loss_test: 0.08193204742566657\n",
      "970 - Loss_train: 0.0834644770226114, Loss_test: 0.08193222883474921\n",
      "971 - Loss_train: 0.08346424410181885, Loss_test: 0.08193241027529671\n",
      "972 - Loss_train: 0.08346401130226203, Loss_test: 0.0819325917162888\n",
      "973 - Loss_train: 0.08346377862378329, Loss_test: 0.08193277317003946\n",
      "974 - Loss_train: 0.0834635460663586, Loss_test: 0.08193295463527854\n",
      "975 - Loss_train: 0.08346331362993029, Loss_test: 0.08193313611470543\n",
      "976 - Loss_train: 0.08346308131495016, Loss_test: 0.08193331761590512\n",
      "977 - Loss_train: 0.0834628491203368, Loss_test: 0.08193349912806296\n",
      "978 - Loss_train: 0.08346261704657655, Loss_test: 0.0819336806521042\n",
      "979 - Loss_train: 0.08346238509390536, Loss_test: 0.08193386218929179\n",
      "980 - Loss_train: 0.08346215326156768, Loss_test: 0.08193404375348268\n",
      "981 - Loss_train: 0.0834619215501846, Loss_test: 0.08193422532042166\n",
      "982 - Loss_train: 0.08346168995898996, Loss_test: 0.08193440687666166\n",
      "983 - Loss_train: 0.08346145848835664, Loss_test: 0.08193458847840937\n",
      "984 - Loss_train: 0.08346122713807379, Loss_test: 0.08193477008622552\n",
      "985 - Loss_train: 0.08346099590826903, Loss_test: 0.08193495170510894\n",
      "986 - Loss_train: 0.08346076479898477, Loss_test: 0.08193513334665893\n",
      "987 - Loss_train: 0.08346053380998138, Loss_test: 0.08193531499823214\n",
      "988 - Loss_train: 0.08346030294082557, Loss_test: 0.08193549664809648\n",
      "989 - Loss_train: 0.08346007219221314, Loss_test: 0.08193567832833251\n",
      "990 - Loss_train: 0.08345984156345904, Loss_test: 0.0819358600208306\n",
      "991 - Loss_train: 0.08345961105454874, Loss_test: 0.08193604171024763\n",
      "992 - Loss_train: 0.08345938066558681, Loss_test: 0.08193622342961351\n",
      "993 - Loss_train: 0.08345915039660072, Loss_test: 0.08193640515181397\n",
      "994 - Loss_train: 0.08345892024730502, Loss_test: 0.08193658689320416\n",
      "995 - Loss_train: 0.08345869021783055, Loss_test: 0.0819367686426677\n",
      "996 - Loss_train: 0.08345846030796804, Loss_test: 0.08193695039667243\n",
      "997 - Loss_train: 0.08345823051766925, Loss_test: 0.08193713220018992\n",
      "998 - Loss_train: 0.08345800084699712, Loss_test: 0.08193731397542856\n",
      "999 - Loss_train: 0.08345777129579496, Loss_test: 0.08193749578869004\n",
      "1000 - Loss_train: 0.08345754186421155, Loss_test: 0.08193767760026903\n",
      "1001 - Loss_train: 0.0834573125517803, Loss_test: 0.08193785942349927\n",
      "1002 - Loss_train: 0.08345708335876241, Loss_test: 0.08193804127775717\n",
      "1003 - Loss_train: 0.08345685428494945, Loss_test: 0.08193822313206933\n",
      "1004 - Loss_train: 0.08345662533042264, Loss_test: 0.08193840499682166\n",
      "1005 - Loss_train: 0.08345639649513265, Loss_test: 0.08193858689274021\n",
      "1006 - Loss_train: 0.08345616777865097, Loss_test: 0.08193876876760464\n",
      "1007 - Loss_train: 0.08345593918122651, Loss_test: 0.08193895069231281\n",
      "1008 - Loss_train: 0.08345571070309737, Loss_test: 0.0819391325958739\n",
      "1009 - Loss_train: 0.08345548234358771, Loss_test: 0.08193931455061046\n",
      "1010 - Loss_train: 0.08345525410296295, Loss_test: 0.08193949650105786\n",
      "1011 - Loss_train: 0.08345502598124863, Loss_test: 0.08193967843558396\n",
      "1012 - Loss_train: 0.08345479797802133, Loss_test: 0.08193986042255838\n",
      "1013 - Loss_train: 0.08345457009336799, Loss_test: 0.08194004239995589\n",
      "1014 - Loss_train: 0.08345434232733681, Loss_test: 0.08194022439801384\n",
      "1015 - Loss_train: 0.08345411468011835, Loss_test: 0.08194040640088335\n",
      "1016 - Loss_train: 0.08345388715100471, Loss_test: 0.08194058842721696\n",
      "1017 - Loss_train: 0.08345365974034637, Loss_test: 0.08194077043871845\n",
      "1018 - Loss_train: 0.0834534324479798, Loss_test: 0.08194095250491945\n",
      "1019 - Loss_train: 0.08345320527381664, Loss_test: 0.08194113455403564\n",
      "1020 - Loss_train: 0.08345297821814777, Loss_test: 0.08194131662704202\n",
      "1021 - Loss_train: 0.08345275128039807, Loss_test: 0.08194149869014611\n",
      "1022 - Loss_train: 0.08345252446070218, Loss_test: 0.0819416807963238\n",
      "1023 - Loss_train: 0.08345229775933283, Loss_test: 0.08194186289974173\n",
      "1024 - Loss_train: 0.08345207117559737, Loss_test: 0.08194204502595842\n",
      "1025 - Loss_train: 0.08345184470980788, Loss_test: 0.08194222714019592\n",
      "1026 - Loss_train: 0.08345161836180491, Loss_test: 0.08194240929086213\n",
      "1027 - Loss_train: 0.08345139213159057, Loss_test: 0.08194259145521716\n",
      "1028 - Loss_train: 0.08345116601931972, Loss_test: 0.08194277360265785\n",
      "1029 - Loss_train: 0.0834509400245005, Loss_test: 0.08194295581512265\n",
      "1030 - Loss_train: 0.08345071414716769, Loss_test: 0.08194313797512337\n",
      "1031 - Loss_train: 0.08345048838781051, Loss_test: 0.08194332018541443\n",
      "1032 - Loss_train: 0.08345026274553143, Loss_test: 0.08194350239718996\n",
      "1033 - Loss_train: 0.08345003722077836, Loss_test: 0.08194368462139019\n",
      "1034 - Loss_train: 0.08344981181328087, Loss_test: 0.08194386686125961\n",
      "1035 - Loss_train: 0.08344958652310823, Loss_test: 0.08194404909836556\n",
      "1036 - Loss_train: 0.08344936135021278, Loss_test: 0.08194423137753144\n",
      "1037 - Loss_train: 0.08344913629435037, Loss_test: 0.08194441364151694\n",
      "1038 - Loss_train: 0.08344891135564014, Loss_test: 0.0819445959282913\n",
      "1039 - Loss_train: 0.08344868653395929, Loss_test: 0.08194477821313448\n",
      "1040 - Loss_train: 0.0834484618292946, Loss_test: 0.08194496053116179\n",
      "1041 - Loss_train: 0.08344823724153905, Loss_test: 0.0819451428485571\n",
      "1042 - Loss_train: 0.08344801277054611, Loss_test: 0.0819453251780454\n",
      "1043 - Loss_train: 0.08344778841639611, Loss_test: 0.08194550751599766\n",
      "1044 - Loss_train: 0.08344756417895487, Loss_test: 0.08194568986625317\n",
      "1045 - Loss_train: 0.0834473400582375, Loss_test: 0.08194587222536263\n",
      "1046 - Loss_train: 0.08344711605416348, Loss_test: 0.08194605460914324\n",
      "1047 - Loss_train: 0.08344689216660263, Loss_test: 0.08194623699464877\n",
      "1048 - Loss_train: 0.08344666839554499, Loss_test: 0.08194641939834209\n",
      "1049 - Loss_train: 0.0834464447408776, Loss_test: 0.08194660181149019\n",
      "1050 - Loss_train: 0.08344622120298363, Loss_test: 0.08194678422612163\n",
      "1051 - Loss_train: 0.08344599778152167, Loss_test: 0.08194696664774671\n",
      "1052 - Loss_train: 0.08344577447611862, Loss_test: 0.0819471491009253\n",
      "1053 - Loss_train: 0.0834455512866746, Loss_test: 0.08194733155449363\n",
      "1054 - Loss_train: 0.08344532821334927, Loss_test: 0.08194751402192565\n",
      "1055 - Loss_train: 0.08344510525612693, Loss_test: 0.08194769649642283\n",
      "1056 - Loss_train: 0.08344488241511674, Loss_test: 0.08194787898514796\n",
      "1057 - Loss_train: 0.08344465968982358, Loss_test: 0.08194806148477117\n",
      "1058 - Loss_train: 0.08344443708082594, Loss_test: 0.08194824400390872\n",
      "1059 - Loss_train: 0.08344421458719031, Loss_test: 0.0819484265175914\n",
      "1060 - Loss_train: 0.08344399220976796, Loss_test: 0.08194860904746448\n",
      "1061 - Loss_train: 0.08344376994814215, Loss_test: 0.08194879158819322\n",
      "1062 - Loss_train: 0.08344354780179951, Loss_test: 0.0819489741466563\n",
      "1063 - Loss_train: 0.08344332577093273, Loss_test: 0.08194915668722311\n",
      "1064 - Loss_train: 0.08344310385561632, Loss_test: 0.08194933927892713\n",
      "1065 - Loss_train: 0.08344288205608971, Loss_test: 0.08194952186729601\n",
      "1066 - Loss_train: 0.0834426603719983, Loss_test: 0.08194970445688966\n",
      "1067 - Loss_train: 0.08344243880316697, Loss_test: 0.08194988706003897\n",
      "1068 - Loss_train: 0.08344221734971898, Loss_test: 0.08195006968185621\n",
      "1069 - Loss_train: 0.08344199601148347, Loss_test: 0.08195025230524995\n",
      "1070 - Loss_train: 0.08344177478809925, Loss_test: 0.08195043495566522\n",
      "1071 - Loss_train: 0.08344155367978749, Loss_test: 0.08195061760224963\n",
      "1072 - Loss_train: 0.08344133268652229, Loss_test: 0.08195080025915007\n",
      "1073 - Loss_train: 0.08344111180844808, Loss_test: 0.08195098292853428\n",
      "1074 - Loss_train: 0.08344089104517947, Loss_test: 0.08195116562211129\n",
      "1075 - Loss_train: 0.08344067039679136, Loss_test: 0.08195134828819628\n",
      "1076 - Loss_train: 0.08344044986313331, Loss_test: 0.08195153100753826\n",
      "1077 - Loss_train: 0.08344022944413032, Loss_test: 0.08195171370538294\n",
      "1078 - Loss_train: 0.08344000913997647, Loss_test: 0.08195189643646172\n",
      "1079 - Loss_train: 0.08343978895046886, Loss_test: 0.08195207916592993\n",
      "1080 - Loss_train: 0.08343956887547067, Loss_test: 0.0819522619144168\n",
      "1081 - Loss_train: 0.08343934891538475, Loss_test: 0.08195244466139341\n",
      "1082 - Loss_train: 0.08343912906969451, Loss_test: 0.08195262742429175\n",
      "1083 - Loss_train: 0.08343890933810807, Loss_test: 0.0819528101972513\n",
      "1084 - Loss_train: 0.08343868972080824, Loss_test: 0.08195299297365122\n",
      "1085 - Loss_train: 0.08343847021771847, Loss_test: 0.08195317577938219\n",
      "1086 - Loss_train: 0.08343825082895029, Loss_test: 0.08195335857669661\n",
      "1087 - Loss_train: 0.0834380315542533, Loss_test: 0.08195354137778008\n",
      "1088 - Loss_train: 0.08343781239368878, Loss_test: 0.08195372421658316\n",
      "1089 - Loss_train: 0.08343759334751667, Loss_test: 0.08195390703985668\n",
      "1090 - Loss_train: 0.0834373744150573, Loss_test: 0.08195408989213329\n",
      "1091 - Loss_train: 0.0834371555964613, Loss_test: 0.08195427274256938\n",
      "1092 - Loss_train: 0.08343693689207028, Loss_test: 0.08195445559566544\n",
      "1093 - Loss_train: 0.08343671830121535, Loss_test: 0.08195463847434334\n",
      "1094 - Loss_train: 0.08343649982430872, Loss_test: 0.08195482134856819\n",
      "1095 - Loss_train: 0.08343628146077926, Loss_test: 0.08195500424112451\n",
      "1096 - Loss_train: 0.08343606321090441, Loss_test: 0.08195518714807719\n",
      "1097 - Loss_train: 0.0834358450749182, Loss_test: 0.08195537006337844\n",
      "1098 - Loss_train: 0.08343562705242952, Loss_test: 0.0819555529883864\n",
      "1099 - Loss_train: 0.08343540914314188, Loss_test: 0.08195573590685921\n",
      "1100 - Loss_train: 0.08343519134756612, Loss_test: 0.0819559188451447\n",
      "1101 - Loss_train: 0.08343497366529808, Loss_test: 0.08195610181602611\n",
      "1102 - Loss_train: 0.08343475609611105, Loss_test: 0.0819562847688509\n",
      "1103 - Loss_train: 0.08343453864033268, Loss_test: 0.08195646773462231\n",
      "1104 - Loss_train: 0.08343432129739388, Loss_test: 0.08195665071199132\n",
      "1105 - Loss_train: 0.08343410406741296, Loss_test: 0.0819568336925451\n",
      "1106 - Loss_train: 0.08343388695050646, Loss_test: 0.08195701670185565\n",
      "1107 - Loss_train: 0.08343366994660657, Loss_test: 0.08195719969486215\n",
      "1108 - Loss_train: 0.0834334530557636, Loss_test: 0.0819573827192561\n",
      "1109 - Loss_train: 0.08343323627791788, Loss_test: 0.08195756574133262\n",
      "1110 - Loss_train: 0.08343301961293471, Loss_test: 0.08195774877129308\n",
      "1111 - Loss_train: 0.08343280306036034, Loss_test: 0.08195793182331569\n",
      "1112 - Loss_train: 0.08343258662043493, Loss_test: 0.08195811486823662\n",
      "1113 - Loss_train: 0.08343237029315355, Loss_test: 0.08195829793333537\n",
      "1114 - Loss_train: 0.08343215407840628, Loss_test: 0.08195848100700585\n",
      "1115 - Loss_train: 0.08343193797620357, Loss_test: 0.08195866408329847\n",
      "1116 - Loss_train: 0.08343172198645966, Loss_test: 0.08195884717880132\n",
      "1117 - Loss_train: 0.08343150610950811, Loss_test: 0.0819590302717393\n",
      "1118 - Loss_train: 0.08343129034492205, Loss_test: 0.08195921339160288\n",
      "1119 - Loss_train: 0.0834310746920868, Loss_test: 0.08195939649199005\n",
      "1120 - Loss_train: 0.08343085915153076, Loss_test: 0.08195957962594999\n",
      "1121 - Loss_train: 0.08343064372309679, Loss_test: 0.08195976274948631\n",
      "1122 - Loss_train: 0.08343042840678823, Loss_test: 0.08195994590244934\n",
      "1123 - Loss_train: 0.08343021320279402, Loss_test: 0.08196012904390447\n",
      "1124 - Loss_train: 0.08342999811049381, Loss_test: 0.08196031220532735\n",
      "1125 - Loss_train: 0.08342978313003546, Loss_test: 0.08196049538486762\n",
      "1126 - Loss_train: 0.08342956826183741, Loss_test: 0.08196067856409024\n",
      "1127 - Loss_train: 0.08342935350514834, Loss_test: 0.08196086174190269\n",
      "1128 - Loss_train: 0.08342913886016559, Loss_test: 0.08196104494827658\n",
      "1129 - Loss_train: 0.08342892432690778, Loss_test: 0.08196122814867421\n",
      "1130 - Loss_train: 0.08342870990528711, Loss_test: 0.08196141136099973\n",
      "1131 - Loss_train: 0.0834284955952485, Loss_test: 0.08196159457759655\n",
      "1132 - Loss_train: 0.08342828139668228, Loss_test: 0.08196177781161666\n",
      "1133 - Loss_train: 0.08342806730963499, Loss_test: 0.08196196104850668\n",
      "1134 - Loss_train: 0.08342785333396943, Loss_test: 0.08196214429812584\n",
      "1135 - Loss_train: 0.08342763946964313, Loss_test: 0.08196232755197823\n",
      "1136 - Loss_train: 0.08342742571695405, Loss_test: 0.08196251081445913\n",
      "1137 - Loss_train: 0.08342721207507355, Loss_test: 0.08196269409217966\n",
      "1138 - Loss_train: 0.08342699854445791, Loss_test: 0.08196287737999958\n",
      "1139 - Loss_train: 0.08342678512493935, Loss_test: 0.08196306066216678\n",
      "1140 - Loss_train: 0.08342657181645331, Loss_test: 0.08196324396338016\n",
      "1141 - Loss_train: 0.08342635861906074, Loss_test: 0.08196342726915157\n",
      "1142 - Loss_train: 0.08342614553254162, Loss_test: 0.08196361058621157\n",
      "1143 - Loss_train: 0.08342593255682683, Loss_test: 0.08196379391454327\n",
      "1144 - Loss_train: 0.08342571969221979, Loss_test: 0.08196397723227937\n",
      "1145 - Loss_train: 0.0834255069383122, Loss_test: 0.08196416057390442\n",
      "1146 - Loss_train: 0.08342529429522984, Loss_test: 0.08196434391194254\n",
      "1147 - Loss_train: 0.083425081762481, Loss_test: 0.08196452727517953\n",
      "1148 - Loss_train: 0.08342486934065159, Loss_test: 0.08196471063407694\n",
      "1149 - Loss_train: 0.08342465702941981, Loss_test: 0.08196489401226482\n",
      "1150 - Loss_train: 0.08342444482827327, Loss_test: 0.08196507738122534\n",
      "1151 - Loss_train: 0.08342423273753917, Loss_test: 0.08196526078700832\n",
      "1152 - Loss_train: 0.08342402075750024, Loss_test: 0.08196544415581421\n",
      "1153 - Loss_train: 0.0834238088874236, Loss_test: 0.08196562757708081\n",
      "1154 - Loss_train: 0.08342359712789232, Loss_test: 0.08196581097271027\n",
      "1155 - Loss_train: 0.08342338547858942, Loss_test: 0.08196599440135799\n",
      "1156 - Loss_train: 0.0834231739395489, Loss_test: 0.0819661778289221\n",
      "1157 - Loss_train: 0.08342296251031361, Loss_test: 0.0819663612585048\n",
      "1158 - Loss_train: 0.08342275119087338, Loss_test: 0.08196654469868875\n",
      "1159 - Loss_train: 0.08342253998144354, Loss_test: 0.08196672813999811\n",
      "1160 - Loss_train: 0.08342232888204687, Loss_test: 0.08196691160857845\n",
      "1161 - Loss_train: 0.08342211789233048, Loss_test: 0.08196709506808665\n",
      "1162 - Loss_train: 0.08342190701251159, Loss_test: 0.08196727855357233\n",
      "1163 - Loss_train: 0.08342169624226928, Loss_test: 0.0819674620284921\n",
      "1164 - Loss_train: 0.08342148558216078, Loss_test: 0.08196764552005806\n",
      "1165 - Loss_train: 0.08342127503169354, Loss_test: 0.08196782901457285\n",
      "1166 - Loss_train: 0.08342106459068027, Loss_test: 0.08196801250835906\n",
      "1167 - Loss_train: 0.08342085425897743, Loss_test: 0.08196819602769873\n",
      "1168 - Loss_train: 0.08342064403665395, Loss_test: 0.08196837954777292\n",
      "1169 - Loss_train: 0.08342043392378337, Loss_test: 0.08196856305303582\n",
      "1170 - Loss_train: 0.08342022392046697, Loss_test: 0.08196874660432533\n",
      "1171 - Loss_train: 0.0834200140260349, Loss_test: 0.08196893014272662\n",
      "1172 - Loss_train: 0.08341980424097017, Loss_test: 0.08196911368399908\n",
      "1173 - Loss_train: 0.08341959456506032, Loss_test: 0.08196929725005202\n",
      "1174 - Loss_train: 0.08341938499864855, Loss_test: 0.08196948079775185\n",
      "1175 - Loss_train: 0.08341917554083092, Loss_test: 0.08196966437247805\n",
      "1176 - Loss_train: 0.08341896619250086, Loss_test: 0.08196984794521844\n",
      "1177 - Loss_train: 0.08341875695295713, Loss_test: 0.08197003153864893\n",
      "1178 - Loss_train: 0.08341854782202393, Loss_test: 0.08197021512846181\n",
      "1179 - Loss_train: 0.08341833879997149, Loss_test: 0.08197039872374581\n",
      "1180 - Loss_train: 0.08341812988650718, Loss_test: 0.08197058233645123\n",
      "1181 - Loss_train: 0.08341792108194625, Loss_test: 0.08197076593020818\n",
      "1182 - Loss_train: 0.0834177123859328, Loss_test: 0.08197094956047338\n",
      "1183 - Loss_train: 0.0834175037985489, Loss_test: 0.08197113318467628\n",
      "1184 - Loss_train: 0.083417295319754, Loss_test: 0.08197131681699675\n",
      "1185 - Loss_train: 0.08341708694934936, Loss_test: 0.08197150045946452\n",
      "1186 - Loss_train: 0.08341687868788797, Loss_test: 0.08197168411706386\n",
      "1187 - Loss_train: 0.08341667053456281, Loss_test: 0.08197186775094084\n",
      "1188 - Loss_train: 0.08341646248929416, Loss_test: 0.08197205141465203\n",
      "1189 - Loss_train: 0.08341625455236096, Loss_test: 0.08197223509036507\n",
      "1190 - Loss_train: 0.08341604672397593, Loss_test: 0.08197241875983177\n",
      "1191 - Loss_train: 0.08341583900334655, Loss_test: 0.08197260243761864\n",
      "1192 - Loss_train: 0.08341563139114958, Loss_test: 0.08197278613943208\n",
      "1193 - Loss_train: 0.0834154238866443, Loss_test: 0.08197296981975664\n",
      "1194 - Loss_train: 0.08341521649025044, Loss_test: 0.08197315352598561\n",
      "1195 - Loss_train: 0.08341500920161647, Loss_test: 0.08197333723204248\n",
      "1196 - Loss_train: 0.08341480202092232, Loss_test: 0.08197352094841567\n",
      "1197 - Loss_train: 0.08341459494792047, Loss_test: 0.08197370465474567\n",
      "1198 - Loss_train: 0.0834143879828263, Loss_test: 0.08197388840591222\n",
      "1199 - Loss_train: 0.08341418112546303, Loss_test: 0.08197407212657914\n",
      "1200 - Loss_train: 0.08341397437556654, Loss_test: 0.08197425585952818\n",
      "1201 - Loss_train: 0.08341376773334334, Loss_test: 0.08197443960753434\n",
      "1202 - Loss_train: 0.0834135611989411, Loss_test: 0.08197462337674376\n",
      "1203 - Loss_train: 0.0834133547715991, Loss_test: 0.08197480712021792\n",
      "1204 - Loss_train: 0.08341314845166943, Loss_test: 0.08197499089968222\n",
      "1205 - Loss_train: 0.08341294223946642, Loss_test: 0.08197517465802723\n",
      "1206 - Loss_train: 0.08341273613427265, Loss_test: 0.08197535843833373\n",
      "1207 - Loss_train: 0.08341253013635132, Loss_test: 0.08197554222512854\n",
      "1208 - Loss_train: 0.08341232424558465, Loss_test: 0.08197572601986891\n",
      "1209 - Loss_train: 0.0834121184623216, Loss_test: 0.08197590981542531\n",
      "1210 - Loss_train: 0.0834119127859398, Loss_test: 0.08197609362041568\n",
      "1211 - Loss_train: 0.08341170721674812, Loss_test: 0.08197627742215484\n",
      "1212 - Loss_train: 0.08341150175429239, Loss_test: 0.08197646125041885\n",
      "1213 - Loss_train: 0.08341129639866017, Loss_test: 0.08197664505740726\n",
      "1214 - Loss_train: 0.0834110911500336, Loss_test: 0.08197682888443528\n",
      "1215 - Loss_train: 0.08341088600809304, Loss_test: 0.08197701272615002\n",
      "1216 - Loss_train: 0.08341068097287202, Loss_test: 0.08197719657081583\n",
      "1217 - Loss_train: 0.0834104760444599, Loss_test: 0.08197738041317319\n",
      "1218 - Loss_train: 0.08341027122258853, Loss_test: 0.08197756426257762\n",
      "1219 - Loss_train: 0.08341006650740915, Loss_test: 0.08197774812255976\n",
      "1220 - Loss_train: 0.0834098618986866, Loss_test: 0.08197793197298985\n",
      "1221 - Loss_train: 0.08340965739645438, Loss_test: 0.0819781158589379\n",
      "1222 - Loss_train: 0.08340945300106169, Loss_test: 0.0819782997418553\n",
      "1223 - Loss_train: 0.08340924871187463, Loss_test: 0.08197848360694827\n",
      "1224 - Loss_train: 0.08340904452904861, Loss_test: 0.08197866749961408\n",
      "1225 - Loss_train: 0.08340884045240504, Loss_test: 0.0819788513949025\n",
      "1226 - Loss_train: 0.08340863648172078, Loss_test: 0.08197903528187568\n",
      "1227 - Loss_train: 0.08340843261710161, Loss_test: 0.08197921917978795\n",
      "1228 - Loss_train: 0.08340822885869138, Loss_test: 0.08197940310224433\n",
      "1229 - Loss_train: 0.08340802520628557, Loss_test: 0.08197958701449709\n",
      "1230 - Loss_train: 0.08340782166011199, Loss_test: 0.08197977094235823\n",
      "1231 - Loss_train: 0.08340761821981135, Loss_test: 0.08197995486461977\n",
      "1232 - Loss_train: 0.08340741488512618, Loss_test: 0.08198013879322023\n",
      "1233 - Loss_train: 0.08340721165667071, Loss_test: 0.08198032273263597\n",
      "1234 - Loss_train: 0.0834070085337587, Loss_test: 0.08198050669686645\n",
      "1235 - Loss_train: 0.08340680551631023, Loss_test: 0.08198069060828939\n",
      "1236 - Loss_train: 0.08340660260456366, Loss_test: 0.08198087458811326\n",
      "1237 - Loss_train: 0.08340639979842075, Loss_test: 0.08198105853378718\n",
      "1238 - Loss_train: 0.08340619709772366, Loss_test: 0.08198124249491873\n",
      "1239 - Loss_train: 0.08340599450292062, Loss_test: 0.08198142646772354\n",
      "1240 - Loss_train: 0.0834057920131967, Loss_test: 0.0819816104387156\n",
      "1241 - Loss_train: 0.0834055896291258, Loss_test: 0.0819817944281569\n",
      "1242 - Loss_train: 0.08340538735007683, Loss_test: 0.08198197841007886\n",
      "1243 - Loss_train: 0.083405185176278, Loss_test: 0.0819821624054323\n",
      "1244 - Loss_train: 0.08340498310804005, Loss_test: 0.08198234640313473\n",
      "1245 - Loss_train: 0.08340478114462237, Loss_test: 0.0819825304013703\n",
      "1246 - Loss_train: 0.08340457928627426, Loss_test: 0.08198271440444842\n",
      "1247 - Loss_train: 0.08340437753325831, Loss_test: 0.08198289842487455\n",
      "1248 - Loss_train: 0.0834041758852699, Loss_test: 0.08198308242473296\n",
      "1249 - Loss_train: 0.08340397434190429, Loss_test: 0.08198326646832457\n",
      "1250 - Loss_train: 0.08340377290338954, Loss_test: 0.08198345047712116\n",
      "1251 - Loss_train: 0.08340357156983118, Loss_test: 0.08198363452528705\n",
      "1252 - Loss_train: 0.08340337034098516, Loss_test: 0.08198381855609839\n",
      "1253 - Loss_train: 0.08340316921705457, Loss_test: 0.08198400258465072\n",
      "1254 - Loss_train: 0.0834029681978781, Loss_test: 0.081984186630047\n",
      "1255 - Loss_train: 0.08340276728304145, Loss_test: 0.08198437069886251\n",
      "1256 - Loss_train: 0.08340256647307447, Loss_test: 0.0819845547513463\n",
      "1257 - Loss_train: 0.08340236576727876, Loss_test: 0.08198473880178239\n",
      "1258 - Loss_train: 0.08340216516579961, Loss_test: 0.08198492286899044\n",
      "1259 - Loss_train: 0.08340196466906039, Loss_test: 0.08198510694173589\n",
      "1260 - Loss_train: 0.08340176427636933, Loss_test: 0.08198529102249612\n",
      "1261 - Loss_train: 0.08340156398798095, Loss_test: 0.08198547509295252\n",
      "1262 - Loss_train: 0.08340136380406896, Loss_test: 0.08198565918503252\n",
      "1263 - Loss_train: 0.08340116372445304, Loss_test: 0.08198584328060676\n",
      "1264 - Loss_train: 0.08340096374887823, Loss_test: 0.08198602736175203\n",
      "1265 - Loss_train: 0.08340076387705427, Loss_test: 0.0819862114592241\n",
      "1266 - Loss_train: 0.08340056410929947, Loss_test: 0.08198639557570346\n",
      "1267 - Loss_train: 0.08340036444550981, Loss_test: 0.08198657966979334\n",
      "1268 - Loss_train: 0.08340016488562564, Loss_test: 0.08198676377147883\n",
      "1269 - Loss_train: 0.08339996542957806, Loss_test: 0.0819869478909899\n",
      "1270 - Loss_train: 0.08339976607731102, Loss_test: 0.08198713202773808\n",
      "1271 - Loss_train: 0.08339956682873899, Loss_test: 0.08198731614397348\n",
      "1272 - Loss_train: 0.08339936768390864, Loss_test: 0.08198750027369624\n",
      "1273 - Loss_train: 0.08339916864270237, Loss_test: 0.08198768440723458\n",
      "1274 - Loss_train: 0.08339896970499594, Loss_test: 0.08198786853062465\n",
      "1275 - Loss_train: 0.08339877087115925, Loss_test: 0.08198805268337381\n",
      "1276 - Loss_train: 0.08339857214058106, Loss_test: 0.08198823683833296\n",
      "1277 - Loss_train: 0.08339837351325778, Loss_test: 0.0819884209791784\n",
      "1278 - Loss_train: 0.08339817498949201, Loss_test: 0.08198860515039608\n",
      "1279 - Loss_train: 0.0833979765688432, Loss_test: 0.08198878929773805\n",
      "1280 - Loss_train: 0.08339777825143987, Loss_test: 0.08198897346478323\n",
      "1281 - Loss_train: 0.08339758003734513, Loss_test: 0.08198915763391032\n",
      "1282 - Loss_train: 0.08339738192631858, Loss_test: 0.08198934181260428\n",
      "1283 - Loss_train: 0.08339718391846604, Loss_test: 0.08198952598343567\n",
      "1284 - Loss_train: 0.0833969860137442, Loss_test: 0.08198971016879292\n",
      "1285 - Loss_train: 0.08339678821226637, Loss_test: 0.0819898943413789\n",
      "1286 - Loss_train: 0.08339659051366206, Loss_test: 0.08199007853542586\n",
      "1287 - Loss_train: 0.08339639291801325, Loss_test: 0.08199026272622399\n",
      "1288 - Loss_train: 0.08339619542519726, Loss_test: 0.08199044691500396\n",
      "1289 - Loss_train: 0.08339599803471122, Loss_test: 0.08199063111873514\n",
      "1290 - Loss_train: 0.0833958007470925, Loss_test: 0.08199081532185067\n",
      "1291 - Loss_train: 0.08339560356203046, Loss_test: 0.0819909995348685\n",
      "1292 - Loss_train: 0.08339540647987355, Loss_test: 0.0819911837371686\n",
      "1293 - Loss_train: 0.08339520950012554, Loss_test: 0.08199136795712648\n",
      "1294 - Loss_train: 0.083395012623046, Loss_test: 0.08199155217307873\n",
      "1295 - Loss_train: 0.08339481584815553, Loss_test: 0.08199173640953805\n",
      "1296 - Loss_train: 0.08339461917561015, Loss_test: 0.0819919206146664\n",
      "1297 - Loss_train: 0.08339442260546044, Loss_test: 0.08199210485581407\n",
      "1298 - Loss_train: 0.08339422613755605, Loss_test: 0.08199228907562685\n",
      "1299 - Loss_train: 0.08339402977191546, Loss_test: 0.08199247332501422\n",
      "1300 - Loss_train: 0.08339383350879916, Loss_test: 0.08199265757235558\n",
      "1301 - Loss_train: 0.0833936373477295, Loss_test: 0.08199284180599468\n",
      "1302 - Loss_train: 0.08339344128861835, Loss_test: 0.0819930260637562\n",
      "1303 - Loss_train: 0.08339324533135747, Loss_test: 0.0819932103109892\n",
      "1304 - Loss_train: 0.08339304947614021, Loss_test: 0.08199339456219165\n",
      "1305 - Loss_train: 0.08339285372314469, Loss_test: 0.08199357882364651\n",
      "1306 - Loss_train: 0.08339265807210354, Loss_test: 0.08199376308462712\n",
      "1307 - Loss_train: 0.08339246252253074, Loss_test: 0.0819939473412095\n",
      "1308 - Loss_train: 0.08339226707470554, Loss_test: 0.08199413161524811\n",
      "1309 - Loss_train: 0.08339207172890056, Loss_test: 0.08199431589098087\n",
      "1310 - Loss_train: 0.08339187648440904, Loss_test: 0.08199450016477612\n",
      "1311 - Loss_train: 0.0833916813415727, Loss_test: 0.08199468443800877\n",
      "1312 - Loss_train: 0.08339148630016764, Loss_test: 0.0819948687248388\n",
      "1313 - Loss_train: 0.08339129136028058, Loss_test: 0.08199505300938967\n",
      "1314 - Loss_train: 0.08339109652217616, Loss_test: 0.08199523728986703\n",
      "1315 - Loss_train: 0.0833909017852957, Loss_test: 0.08199542158531646\n",
      "1316 - Loss_train: 0.08339070714957761, Loss_test: 0.08199560587150742\n",
      "1317 - Loss_train: 0.08339051261538057, Loss_test: 0.08199579019337834\n",
      "1318 - Loss_train: 0.08339031818221937, Loss_test: 0.0819959744787573\n",
      "1319 - Loss_train: 0.08339012385030621, Loss_test: 0.08199615877860454\n",
      "1320 - Loss_train: 0.08338992961953007, Loss_test: 0.08199634309640387\n",
      "1321 - Loss_train: 0.08338973549016014, Loss_test: 0.08199652738815826\n",
      "1322 - Loss_train: 0.08338954146139972, Loss_test: 0.08199671170991234\n",
      "1323 - Loss_train: 0.08338934753361282, Loss_test: 0.08199689603549486\n",
      "1324 - Loss_train: 0.0833891537070887, Loss_test: 0.08199708033709768\n",
      "1325 - Loss_train: 0.08338895998144366, Loss_test: 0.08199726467203555\n",
      "1326 - Loss_train: 0.08338876635634587, Loss_test: 0.08199744899803375\n",
      "1327 - Loss_train: 0.08338857283199473, Loss_test: 0.08199763331355847\n",
      "1328 - Loss_train: 0.08338837940859714, Loss_test: 0.08199781765987912\n",
      "1329 - Loss_train: 0.08338818608579021, Loss_test: 0.08199800198838098\n",
      "1330 - Loss_train: 0.08338799286343054, Loss_test: 0.081998186311027\n",
      "1331 - Loss_train: 0.08338779974151996, Loss_test: 0.08199837065391705\n",
      "1332 - Loss_train: 0.08338760672040253, Loss_test: 0.08199855500035276\n",
      "1333 - Loss_train: 0.08338741379960113, Loss_test: 0.08199873935491254\n",
      "1334 - Loss_train: 0.08338722097903019, Loss_test: 0.08199892367952598\n",
      "1335 - Loss_train: 0.08338702825908552, Loss_test: 0.08199910803377346\n",
      "1336 - Loss_train: 0.08338683563915665, Loss_test: 0.08199929239479742\n",
      "1337 - Loss_train: 0.08338664311947323, Loss_test: 0.0819994767502098\n",
      "1338 - Loss_train: 0.08338645070003584, Loss_test: 0.08199966109963464\n",
      "1339 - Loss_train: 0.08338625838069522, Loss_test: 0.08199984546010039\n",
      "1340 - Loss_train: 0.08338606616159822, Loss_test: 0.0820000298252205\n",
      "1341 - Loss_train: 0.08338587404256469, Loss_test: 0.08200021419508381\n",
      "1342 - Loss_train: 0.083385682023364, Loss_test: 0.08200039855468672\n",
      "1343 - Loss_train: 0.08338549010420547, Loss_test: 0.08200058293210988\n",
      "1344 - Loss_train: 0.0833852982851096, Loss_test: 0.08200076729271898\n",
      "1345 - Loss_train: 0.08338510656576019, Loss_test: 0.08200095168528014\n",
      "1346 - Loss_train: 0.08338491494619665, Loss_test: 0.08200113604377864\n",
      "1347 - Loss_train: 0.08338472342610001, Loss_test: 0.08200132042984318\n",
      "1348 - Loss_train: 0.08338453200601156, Loss_test: 0.08200150481219268\n",
      "1349 - Loss_train: 0.08338434068526276, Loss_test: 0.08200168919326917\n",
      "1350 - Loss_train: 0.08338414946402435, Loss_test: 0.08200187357300237\n",
      "1351 - Loss_train: 0.08338395834256485, Loss_test: 0.08200205796922154\n",
      "1352 - Loss_train: 0.08338376732027687, Loss_test: 0.08200224236623459\n",
      "1353 - Loss_train: 0.08338357639772924, Loss_test: 0.08200242674389627\n",
      "1354 - Loss_train: 0.08338338557436613, Loss_test: 0.08200261114830894\n",
      "1355 - Loss_train: 0.08338319485017148, Loss_test: 0.08200279553816371\n",
      "1356 - Loss_train: 0.08338300422545979, Loss_test: 0.08200297994522993\n",
      "1357 - Loss_train: 0.08338281369999517, Loss_test: 0.08200316432868546\n",
      "1358 - Loss_train: 0.08338262327345948, Loss_test: 0.08200334873497069\n",
      "1359 - Loss_train: 0.08338243294601758, Loss_test: 0.08200353313438566\n",
      "1360 - Loss_train: 0.0833822427176208, Loss_test: 0.08200371756346644\n",
      "1361 - Loss_train: 0.0833820525882024, Loss_test: 0.08200390195825337\n",
      "1362 - Loss_train: 0.08338186255772842, Loss_test: 0.08200408636919182\n",
      "1363 - Loss_train: 0.08338167262612614, Loss_test: 0.082004270781741\n",
      "1364 - Loss_train: 0.08338148279333141, Loss_test: 0.08200445520586466\n",
      "1365 - Loss_train: 0.08338129305944067, Loss_test: 0.08200463960291259\n",
      "1366 - Loss_train: 0.08338110342451563, Loss_test: 0.08200482402382217\n",
      "1367 - Loss_train: 0.08338091388793126, Loss_test: 0.08200500844655605\n",
      "1368 - Loss_train: 0.08338072445005341, Loss_test: 0.08200519285652921\n",
      "1369 - Loss_train: 0.08338053511065475, Loss_test: 0.08200537729232246\n",
      "1370 - Loss_train: 0.08338034586984427, Loss_test: 0.08200556171034129\n",
      "1371 - Loss_train: 0.08338015672756965, Loss_test: 0.08200574614691968\n",
      "1372 - Loss_train: 0.08337996768409936, Loss_test: 0.0820059305731984\n",
      "1373 - Loss_train: 0.08337977873871996, Loss_test: 0.08200611501332605\n",
      "1374 - Loss_train: 0.08337958989152922, Loss_test: 0.0820062994163736\n",
      "1375 - Loss_train: 0.08337940114271108, Loss_test: 0.08200648386716491\n",
      "1376 - Loss_train: 0.08337921249229964, Loss_test: 0.0820066682929817\n",
      "1377 - Loss_train: 0.08337902394005005, Loss_test: 0.08200685274169554\n",
      "1378 - Loss_train: 0.08337883548559624, Loss_test: 0.08200703717200691\n",
      "1379 - Loss_train: 0.08337864712941614, Loss_test: 0.08200722160206764\n",
      "1380 - Loss_train: 0.08337845887116971, Loss_test: 0.08200740604166518\n",
      "1381 - Loss_train: 0.08337827071088598, Loss_test: 0.0820075904889653\n",
      "1382 - Loss_train: 0.08337808264845675, Loss_test: 0.0820077749371684\n",
      "1383 - Loss_train: 0.0833778946843106, Loss_test: 0.08200795937714445\n",
      "1384 - Loss_train: 0.08337770681766227, Loss_test: 0.08200814382094393\n",
      "1385 - Loss_train: 0.08337751904884769, Loss_test: 0.08200832828242395\n",
      "1386 - Loss_train: 0.0833773313779489, Loss_test: 0.08200851271443516\n",
      "1387 - Loss_train: 0.083377143804454, Loss_test: 0.08200869717081494\n",
      "1388 - Loss_train: 0.08337695632853134, Loss_test: 0.08200888161185887\n",
      "1389 - Loss_train: 0.0833767689504717, Loss_test: 0.08200906606727228\n",
      "1390 - Loss_train: 0.08337658166962283, Loss_test: 0.08200925051584376\n",
      "1391 - Loss_train: 0.08337639448635843, Loss_test: 0.08200943499089172\n",
      "1392 - Loss_train: 0.08337620740048535, Loss_test: 0.08200961943848034\n",
      "1393 - Loss_train: 0.08337602041193877, Loss_test: 0.08200980388664549\n",
      "1394 - Loss_train: 0.08337583352100299, Loss_test: 0.08200998836243795\n",
      "1395 - Loss_train: 0.08337564672712885, Loss_test: 0.08201017280564311\n",
      "1396 - Loss_train: 0.08337546003071299, Loss_test: 0.08201035726782789\n",
      "1397 - Loss_train: 0.08337527343118104, Loss_test: 0.08201054173692583\n",
      "1398 - Loss_train: 0.08337508692864738, Loss_test: 0.08201072617784151\n",
      "1399 - Loss_train: 0.08337490052320842, Loss_test: 0.08201091065625031\n",
      "1400 - Loss_train: 0.08337471421506291, Loss_test: 0.0820110951223925\n",
      "1401 - Loss_train: 0.08337452800358724, Loss_test: 0.08201127956936358\n",
      "1402 - Loss_train: 0.08337434188906973, Loss_test: 0.08201146405252847\n",
      "1403 - Loss_train: 0.08337415587170201, Loss_test: 0.08201164850609054\n",
      "1404 - Loss_train: 0.0833739699509883, Loss_test: 0.08201183297195808\n",
      "1405 - Loss_train: 0.08337378412714144, Loss_test: 0.08201201745002569\n",
      "1406 - Loss_train: 0.08337359839999331, Loss_test: 0.08201220191359447\n",
      "1407 - Loss_train: 0.08337341276945916, Loss_test: 0.08201238637940475\n",
      "1408 - Loss_train: 0.08337322723556684, Loss_test: 0.08201257085706695\n",
      "1409 - Loss_train: 0.08337304179797073, Loss_test: 0.08201275532534881\n",
      "1410 - Loss_train: 0.08337285645678398, Loss_test: 0.0820129397989294\n",
      "1411 - Loss_train: 0.08337267121215244, Loss_test: 0.08201312424601402\n",
      "1412 - Loss_train: 0.08337248606383216, Loss_test: 0.0820133087385947\n",
      "1413 - Loss_train: 0.08337230101197046, Loss_test: 0.08201349322045999\n",
      "1414 - Loss_train: 0.08337211605639952, Loss_test: 0.08201367767868716\n",
      "1415 - Loss_train: 0.08337193119709091, Loss_test: 0.08201386216292078\n",
      "1416 - Loss_train: 0.08337174643395497, Loss_test: 0.08201404663821937\n",
      "1417 - Loss_train: 0.08337156176701822, Loss_test: 0.08201423111054193\n",
      "1418 - Loss_train: 0.08337137719614489, Loss_test: 0.08201441559366585\n",
      "1419 - Loss_train: 0.08337119272131088, Loss_test: 0.08201460007025135\n",
      "1420 - Loss_train: 0.08337100834244934, Loss_test: 0.08201478454053479\n",
      "1421 - Loss_train: 0.08337082405951125, Loss_test: 0.08201496900893313\n",
      "1422 - Loss_train: 0.0833706398724134, Loss_test: 0.08201515349296028\n",
      "1423 - Loss_train: 0.08337045578139744, Loss_test: 0.08201533797770301\n",
      "1424 - Loss_train: 0.08337027178613816, Loss_test: 0.08201552245123864\n",
      "1425 - Loss_train: 0.08337008788695652, Loss_test: 0.0820157069243917\n",
      "1426 - Loss_train: 0.08336990408308166, Loss_test: 0.08201589141473634\n",
      "1427 - Loss_train: 0.08336972037487773, Loss_test: 0.08201607587504896\n",
      "1428 - Loss_train: 0.08336953676244953, Loss_test: 0.08201626036925104\n",
      "1429 - Loss_train: 0.08336935324573977, Loss_test: 0.08201644484419378\n",
      "1430 - Loss_train: 0.08336916982448563, Loss_test: 0.08201662933552141\n",
      "1431 - Loss_train: 0.08336898649842679, Loss_test: 0.08201681379697387\n",
      "1432 - Loss_train: 0.08336880326774067, Loss_test: 0.08201699829070629\n",
      "1433 - Loss_train: 0.08336862013272668, Loss_test: 0.08201718275651673\n",
      "1434 - Loss_train: 0.08336843709270902, Loss_test: 0.08201736724380236\n",
      "1435 - Loss_train: 0.08336825414801828, Loss_test: 0.08201755173589488\n",
      "1436 - Loss_train: 0.08336807129850983, Loss_test: 0.08201773620745127\n",
      "1437 - Loss_train: 0.08336788854412912, Loss_test: 0.08201792068470674\n",
      "1438 - Loss_train: 0.08336770588494995, Loss_test: 0.08201810517195901\n",
      "1439 - Loss_train: 0.08336752332069668, Loss_test: 0.08201828964792465\n",
      "1440 - Loss_train: 0.08336734085183622, Loss_test: 0.08201847413874526\n",
      "1441 - Loss_train: 0.0833671584779525, Loss_test: 0.08201865860646086\n",
      "1442 - Loss_train: 0.08336697619863938, Loss_test: 0.08201884307781315\n",
      "1443 - Loss_train: 0.08336679401423897, Loss_test: 0.0820190275624244\n",
      "1444 - Loss_train: 0.08336661192458242, Loss_test: 0.08201921204207405\n",
      "1445 - Loss_train: 0.08336642992966235, Loss_test: 0.08201939650977243\n",
      "1446 - Loss_train: 0.08336624802956835, Loss_test: 0.08201958100396559\n",
      "1447 - Loss_train: 0.08336606622438976, Loss_test: 0.08201976547539734\n",
      "1448 - Loss_train: 0.08336588451362814, Loss_test: 0.08201994995038334\n",
      "1449 - Loss_train: 0.08336570289725354, Loss_test: 0.08202013442446565\n",
      "1450 - Loss_train: 0.08336552137548768, Loss_test: 0.08202031889873423\n",
      "1451 - Loss_train: 0.08336533994815454, Loss_test: 0.08202050338305288\n",
      "1452 - Loss_train: 0.08336515861564307, Loss_test: 0.08202068787270603\n",
      "1453 - Loss_train: 0.08336497737705521, Loss_test: 0.08202087233820561\n",
      "1454 - Loss_train: 0.08336479623283678, Loss_test: 0.08202105681454405\n",
      "1455 - Loss_train: 0.0833646151829029, Loss_test: 0.08202124128805104\n",
      "1456 - Loss_train: 0.08336443422746698, Loss_test: 0.08202142576290279\n",
      "1457 - Loss_train: 0.0833642533660041, Loss_test: 0.08202161023897453\n",
      "1458 - Loss_train: 0.08336407259854728, Loss_test: 0.08202179470853178\n",
      "1459 - Loss_train: 0.08336389192553766, Loss_test: 0.08202197918249146\n",
      "1460 - Loss_train: 0.083363711346457, Loss_test: 0.08202216365325538\n",
      "1461 - Loss_train: 0.0833635308614787, Loss_test: 0.08202234814344375\n",
      "1462 - Loss_train: 0.08336335047017818, Loss_test: 0.082022532599065\n",
      "1463 - Loss_train: 0.08336317017265243, Loss_test: 0.08202271706126124\n",
      "1464 - Loss_train: 0.08336298996899001, Loss_test: 0.0820229015389366\n",
      "1465 - Loss_train: 0.08336280985913948, Loss_test: 0.08202308600721045\n",
      "1466 - Loss_train: 0.08336262984294404, Loss_test: 0.08202327048407465\n",
      "1467 - Loss_train: 0.08336244992046112, Loss_test: 0.08202345494527558\n",
      "1468 - Loss_train: 0.08336227009162706, Loss_test: 0.08202363941784066\n",
      "1469 - Loss_train: 0.08336209035637446, Loss_test: 0.08202382388485573\n",
      "1470 - Loss_train: 0.08336191071462896, Loss_test: 0.08202400834145195\n",
      "1471 - Loss_train: 0.08336173116681257, Loss_test: 0.08202419282088971\n",
      "1472 - Loss_train: 0.08336155171199078, Loss_test: 0.08202437726711212\n",
      "1473 - Loss_train: 0.08336137235061945, Loss_test: 0.08202456173750872\n",
      "1474 - Loss_train: 0.08336119308261855, Loss_test: 0.08202474618816355\n",
      "1475 - Loss_train: 0.08336101390787524, Loss_test: 0.08202493066859777\n",
      "1476 - Loss_train: 0.08336083482646085, Loss_test: 0.08202511512341744\n",
      "1477 - Loss_train: 0.08336065583816148, Loss_test: 0.08202529958182664\n",
      "1478 - Loss_train: 0.08336047694345898, Loss_test: 0.08202548403501647\n",
      "1479 - Loss_train: 0.0833602981414331, Loss_test: 0.08202566849799287\n",
      "1480 - Loss_train: 0.08336011943243407, Loss_test: 0.0820258529525634\n",
      "1481 - Loss_train: 0.08335994081675267, Loss_test: 0.08202603741036865\n",
      "1482 - Loss_train: 0.08335976229402386, Loss_test: 0.08202622185669842\n",
      "1483 - Loss_train: 0.08335958386392096, Loss_test: 0.08202640630072132\n",
      "1484 - Loss_train: 0.08335940552696268, Loss_test: 0.08202659076516267\n",
      "1485 - Loss_train: 0.0833592272826093, Loss_test: 0.08202677520442585\n",
      "1486 - Loss_train: 0.083359049131158, Loss_test: 0.08202695965021521\n",
      "1487 - Loss_train: 0.08335887107233572, Loss_test: 0.08202714411059189\n",
      "1488 - Loss_train: 0.08335869310603958, Loss_test: 0.08202732855368575\n",
      "1489 - Loss_train: 0.0833585152324468, Loss_test: 0.08202751300941989\n",
      "1490 - Loss_train: 0.08335833745139064, Loss_test: 0.08202769742811339\n",
      "1491 - Loss_train: 0.08335815976294964, Loss_test: 0.08202788189684751\n",
      "1492 - Loss_train: 0.08335798216691279, Loss_test: 0.08202806631136436\n",
      "1493 - Loss_train: 0.08335780466334065, Loss_test: 0.08202825076871473\n",
      "1494 - Loss_train: 0.08335762725238689, Loss_test: 0.08202843520843509\n",
      "1495 - Loss_train: 0.08335744993366467, Loss_test: 0.08202861964402813\n",
      "1496 - Loss_train: 0.08335727270689507, Loss_test: 0.08202880406753912\n",
      "1497 - Loss_train: 0.08335709557248863, Loss_test: 0.08202898850242528\n",
      "1498 - Loss_train: 0.08335691853027993, Loss_test: 0.08202917294436989\n",
      "1499 - Loss_train: 0.08335674158006096, Loss_test: 0.08202935737052874\n",
      "1500 - Loss_train: 0.08335656472205621, Loss_test: 0.08202954181504138\n",
      "1501 - Loss_train: 0.08335638795620716, Loss_test: 0.0820297262343363\n",
      "1502 - Loss_train: 0.08335621128242626, Loss_test: 0.0820299106775662\n",
      "1503 - Loss_train: 0.08335603470069289, Loss_test: 0.08203009507674054\n",
      "1504 - Loss_train: 0.08335585821092235, Loss_test: 0.08203027950099885\n",
      "1505 - Loss_train: 0.08335568181273499, Loss_test: 0.08203046393320397\n",
      "1506 - Loss_train: 0.08335550550640486, Loss_test: 0.08203064834297276\n",
      "1507 - Loss_train: 0.08335532929175697, Loss_test: 0.08203083276863249\n",
      "1508 - Loss_train: 0.08335515316912881, Loss_test: 0.08203101717582044\n",
      "1509 - Loss_train: 0.08335497713790851, Loss_test: 0.0820312016002593\n",
      "1510 - Loss_train: 0.08335480119822308, Loss_test: 0.08203138601250978\n",
      "1511 - Loss_train: 0.08335462535034473, Loss_test: 0.08203157040161253\n",
      "1512 - Loss_train: 0.08335444959378475, Loss_test: 0.0820317548281323\n",
      "1513 - Loss_train: 0.08335427392901797, Loss_test: 0.08203193922839766\n",
      "1514 - Loss_train: 0.08335409835531975, Loss_test: 0.0820321236367227\n",
      "1515 - Loss_train: 0.08335392287325276, Loss_test: 0.08203230803686329\n",
      "1516 - Loss_train: 0.08335374748221486, Loss_test: 0.0820324924400808\n",
      "1517 - Loss_train: 0.08335357218251185, Loss_test: 0.08203267684055435\n",
      "1518 - Loss_train: 0.08335339697430491, Loss_test: 0.08203286125244227\n",
      "1519 - Loss_train: 0.08335322185699079, Loss_test: 0.08203304562052662\n",
      "1520 - Loss_train: 0.08335304683081257, Loss_test: 0.08203323003580848\n",
      "1521 - Loss_train: 0.08335287189573459, Loss_test: 0.08203341442086072\n",
      "1522 - Loss_train: 0.08335269705200068, Loss_test: 0.08203359880859681\n",
      "1523 - Loss_train: 0.08335252229893449, Loss_test: 0.08203378320341555\n",
      "1524 - Loss_train: 0.08335234763702899, Loss_test: 0.08203396757855125\n",
      "1525 - Loss_train: 0.08335217306610034, Loss_test: 0.08203415195539641\n",
      "1526 - Loss_train: 0.08335199858607041, Loss_test: 0.08203433635273628\n",
      "1527 - Loss_train: 0.08335182419653982, Loss_test: 0.08203452070788889\n",
      "1528 - Loss_train: 0.08335164989774646, Loss_test: 0.08203470510475323\n",
      "1529 - Loss_train: 0.0833514756899239, Loss_test: 0.08203488946673454\n",
      "1530 - Loss_train: 0.08335130157248027, Loss_test: 0.0820350738336713\n",
      "1531 - Loss_train: 0.08335112754599532, Loss_test: 0.08203525821703377\n",
      "1532 - Loss_train: 0.08335095360977557, Loss_test: 0.08203544258705564\n",
      "1533 - Loss_train: 0.08335077976401635, Loss_test: 0.08203562693932975\n",
      "1534 - Loss_train: 0.08335060600880038, Loss_test: 0.08203581131050379\n",
      "1535 - Loss_train: 0.08335043234405373, Loss_test: 0.08203599567308315\n",
      "1536 - Loss_train: 0.08335025876977638, Loss_test: 0.08203618002255939\n",
      "1537 - Loss_train: 0.08335008528584463, Loss_test: 0.08203636439785258\n",
      "1538 - Loss_train: 0.08334991189202647, Loss_test: 0.0820365487207112\n",
      "1539 - Loss_train: 0.08334973858879134, Loss_test: 0.08203673309938925\n",
      "1540 - Loss_train: 0.08334956537546509, Loss_test: 0.082036917430203\n",
      "1541 - Loss_train: 0.08334939225235598, Loss_test: 0.08203710178470285\n",
      "1542 - Loss_train: 0.08334921921926648, Loss_test: 0.08203728612442067\n",
      "1543 - Loss_train: 0.08334904627627197, Loss_test: 0.0820374704611678\n",
      "1544 - Loss_train: 0.08334887342324969, Loss_test: 0.08203765479996696\n",
      "1545 - Loss_train: 0.08334870066019738, Loss_test: 0.0820378391379722\n",
      "1546 - Loss_train: 0.08334852798705247, Loss_test: 0.08203802346328085\n",
      "1547 - Loss_train: 0.08334835540386203, Loss_test: 0.0820382078039622\n",
      "1548 - Loss_train: 0.08334818291043856, Loss_test: 0.08203839211273677\n",
      "1549 - Loss_train: 0.08334801050712315, Loss_test: 0.08203857645723428\n",
      "1550 - Loss_train: 0.08334783819326082, Loss_test: 0.0820387607732171\n",
      "1551 - Loss_train: 0.08334766596913601, Loss_test: 0.08203894509048146\n",
      "1552 - Loss_train: 0.08334749383481789, Loss_test: 0.08203912941115778\n",
      "1553 - Loss_train: 0.08334732179009946, Loss_test: 0.08203931371776668\n",
      "1554 - Loss_train: 0.08334714983468766, Loss_test: 0.0820394980386137\n",
      "1555 - Loss_train: 0.08334697796866235, Loss_test: 0.08203968232767632\n",
      "1556 - Loss_train: 0.0833468061921408, Loss_test: 0.0820398666500158\n",
      "1557 - Loss_train: 0.08334663450507249, Loss_test: 0.08204005095257044\n",
      "1558 - Loss_train: 0.08334646290759165, Loss_test: 0.08204023524932558\n",
      "1559 - Loss_train: 0.08334629139920838, Loss_test: 0.08204041954055324\n",
      "1560 - Loss_train: 0.08334611998007044, Loss_test: 0.08204060382699317\n",
      "1561 - Loss_train: 0.08334594865008416, Loss_test: 0.0820407881344595\n",
      "1562 - Loss_train: 0.08334577740933188, Loss_test: 0.08204097241173486\n",
      "1563 - Loss_train: 0.08334560625805133, Loss_test: 0.08204115670730112\n",
      "1564 - Loss_train: 0.08334543519545548, Loss_test: 0.08204134097100076\n",
      "1565 - Loss_train: 0.08334526422210506, Loss_test: 0.08204152525634409\n",
      "1566 - Loss_train: 0.08334509333781533, Loss_test: 0.08204170952765719\n",
      "1567 - Loss_train: 0.08334492254218903, Loss_test: 0.0820418937937404\n",
      "1568 - Loss_train: 0.08334475183544796, Loss_test: 0.08204207808588203\n",
      "1569 - Loss_train: 0.08334458121759715, Loss_test: 0.08204226234717289\n",
      "1570 - Loss_train: 0.083344410688414, Loss_test: 0.08204244660562297\n",
      "1571 - Loss_train: 0.08334424024809327, Loss_test: 0.08204263085690675\n",
      "1572 - Loss_train: 0.08334406989642892, Loss_test: 0.08204281511930836\n",
      "1573 - Loss_train: 0.08334389963344238, Loss_test: 0.08204299937384159\n",
      "1574 - Loss_train: 0.08334372945904879, Loss_test: 0.08204318362228367\n",
      "1575 - Loss_train: 0.08334355937330118, Loss_test: 0.08204336786697575\n",
      "1576 - Loss_train: 0.0833433893759933, Loss_test: 0.08204355212801158\n",
      "1577 - Loss_train: 0.08334321946750256, Loss_test: 0.08204373634481738\n",
      "1578 - Loss_train: 0.08334304964716147, Loss_test: 0.08204392058803837\n",
      "1579 - Loss_train: 0.0833428799151446, Loss_test: 0.08204410482251147\n",
      "1580 - Loss_train: 0.0833427102715112, Loss_test: 0.0820442890539936\n",
      "1581 - Loss_train: 0.08334254071621448, Loss_test: 0.08204447328112645\n",
      "1582 - Loss_train: 0.0833423712490113, Loss_test: 0.08204465749288412\n",
      "1583 - Loss_train: 0.08334220187008548, Loss_test: 0.08204484171844012\n",
      "1584 - Loss_train: 0.08334203257966172, Loss_test: 0.08204502594583239\n",
      "1585 - Loss_train: 0.08334186337735636, Loss_test: 0.08204521015491215\n",
      "1586 - Loss_train: 0.08334169426298674, Loss_test: 0.0820453943506593\n",
      "1587 - Loss_train: 0.08334152523638591, Loss_test: 0.08204557856354111\n",
      "1588 - Loss_train: 0.08334135629786661, Loss_test: 0.08204576277053195\n",
      "1589 - Loss_train: 0.0833411874474743, Loss_test: 0.08204594696527404\n",
      "1590 - Loss_train: 0.08334101868467524, Loss_test: 0.08204613115959888\n",
      "1591 - Loss_train: 0.08334085000971467, Loss_test: 0.08204631533864734\n",
      "1592 - Loss_train: 0.08334068142282183, Loss_test: 0.0820464995366782\n",
      "1593 - Loss_train: 0.08334051292336156, Loss_test: 0.08204668372658362\n",
      "1594 - Loss_train: 0.08334034451162271, Loss_test: 0.08204686789429395\n",
      "1595 - Loss_train: 0.0833401761875296, Loss_test: 0.08204705207969974\n",
      "1596 - Loss_train: 0.0833400079512679, Loss_test: 0.0820472362349362\n",
      "1597 - Loss_train: 0.0833398398022298, Loss_test: 0.08204742040464028\n",
      "1598 - Loss_train: 0.08333967174111685, Loss_test: 0.0820476045851166\n",
      "1599 - Loss_train: 0.08333950376721282, Loss_test: 0.0820477887394886\n",
      "1600 - Loss_train: 0.08333933588077577, Loss_test: 0.08204797290164506\n",
      "1601 - Loss_train: 0.08333916808192791, Loss_test: 0.08204815704467673\n",
      "1602 - Loss_train: 0.08333900037014016, Loss_test: 0.08204834121530653\n",
      "1603 - Loss_train: 0.08333883274567808, Loss_test: 0.08204852534358381\n",
      "1604 - Loss_train: 0.0833386652084344, Loss_test: 0.08204870948191748\n",
      "1605 - Loss_train: 0.08333849775841681, Loss_test: 0.0820488936200177\n",
      "1606 - Loss_train: 0.08333833039552963, Loss_test: 0.08204907777838016\n",
      "1607 - Loss_train: 0.08333816311995906, Loss_test: 0.0820492618991803\n",
      "1608 - Loss_train: 0.08333799593113876, Loss_test: 0.08204944600484267\n",
      "1609 - Loss_train: 0.08333782882937676, Loss_test: 0.08204963014219778\n",
      "1610 - Loss_train: 0.08333766181468887, Loss_test: 0.08204981426499729\n",
      "1611 - Loss_train: 0.08333749488686765, Loss_test: 0.08204999837178603\n",
      "1612 - Loss_train: 0.08333732804589124, Loss_test: 0.08205018248268146\n",
      "1613 - Loss_train: 0.08333716129180134, Loss_test: 0.08205036658951935\n",
      "1614 - Loss_train: 0.08333699462445986, Loss_test: 0.0820505506750539\n",
      "1615 - Loss_train: 0.083336828043952, Loss_test: 0.08205073478689419\n",
      "1616 - Loss_train: 0.08333666155034296, Loss_test: 0.08205091886675554\n",
      "1617 - Loss_train: 0.08333649514343304, Loss_test: 0.08205110297174814\n",
      "1618 - Loss_train: 0.08333632882279791, Loss_test: 0.08205128704564818\n",
      "1619 - Loss_train: 0.08333616258881398, Loss_test: 0.08205147113738612\n",
      "1620 - Loss_train: 0.08333599644127801, Loss_test: 0.08205165518826057\n",
      "1621 - Loss_train: 0.08333583038018402, Loss_test: 0.0820518392813975\n",
      "1622 - Loss_train: 0.08333566440595086, Loss_test: 0.08205202334994181\n",
      "1623 - Loss_train: 0.08333549851777378, Loss_test: 0.08205220741128938\n",
      "1624 - Loss_train: 0.08333533271620763, Loss_test: 0.08205239146639955\n",
      "1625 - Loss_train: 0.08333516700095522, Loss_test: 0.08205257553327487\n",
      "1626 - Loss_train: 0.08333500137157356, Loss_test: 0.08205275956957386\n",
      "1627 - Loss_train: 0.08333483582839078, Loss_test: 0.08205294360875386\n",
      "1628 - Loss_train: 0.08333467037138043, Loss_test: 0.08205312765009477\n",
      "1629 - Loss_train: 0.08333450500055689, Loss_test: 0.0820533116975713\n",
      "1630 - Loss_train: 0.08333433971606866, Loss_test: 0.08205349572756519\n",
      "1631 - Loss_train: 0.08333417451761231, Loss_test: 0.08205367975401978\n",
      "1632 - Loss_train: 0.08333400940482372, Loss_test: 0.0820538637774428\n",
      "1633 - Loss_train: 0.08333384437800248, Loss_test: 0.08205404779517084\n",
      "1634 - Loss_train: 0.08333367943705987, Loss_test: 0.08205423181611382\n",
      "1635 - Loss_train: 0.0833335145818813, Loss_test: 0.08205441580811199\n",
      "1636 - Loss_train: 0.08333334981254709, Loss_test: 0.08205459983147943\n",
      "1637 - Loss_train: 0.08333318512899811, Loss_test: 0.08205478382092689\n",
      "1638 - Loss_train: 0.08333302053122148, Loss_test: 0.08205496782011559\n",
      "1639 - Loss_train: 0.08333285601928253, Loss_test: 0.0820551518101031\n",
      "1640 - Loss_train: 0.08333269159271092, Loss_test: 0.08205533578872266\n",
      "1641 - Loss_train: 0.08333252725180182, Loss_test: 0.08205551978349955\n",
      "1642 - Loss_train: 0.08333236299663906, Loss_test: 0.08205570374414733\n",
      "1643 - Loss_train: 0.083332198826682, Loss_test: 0.08205588772834158\n",
      "1644 - Loss_train: 0.08333203474220248, Loss_test: 0.08205607169071602\n",
      "1645 - Loss_train: 0.08333187074309399, Loss_test: 0.08205625565174739\n",
      "1646 - Loss_train: 0.08333170682940817, Loss_test: 0.08205643960588628\n",
      "1647 - Loss_train: 0.08333154300120854, Loss_test: 0.08205662355655165\n",
      "1648 - Loss_train: 0.08333137925809728, Loss_test: 0.08205680751294073\n",
      "1649 - Loss_train: 0.08333121560047217, Loss_test: 0.08205699144142523\n",
      "1650 - Loss_train: 0.08333105202772033, Loss_test: 0.08205717538905927\n",
      "1651 - Loss_train: 0.08333088854016667, Loss_test: 0.08205735932913409\n",
      "1652 - Loss_train: 0.08333072513761547, Loss_test: 0.08205754325019171\n",
      "1653 - Loss_train: 0.08333056182014259, Loss_test: 0.08205772717655865\n",
      "1654 - Loss_train: 0.08333039858775444, Loss_test: 0.08205791109433265\n",
      "1655 - Loss_train: 0.08333023544016087, Loss_test: 0.08205809501998348\n",
      "1656 - Loss_train: 0.08333007237784336, Loss_test: 0.08205827890851206\n",
      "1657 - Loss_train: 0.08332990940017834, Loss_test: 0.08205846283059298\n",
      "1658 - Loss_train: 0.08332974650732324, Loss_test: 0.08205864670914371\n",
      "1659 - Loss_train: 0.08332958369949547, Loss_test: 0.08205883061048605\n",
      "1660 - Loss_train: 0.08332942097636588, Loss_test: 0.08205901449773485\n",
      "1661 - Loss_train: 0.08332925833793599, Loss_test: 0.08205919837664336\n",
      "1662 - Loss_train: 0.08332909578394543, Loss_test: 0.08205938225588466\n",
      "1663 - Loss_train: 0.08332893331450235, Loss_test: 0.08205956611401398\n",
      "1664 - Loss_train: 0.08332877092991252, Loss_test: 0.08205974998642995\n",
      "1665 - Loss_train: 0.08332860862986201, Loss_test: 0.0820599338666249\n",
      "1666 - Loss_train: 0.08332844641402196, Loss_test: 0.08206011770790403\n",
      "1667 - Loss_train: 0.0833282842825821, Loss_test: 0.08206030155983371\n",
      "1668 - Loss_train: 0.08332812223577944, Loss_test: 0.08206048541127077\n",
      "1669 - Loss_train: 0.08332796027332397, Loss_test: 0.08206066924842911\n",
      "1670 - Loss_train: 0.08332779839490909, Loss_test: 0.0820608530767134\n",
      "1671 - Loss_train: 0.08332763660083248, Loss_test: 0.08206103691627105\n",
      "1672 - Loss_train: 0.08332747489085664, Loss_test: 0.082061220722937\n",
      "1673 - Loss_train: 0.08332731326499378, Loss_test: 0.08206140454984072\n",
      "1674 - Loss_train: 0.08332715172326341, Loss_test: 0.08206158836676539\n",
      "1675 - Loss_train: 0.08332699026567564, Loss_test: 0.0820617721798145\n",
      "1676 - Loss_train: 0.08332682889226328, Loss_test: 0.08206195598145652\n",
      "1677 - Loss_train: 0.08332666760273544, Loss_test: 0.08206213976776745\n",
      "1678 - Loss_train: 0.08332650639718256, Loss_test: 0.0820623235723757\n",
      "1679 - Loss_train: 0.08332634527549124, Loss_test: 0.08206250735680634\n",
      "1680 - Loss_train: 0.08332618423761588, Loss_test: 0.0820626911419746\n",
      "1681 - Loss_train: 0.08332602328367088, Loss_test: 0.08206287490123822\n",
      "1682 - Loss_train: 0.08332586241339446, Loss_test: 0.08206305868267348\n",
      "1683 - Loss_train: 0.0833257016269085, Loss_test: 0.08206324244767656\n",
      "1684 - Loss_train: 0.08332554092439531, Loss_test: 0.08206342620278834\n",
      "1685 - Loss_train: 0.08332538030522013, Loss_test: 0.08206360995915295\n",
      "1686 - Loss_train: 0.08332521976962835, Loss_test: 0.08206379371255837\n",
      "1687 - Loss_train: 0.08332505931763255, Loss_test: 0.08206397744196113\n",
      "1688 - Loss_train: 0.08332489894912862, Loss_test: 0.08206416117491662\n",
      "1689 - Loss_train: 0.08332473866413408, Loss_test: 0.08206434490923596\n",
      "1690 - Loss_train: 0.08332457846252199, Loss_test: 0.082064528634729\n",
      "1691 - Loss_train: 0.08332441834429241, Loss_test: 0.0820647123570627\n",
      "1692 - Loss_train: 0.08332425830935537, Loss_test: 0.08206489606629891\n",
      "1693 - Loss_train: 0.08332409835778587, Loss_test: 0.0820650797732372\n",
      "1694 - Loss_train: 0.08332393848960133, Loss_test: 0.08206526346949282\n",
      "1695 - Loss_train: 0.08332377870474451, Loss_test: 0.0820654471717812\n",
      "1696 - Loss_train: 0.08332361900277835, Loss_test: 0.08206563084813254\n",
      "1697 - Loss_train: 0.08332345938396642, Loss_test: 0.08206581455173108\n",
      "1698 - Loss_train: 0.08332329984820441, Loss_test: 0.08206599822740995\n",
      "1699 - Loss_train: 0.08332314039556964, Loss_test: 0.08206618188668564\n",
      "1700 - Loss_train: 0.083322981025798, Loss_test: 0.08206636555633301\n",
      "1701 - Loss_train: 0.08332282173896646, Loss_test: 0.0820665492203534\n",
      "1702 - Loss_train: 0.08332266253511289, Loss_test: 0.08206673287539912\n",
      "1703 - Loss_train: 0.08332250341421012, Loss_test: 0.08206691650480556\n",
      "1704 - Loss_train: 0.08332234437599413, Loss_test: 0.08206710016385717\n",
      "1705 - Loss_train: 0.08332218542066337, Loss_test: 0.08206728379076365\n",
      "1706 - Loss_train: 0.08332202654793186, Loss_test: 0.0820674674262926\n",
      "1707 - Loss_train: 0.0833218677579205, Loss_test: 0.08206765104411122\n",
      "1708 - Loss_train: 0.0833217090505774, Loss_test: 0.08206783465664476\n",
      "1709 - Loss_train: 0.08332155042612824, Loss_test: 0.08206801827511212\n",
      "1710 - Loss_train: 0.08332139188404912, Loss_test: 0.08206820189211975\n",
      "1711 - Loss_train: 0.08332123342465061, Loss_test: 0.0820683854678847\n",
      "1712 - Loss_train: 0.08332107504776103, Loss_test: 0.08206856907609222\n",
      "1713 - Loss_train: 0.08332091675328802, Loss_test: 0.08206875266200678\n",
      "1714 - Loss_train: 0.08332075854095003, Loss_test: 0.08206893624353809\n",
      "1715 - Loss_train: 0.08332060041092022, Loss_test: 0.08206911980462886\n",
      "1716 - Loss_train: 0.08332044236321368, Loss_test: 0.08206930338829715\n",
      "1717 - Loss_train: 0.08332028439783735, Loss_test: 0.08206948694542957\n",
      "1718 - Loss_train: 0.08332012651470552, Loss_test: 0.08206967050414592\n",
      "1719 - Loss_train: 0.08331996871390683, Loss_test: 0.08206985403583172\n",
      "1720 - Loss_train: 0.08331981099528371, Loss_test: 0.08207003759531356\n",
      "1721 - Loss_train: 0.0833196533587542, Loss_test: 0.08207022112354183\n",
      "1722 - Loss_train: 0.08331949580427277, Loss_test: 0.08207040465971815\n",
      "1723 - Loss_train: 0.0833193383317353, Loss_test: 0.08207058817709272\n",
      "1724 - Loss_train: 0.08331918094098244, Loss_test: 0.08207077169575677\n",
      "1725 - Loss_train: 0.0833190236321529, Loss_test: 0.0820709552010196\n",
      "1726 - Loss_train: 0.08331886640512128, Loss_test: 0.0820711387042147\n",
      "1727 - Loss_train: 0.08331870926014487, Loss_test: 0.08207132220823596\n",
      "1728 - Loss_train: 0.08331855219698671, Loss_test: 0.0820715056825798\n",
      "1729 - Loss_train: 0.08331839521529907, Loss_test: 0.08207168918288417\n",
      "1730 - Loss_train: 0.08331823831570707, Loss_test: 0.08207187265483067\n",
      "1731 - Loss_train: 0.08331808149747318, Loss_test: 0.0820720561274108\n",
      "1732 - Loss_train: 0.08331792476086043, Loss_test: 0.08207223960488687\n",
      "1733 - Loss_train: 0.08331776810582098, Loss_test: 0.0820724230400244\n",
      "1734 - Loss_train: 0.0833176115323018, Loss_test: 0.08207260651272208\n",
      "1735 - Loss_train: 0.08331745504028659, Loss_test: 0.08207278995224872\n",
      "1736 - Loss_train: 0.08331729862969807, Loss_test: 0.08207297338911003\n",
      "1737 - Loss_train: 0.08331714230089296, Loss_test: 0.08207315682406868\n",
      "1738 - Loss_train: 0.0833169860534727, Loss_test: 0.08207334024241593\n",
      "1739 - Loss_train: 0.08331682988724515, Loss_test: 0.08207352366561375\n",
      "1740 - Loss_train: 0.08331667380234557, Loss_test: 0.08207370706999237\n",
      "1741 - Loss_train: 0.08331651779838191, Loss_test: 0.08207389047119888\n",
      "1742 - Loss_train: 0.08331636187569316, Loss_test: 0.08207407386708891\n",
      "1743 - Loss_train: 0.08331620603414673, Loss_test: 0.0820742572694472\n",
      "1744 - Loss_train: 0.08331605027371967, Loss_test: 0.08207444065473592\n",
      "1745 - Loss_train: 0.08331589459437955, Loss_test: 0.08207462402654352\n",
      "1746 - Loss_train: 0.08331573899610599, Loss_test: 0.08207480739486533\n",
      "1747 - Loss_train: 0.08331558347870666, Loss_test: 0.08207499076105694\n",
      "1748 - Loss_train: 0.08331542804238724, Loss_test: 0.08207517411819691\n",
      "1749 - Loss_train: 0.08331527268718028, Loss_test: 0.08207535746925862\n",
      "1750 - Loss_train: 0.0833151174127703, Loss_test: 0.08207554080606252\n",
      "1751 - Loss_train: 0.0833149622191551, Loss_test: 0.08207572414575112\n",
      "1752 - Loss_train: 0.08331480710611018, Loss_test: 0.08207590747529168\n",
      "1753 - Loss_train: 0.0833146520740813, Loss_test: 0.0820760907954836\n",
      "1754 - Loss_train: 0.08331449712242034, Loss_test: 0.08207627410503607\n",
      "1755 - Loss_train: 0.0833143422514256, Loss_test: 0.0820764574223197\n",
      "1756 - Loss_train: 0.08331418746106493, Loss_test: 0.08207664072582992\n",
      "1757 - Loss_train: 0.08331403275146651, Loss_test: 0.08207682401347341\n",
      "1758 - Loss_train: 0.08331387812224442, Loss_test: 0.08207700730387524\n",
      "1759 - Loss_train: 0.08331372357374585, Loss_test: 0.0820771905813975\n",
      "1760 - Loss_train: 0.0833135691054419, Loss_test: 0.0820773738520456\n",
      "1761 - Loss_train: 0.08331341471742337, Loss_test: 0.08207755710453103\n",
      "1762 - Loss_train: 0.08331326041012144, Loss_test: 0.08207774038206395\n",
      "1763 - Loss_train: 0.08331310618309397, Loss_test: 0.08207792362739581\n",
      "1764 - Loss_train: 0.08331295203640336, Loss_test: 0.08207810687519426\n",
      "1765 - Loss_train: 0.08331279796995686, Loss_test: 0.08207829009641335\n",
      "1766 - Loss_train: 0.08331264398334837, Loss_test: 0.0820784733402709\n",
      "1767 - Loss_train: 0.08331249007691757, Loss_test: 0.0820786565569764\n",
      "1768 - Loss_train: 0.08331233625060894, Loss_test: 0.08207883976988578\n",
      "1769 - Loss_train: 0.0833121825046827, Loss_test: 0.08207902298434137\n",
      "1770 - Loss_train: 0.08331202883870416, Loss_test: 0.0820792061830587\n",
      "1771 - Loss_train: 0.08331187525246213, Loss_test: 0.08207938937242674\n",
      "1772 - Loss_train: 0.0833117217462094, Loss_test: 0.08207957255849264\n",
      "1773 - Loss_train: 0.0833115683199183, Loss_test: 0.08207975574073925\n",
      "1774 - Loss_train: 0.08331141497345634, Loss_test: 0.08207993890749749\n",
      "1775 - Loss_train: 0.08331126170710444, Loss_test: 0.08208012206602659\n",
      "1776 - Loss_train: 0.08331110852045394, Loss_test: 0.08208030521924617\n",
      "1777 - Loss_train: 0.08331095541332628, Loss_test: 0.0820804883644856\n",
      "1778 - Loss_train: 0.0833108023858679, Loss_test: 0.08208067151246372\n",
      "1779 - Loss_train: 0.08331064943829897, Loss_test: 0.0820808546441863\n",
      "1780 - Loss_train: 0.08331049657030305, Loss_test: 0.08208103775714612\n",
      "1781 - Loss_train: 0.08331034378167596, Loss_test: 0.08208122088923295\n",
      "1782 - Loss_train: 0.08331019107251361, Loss_test: 0.08208140399388976\n",
      "1783 - Loss_train: 0.08331003844293573, Loss_test: 0.08208158710262085\n",
      "1784 - Loss_train: 0.08330988589304089, Loss_test: 0.08208177020507527\n",
      "1785 - Loss_train: 0.08330973342250489, Loss_test: 0.0820819532828272\n",
      "1786 - Loss_train: 0.08330958103093654, Loss_test: 0.08208213635990923\n",
      "1787 - Loss_train: 0.0833094287188413, Loss_test: 0.08208231944600074\n",
      "1788 - Loss_train: 0.08330927648599623, Loss_test: 0.08208250249547266\n",
      "1789 - Loss_train: 0.08330912433261715, Loss_test: 0.08208268555321002\n",
      "1790 - Loss_train: 0.0833089722584017, Loss_test: 0.08208286861010636\n",
      "1791 - Loss_train: 0.08330882026310185, Loss_test: 0.08208305165984177\n",
      "1792 - Loss_train: 0.08330866834692904, Loss_test: 0.08208323468314233\n",
      "1793 - Loss_train: 0.0833085165098372, Loss_test: 0.08208341771584987\n",
      "1794 - Loss_train: 0.08330836475209347, Loss_test: 0.08208360073685467\n",
      "1795 - Loss_train: 0.08330821307295665, Loss_test: 0.08208378373874194\n",
      "1796 - Loss_train: 0.0833080614731608, Loss_test: 0.08208396674752506\n",
      "1797 - Loss_train: 0.08330790995198406, Loss_test: 0.08208414972766946\n",
      "1798 - Loss_train: 0.08330775850963637, Loss_test: 0.082084332735806\n",
      "1799 - Loss_train: 0.08330760714634244, Loss_test: 0.0820845156980645\n",
      "1800 - Loss_train: 0.08330745586189661, Loss_test: 0.08208469868299123\n",
      "1801 - Loss_train: 0.08330730465596771, Loss_test: 0.08208488164592635\n",
      "1802 - Loss_train: 0.08330715352879725, Loss_test: 0.08208506459925836\n",
      "1803 - Loss_train: 0.08330700248025812, Loss_test: 0.08208524754597707\n",
      "1804 - Loss_train: 0.08330685151029305, Loss_test: 0.08208543048444115\n",
      "1805 - Loss_train: 0.0833067006189346, Loss_test: 0.08208561341671772\n",
      "1806 - Loss_train: 0.08330654980643516, Loss_test: 0.08208579634490437\n",
      "1807 - Loss_train: 0.08330639907204158, Loss_test: 0.08208597924537168\n",
      "1808 - Loss_train: 0.08330624841624718, Loss_test: 0.08208616216475234\n",
      "1809 - Loss_train: 0.08330609783904638, Loss_test: 0.08208634506161344\n",
      "1810 - Loss_train: 0.08330594734019194, Loss_test: 0.0820865279510282\n",
      "1811 - Loss_train: 0.08330579691951973, Loss_test: 0.0820867108398442\n",
      "1812 - Loss_train: 0.08330564657733926, Loss_test: 0.08208689371204855\n",
      "1813 - Loss_train: 0.08330549631343044, Loss_test: 0.0820870765779545\n",
      "1814 - Loss_train: 0.08330534612743766, Loss_test: 0.08208725944476854\n",
      "1815 - Loss_train: 0.08330519601968249, Loss_test: 0.08208744229024102\n",
      "1816 - Loss_train: 0.08330504599029877, Loss_test: 0.08208762512803779\n",
      "1817 - Loss_train: 0.08330489603889721, Loss_test: 0.08208780796886032\n",
      "1818 - Loss_train: 0.08330474616536905, Loss_test: 0.08208799080032823\n",
      "1819 - Loss_train: 0.08330459636987418, Loss_test: 0.08208817361006351\n",
      "1820 - Loss_train: 0.0833044466525857, Loss_test: 0.08208835642094454\n",
      "1821 - Loss_train: 0.08330429701297289, Loss_test: 0.08208853921022043\n",
      "1822 - Loss_train: 0.08330414745126152, Loss_test: 0.08208872202367325\n",
      "1823 - Loss_train: 0.08330399796738666, Loss_test: 0.08208890479317008\n",
      "1824 - Loss_train: 0.08330384856137468, Loss_test: 0.08208908757194845\n",
      "1825 - Loss_train: 0.08330369923310549, Loss_test: 0.0820892703545816\n",
      "1826 - Loss_train: 0.08330354998250086, Loss_test: 0.08208945310862453\n",
      "1827 - Loss_train: 0.08330340080962048, Loss_test: 0.08208963584742478\n",
      "1828 - Loss_train: 0.08330325171457573, Loss_test: 0.08208981859977303\n",
      "1829 - Loss_train: 0.08330310269693197, Loss_test: 0.08209000134143447\n",
      "1830 - Loss_train: 0.0833029537569306, Loss_test: 0.08209018406004892\n",
      "1831 - Loss_train: 0.08330280489464008, Loss_test: 0.08209036677574684\n",
      "1832 - Loss_train: 0.08330265610956837, Loss_test: 0.0820905494781349\n",
      "1833 - Loss_train: 0.08330250740221089, Loss_test: 0.08209073218762952\n",
      "1834 - Loss_train: 0.08330235877224657, Loss_test: 0.08209091486358411\n",
      "1835 - Loss_train: 0.08330221021961182, Loss_test: 0.0820910975714496\n",
      "1836 - Loss_train: 0.08330206174418235, Loss_test: 0.0820912802490346\n",
      "1837 - Loss_train: 0.08330191334616933, Loss_test: 0.0820914628877612\n",
      "1838 - Loss_train: 0.08330176502525205, Loss_test: 0.08209164556878855\n",
      "1839 - Loss_train: 0.08330161678170192, Loss_test: 0.08209182821303665\n",
      "1840 - Loss_train: 0.08330146861507937, Loss_test: 0.08209201084693021\n",
      "1841 - Loss_train: 0.08330132052582069, Loss_test: 0.08209219348258605\n",
      "1842 - Loss_train: 0.0833011725134403, Loss_test: 0.08209237611385166\n",
      "1843 - Loss_train: 0.08330102457793413, Loss_test: 0.08209255870883807\n",
      "1844 - Loss_train: 0.08330087671983752, Loss_test: 0.08209274133322303\n",
      "1845 - Loss_train: 0.08330072893844923, Loss_test: 0.08209292392842289\n",
      "1846 - Loss_train: 0.08330058123422807, Loss_test: 0.08209310651828405\n",
      "1847 - Loss_train: 0.08330043360666085, Loss_test: 0.08209328907822973\n",
      "1848 - Loss_train: 0.08330028605590523, Loss_test: 0.08209347166511025\n",
      "1849 - Loss_train: 0.08330013858196819, Loss_test: 0.08209365422481907\n",
      "1850 - Loss_train: 0.08329999118484438, Loss_test: 0.08209383679044775\n",
      "1851 - Loss_train: 0.0832998438644097, Loss_test: 0.08209401932700565\n",
      "1852 - Loss_train: 0.08329969662061522, Loss_test: 0.08209420185848348\n",
      "1853 - Loss_train: 0.0832995494534611, Loss_test: 0.08209438438802799\n",
      "1854 - Loss_train: 0.08329940236297462, Loss_test: 0.08209456691433072\n",
      "1855 - Loss_train: 0.08329925534916968, Loss_test: 0.08209474941262923\n",
      "1856 - Loss_train: 0.08329910841177476, Loss_test: 0.08209493191417092\n",
      "1857 - Loss_train: 0.08329896155083333, Loss_test: 0.0820951143980087\n",
      "1858 - Loss_train: 0.083298814766277, Loss_test: 0.08209529688948028\n",
      "1859 - Loss_train: 0.08329866805844874, Loss_test: 0.08209547936127415\n",
      "1860 - Loss_train: 0.08329852142669643, Loss_test: 0.08209566182968299\n",
      "1861 - Loss_train: 0.08329837487147126, Loss_test: 0.08209584428434663\n",
      "1862 - Loss_train: 0.08329822839258912, Loss_test: 0.08209602672929821\n",
      "1863 - Loss_train: 0.08329808198993782, Loss_test: 0.08209620916913714\n",
      "1864 - Loss_train: 0.08329793566323168, Loss_test: 0.08209639160889871\n",
      "1865 - Loss_train: 0.08329778941271657, Loss_test: 0.08209657402237268\n",
      "1866 - Loss_train: 0.08329764323832817, Loss_test: 0.08209675641720889\n",
      "1867 - Loss_train: 0.08329749714002356, Loss_test: 0.08209693882648156\n",
      "1868 - Loss_train: 0.08329735111786228, Loss_test: 0.08209712121671572\n",
      "1869 - Loss_train: 0.08329720517160515, Loss_test: 0.08209730359724138\n",
      "1870 - Loss_train: 0.08329705930152442, Loss_test: 0.08209748597528004\n",
      "1871 - Loss_train: 0.08329691350729979, Loss_test: 0.08209766834558423\n",
      "1872 - Loss_train: 0.08329676778884805, Loss_test: 0.08209785068831048\n",
      "1873 - Loss_train: 0.08329662214628167, Loss_test: 0.08209803304516498\n",
      "1874 - Loss_train: 0.08329647657951321, Loss_test: 0.08209821537183433\n",
      "1875 - Loss_train: 0.08329633108862358, Loss_test: 0.08209839771294561\n",
      "1876 - Loss_train: 0.08329618567356656, Loss_test: 0.08209858002907013\n",
      "1877 - Loss_train: 0.08329604033396575, Loss_test: 0.08209876233675045\n",
      "1878 - Loss_train: 0.08329589507000092, Loss_test: 0.08209894463333406\n",
      "1879 - Loss_train: 0.08329574988195237, Loss_test: 0.08209912691833049\n",
      "1880 - Loss_train: 0.08329560476911606, Loss_test: 0.08209930920127721\n",
      "1881 - Loss_train: 0.08329545973187087, Loss_test: 0.08209949147738246\n",
      "1882 - Loss_train: 0.08329531477009113, Loss_test: 0.08209967373167341\n",
      "1883 - Loss_train: 0.08329516988379, Loss_test: 0.082099855997839\n",
      "1884 - Loss_train: 0.08329502507302539, Loss_test: 0.08210003824262989\n",
      "1885 - Loss_train: 0.08329488033749669, Loss_test: 0.08210022046159754\n",
      "1886 - Loss_train: 0.08329473567738824, Loss_test: 0.08210040270363354\n",
      "1887 - Loss_train: 0.0832945910925624, Loss_test: 0.08210058490504091\n",
      "1888 - Loss_train: 0.08329444658299535, Loss_test: 0.08210076711844096\n",
      "1889 - Loss_train: 0.0832943021486094, Loss_test: 0.08210094930181402\n",
      "1890 - Loss_train: 0.08329415778939908, Loss_test: 0.08210113151162787\n",
      "1891 - Loss_train: 0.08329401350548345, Loss_test: 0.08210131368095613\n",
      "1892 - Loss_train: 0.08329386929677612, Loss_test: 0.08210149584636665\n",
      "1893 - Loss_train: 0.08329372516310891, Loss_test: 0.08210167799677465\n",
      "1894 - Loss_train: 0.08329358110424934, Loss_test: 0.08210186015642482\n",
      "1895 - Loss_train: 0.08329343712033835, Loss_test: 0.0821020422870062\n",
      "1896 - Loss_train: 0.08329329321141635, Loss_test: 0.0821022244198251\n",
      "1897 - Loss_train: 0.08329314937759406, Loss_test: 0.08210240654012958\n",
      "1898 - Loss_train: 0.08329300561840484, Loss_test: 0.08210258864732724\n",
      "1899 - Loss_train: 0.08329286193409023, Loss_test: 0.08210277075505866\n",
      "1900 - Loss_train: 0.08329271832457896, Loss_test: 0.08210295284473991\n",
      "1901 - Loss_train: 0.083292574789705, Loss_test: 0.08210313491581309\n",
      "1902 - Loss_train: 0.08329243132962591, Loss_test: 0.08210331699324815\n",
      "1903 - Loss_train: 0.08329228794429173, Loss_test: 0.08210349905987353\n",
      "1904 - Loss_train: 0.08329214463365502, Loss_test: 0.08210368110570998\n",
      "1905 - Loss_train: 0.08329200139773996, Loss_test: 0.08210386315331428\n",
      "1906 - Loss_train: 0.08329185823636556, Loss_test: 0.08210404517909935\n",
      "1907 - Loss_train: 0.0832917151495637, Loss_test: 0.08210422720562359\n",
      "1908 - Loss_train: 0.08329157213695551, Loss_test: 0.08210440921823674\n",
      "1909 - Loss_train: 0.08329142919884336, Loss_test: 0.08210459122692143\n",
      "1910 - Loss_train: 0.08329128633540345, Loss_test: 0.08210477321288755\n",
      "1911 - Loss_train: 0.08329114354600765, Loss_test: 0.0821049552045618\n",
      "1912 - Loss_train: 0.08329100083097367, Loss_test: 0.0821051371838406\n",
      "1913 - Loss_train: 0.08329085819023922, Loss_test: 0.08210531913892424\n",
      "1914 - Loss_train: 0.08329071562377403, Loss_test: 0.08210550108470067\n",
      "1915 - Loss_train: 0.08329057313154355, Loss_test: 0.08210568303638835\n",
      "1916 - Loss_train: 0.08329043071337187, Loss_test: 0.08210586495982004\n",
      "1917 - Loss_train: 0.08329028836952598, Loss_test: 0.08210604689173787\n",
      "1918 - Loss_train: 0.08329014609960314, Loss_test: 0.08210622881007165\n",
      "1919 - Loss_train: 0.08329000390407514, Loss_test: 0.0821064107099582\n",
      "1920 - Loss_train: 0.08328986178229586, Loss_test: 0.08210659261060192\n",
      "1921 - Loss_train: 0.08328971973443934, Loss_test: 0.08210677448380378\n",
      "1922 - Loss_train: 0.08328957776044883, Loss_test: 0.08210695636632566\n",
      "1923 - Loss_train: 0.08328943586069595, Loss_test: 0.08210713822000379\n",
      "1924 - Loss_train: 0.08328929403459674, Loss_test: 0.08210732008509748\n",
      "1925 - Loss_train: 0.08328915228231128, Loss_test: 0.0821075019177817\n",
      "1926 - Loss_train: 0.08328901060407394, Loss_test: 0.08210768375782143\n",
      "1927 - Loss_train: 0.08328886899953376, Loss_test: 0.08210786556858016\n",
      "1928 - Loss_train: 0.08328872746870823, Loss_test: 0.0821080473846889\n",
      "1929 - Loss_train: 0.08328858601133582, Loss_test: 0.08210822919494663\n",
      "1930 - Loss_train: 0.08328844462757072, Loss_test: 0.082108410982904\n",
      "1931 - Loss_train: 0.08328830331731042, Loss_test: 0.0821085927600606\n",
      "1932 - Loss_train: 0.08328816208082711, Loss_test: 0.08210877453952699\n",
      "1933 - Loss_train: 0.083288020917708, Loss_test: 0.0821089563001721\n",
      "1934 - Loss_train: 0.08328787982829522, Loss_test: 0.08210913804811204\n",
      "1935 - Loss_train: 0.08328773881211064, Loss_test: 0.08210931979411278\n",
      "1936 - Loss_train: 0.08328759786933271, Loss_test: 0.08210950152563838\n",
      "1937 - Loss_train: 0.08328745700019143, Loss_test: 0.08210968324611954\n",
      "1938 - Loss_train: 0.08328731620417855, Loss_test: 0.08210986496563612\n",
      "1939 - Loss_train: 0.08328717548140122, Loss_test: 0.08211004665194457\n",
      "1940 - Loss_train: 0.0832870348318895, Loss_test: 0.0821102283463999\n",
      "1941 - Loss_train: 0.08328689425581347, Loss_test: 0.08211041003557229\n",
      "1942 - Loss_train: 0.08328675375285116, Loss_test: 0.08211059169444322\n",
      "1943 - Loss_train: 0.08328661332297317, Loss_test: 0.08211077335270298\n",
      "1944 - Loss_train: 0.08328647296628552, Loss_test: 0.08211095500450445\n",
      "1945 - Loss_train: 0.08328633268269507, Loss_test: 0.08211113663806847\n",
      "1946 - Loss_train: 0.083286192471928, Loss_test: 0.0821113182777985\n",
      "1947 - Loss_train: 0.08328605233442055, Loss_test: 0.08211149987991051\n",
      "1948 - Loss_train: 0.08328591226952617, Loss_test: 0.08211168150259372\n",
      "1949 - Loss_train: 0.08328577227758055, Loss_test: 0.08211186308997225\n",
      "1950 - Loss_train: 0.08328563235878363, Loss_test: 0.08211204467304256\n",
      "1951 - Loss_train: 0.08328549251250625, Loss_test: 0.08211222625022678\n",
      "1952 - Loss_train: 0.08328535273907126, Loss_test: 0.08211240781610668\n",
      "1953 - Loss_train: 0.08328521303865986, Loss_test: 0.08211258936558175\n",
      "1954 - Loss_train: 0.08328507341071693, Loss_test: 0.08211277091462948\n",
      "1955 - Loss_train: 0.08328493385543241, Loss_test: 0.08211295243905894\n",
      "1956 - Loss_train: 0.08328479437307314, Loss_test: 0.08211313396645678\n",
      "1957 - Loss_train: 0.08328465496304964, Loss_test: 0.08211331547539015\n",
      "1958 - Loss_train: 0.08328451562567327, Loss_test: 0.08211349697148657\n",
      "1959 - Loss_train: 0.0832843763608703, Loss_test: 0.08211367846203797\n",
      "1960 - Loss_train: 0.08328423716843937, Loss_test: 0.08211385995142832\n",
      "1961 - Loss_train: 0.08328409804850766, Loss_test: 0.08211404141587968\n",
      "1962 - Loss_train: 0.08328395900121603, Loss_test: 0.08211422287531792\n",
      "1963 - Loss_train: 0.08328382002594015, Loss_test: 0.08211440431928554\n",
      "1964 - Loss_train: 0.08328368112309406, Loss_test: 0.08211458575828258\n",
      "1965 - Loss_train: 0.08328354229276269, Loss_test: 0.0821147671849079\n",
      "1966 - Loss_train: 0.08328340353448463, Loss_test: 0.08211494860234511\n",
      "1967 - Loss_train: 0.08328326484853357, Loss_test: 0.08211513000925512\n",
      "1968 - Loss_train: 0.08328312623477388, Loss_test: 0.08211531139071797\n",
      "1969 - Loss_train: 0.08328298769309944, Loss_test: 0.08211549277625442\n",
      "1970 - Loss_train: 0.08328284922354195, Loss_test: 0.08211567415581167\n",
      "1971 - Loss_train: 0.08328271082632459, Loss_test: 0.08211585551769182\n",
      "1972 - Loss_train: 0.08328257250116403, Loss_test: 0.08211603686883082\n",
      "1973 - Loss_train: 0.08328243424773618, Loss_test: 0.08211621820642735\n",
      "1974 - Loss_train: 0.08328229606650647, Loss_test: 0.08211639951790126\n",
      "1975 - Loss_train: 0.08328215795695479, Loss_test: 0.08211658084227552\n",
      "1976 - Loss_train: 0.08328201991923699, Loss_test: 0.08211676214921455\n",
      "1977 - Loss_train: 0.08328188195346911, Loss_test: 0.08211694343863969\n",
      "1978 - Loss_train: 0.08328174405945714, Loss_test: 0.08211712472890387\n",
      "1979 - Loss_train: 0.08328160623720246, Loss_test: 0.08211730599877348\n",
      "1980 - Loss_train: 0.0832814684868148, Loss_test: 0.08211748726246462\n",
      "1981 - Loss_train: 0.08328133080807669, Loss_test: 0.0821176685162521\n",
      "1982 - Loss_train: 0.08328119320097785, Loss_test: 0.08211784976383597\n",
      "1983 - Loss_train: 0.08328105566583881, Loss_test: 0.08211803099526366\n",
      "1984 - Loss_train: 0.08328091820220646, Loss_test: 0.08211821221382165\n",
      "1985 - Loss_train: 0.08328078080982085, Loss_test: 0.08211839340417977\n",
      "1986 - Loss_train: 0.08328064348917542, Loss_test: 0.08211857461246687\n",
      "1987 - Loss_train: 0.08328050623989723, Loss_test: 0.0821187557982949\n",
      "1988 - Loss_train: 0.08328036906234225, Loss_test: 0.08211893696530927\n",
      "1989 - Loss_train: 0.08328023195586493, Loss_test: 0.08211911813064587\n",
      "1990 - Loss_train: 0.08328009492104303, Loss_test: 0.08211929928467426\n",
      "1991 - Loss_train: 0.08327995795721338, Loss_test: 0.08211948042338475\n",
      "1992 - Loss_train: 0.0832798210647667, Loss_test: 0.08211966156312975\n",
      "1993 - Loss_train: 0.0832796842435741, Loss_test: 0.08211984266508501\n",
      "1994 - Loss_train: 0.08327954749359037, Loss_test: 0.08212002377796271\n",
      "1995 - Loss_train: 0.08327941081483417, Loss_test: 0.08212020488145697\n",
      "1996 - Loss_train: 0.08327927420712398, Loss_test: 0.08212038595486253\n",
      "1997 - Loss_train: 0.08327913767082568, Loss_test: 0.08212056703790215\n",
      "1998 - Loss_train: 0.08327900120540561, Loss_test: 0.08212074809543633\n",
      "1999 - Loss_train: 0.08327886481101522, Loss_test: 0.08212092914484734\n",
      "2000 - Loss_train: 0.08327872848761039, Loss_test: 0.0821211101700262\n",
      "2001 - Loss_train: 0.08327859223531303, Loss_test: 0.08212129122047665\n",
      "2002 - Loss_train: 0.08327845605387654, Loss_test: 0.08212147221728389\n",
      "2003 - Loss_train: 0.08327831994328679, Loss_test: 0.08212165323608817\n",
      "2004 - Loss_train: 0.08327818390374286, Loss_test: 0.0821218342298073\n",
      "2005 - Loss_train: 0.0832780479347461, Loss_test: 0.0821220152015914\n",
      "2006 - Loss_train: 0.08327791203679719, Loss_test: 0.0821221961821596\n",
      "2007 - Loss_train: 0.08327777620933473, Loss_test: 0.0821223771310702\n",
      "2008 - Loss_train: 0.0832776404526028, Loss_test: 0.08212255808749216\n",
      "2009 - Loss_train: 0.08327750476677186, Loss_test: 0.08212273902103076\n",
      "2010 - Loss_train: 0.08327736915136304, Loss_test: 0.08212291993474702\n",
      "2011 - Loss_train: 0.08327723360643997, Loss_test: 0.08212310086030783\n",
      "2012 - Loss_train: 0.08327709813241509, Loss_test: 0.08212328176411338\n",
      "2013 - Loss_train: 0.08327696272867627, Loss_test: 0.08212346264643358\n",
      "2014 - Loss_train: 0.08327682739541818, Loss_test: 0.08212364352494878\n",
      "2015 - Loss_train: 0.08327669213271642, Loss_test: 0.08212382439915598\n",
      "2016 - Loss_train: 0.08327655694034777, Loss_test: 0.0821240052620695\n",
      "2017 - Loss_train: 0.08327642181832841, Loss_test: 0.08212418609948363\n",
      "2018 - Loss_train: 0.08327628676673592, Loss_test: 0.08212436692751407\n",
      "2019 - Loss_train: 0.08327615178533891, Loss_test: 0.08212454775538948\n",
      "2020 - Loss_train: 0.08327601687418713, Loss_test: 0.08212472856201895\n",
      "2021 - Loss_train: 0.08327588203327975, Loss_test: 0.08212490936493427\n",
      "2022 - Loss_train: 0.08327574726253249, Loss_test: 0.08212509014768175\n",
      "2023 - Loss_train: 0.08327561256199162, Loss_test: 0.08212527092092067\n",
      "2024 - Loss_train: 0.08327547793171164, Loss_test: 0.08212545168595634\n",
      "2025 - Loss_train: 0.08327534337147832, Loss_test: 0.08212563243443777\n",
      "2026 - Loss_train: 0.08327520888110496, Loss_test: 0.08212581316680531\n",
      "2027 - Loss_train: 0.08327507446083532, Loss_test: 0.0821259939069593\n",
      "2028 - Loss_train: 0.08327494011076512, Loss_test: 0.08212617460823876\n",
      "2029 - Loss_train: 0.08327480583046752, Loss_test: 0.08212635532601284\n",
      "2030 - Loss_train: 0.08327467161999148, Loss_test: 0.08212653601882504\n",
      "2031 - Loss_train: 0.08327453747945805, Loss_test: 0.08212671668077115\n",
      "2032 - Loss_train: 0.08327440340870784, Loss_test: 0.0821268973607134\n",
      "2033 - Loss_train: 0.08327426940769281, Loss_test: 0.08212707802073375\n",
      "2034 - Loss_train: 0.0832741354764858, Loss_test: 0.08212725866711676\n",
      "2035 - Loss_train: 0.08327400161495986, Loss_test: 0.08212743929032547\n",
      "2036 - Loss_train: 0.08327386782315715, Loss_test: 0.08212761992941557\n",
      "2037 - Loss_train: 0.0832737341010413, Loss_test: 0.08212780053493508\n",
      "2038 - Loss_train: 0.08327360044844467, Loss_test: 0.0821279811329206\n",
      "2039 - Loss_train: 0.0832734668654623, Loss_test: 0.08212816170392806\n",
      "2040 - Loss_train: 0.08327333335194376, Loss_test: 0.08212834229623417\n",
      "2041 - Loss_train: 0.08327319990793815, Loss_test: 0.08212852284158362\n",
      "2042 - Loss_train: 0.08327306653340236, Loss_test: 0.08212870339626022\n",
      "2043 - Loss_train: 0.0832729332282879, Loss_test: 0.08212888393881596\n",
      "2044 - Loss_train: 0.0832727999925934, Loss_test: 0.0821290644579808\n",
      "2045 - Loss_train: 0.08327266682625294, Loss_test: 0.08212924496953372\n",
      "2046 - Loss_train: 0.08327253372926002, Loss_test: 0.08212942547899978\n",
      "2047 - Loss_train: 0.08327240070157597, Loss_test: 0.0821296059715184\n",
      "2048 - Loss_train: 0.08327226774312892, Loss_test: 0.08212978644816761\n",
      "2049 - Loss_train: 0.08327213485386677, Loss_test: 0.08212996691401908\n",
      "2050 - Loss_train: 0.08327200203396669, Loss_test: 0.08213014736018348\n",
      "2051 - Loss_train: 0.08327186928322701, Loss_test: 0.08213032780881094\n",
      "2052 - Loss_train: 0.0832717366014329, Loss_test: 0.08213050823528442\n",
      "2053 - Loss_train: 0.08327160398865724, Loss_test: 0.08213068863740319\n",
      "2054 - Loss_train: 0.08327147144518994, Loss_test: 0.08213086905702636\n",
      "2055 - Loss_train: 0.08327133897042661, Loss_test: 0.08213104944876008\n",
      "2056 - Loss_train: 0.08327120656481875, Loss_test: 0.08213122981057232\n",
      "2057 - Loss_train: 0.08327107422801866, Loss_test: 0.08213141019978554\n",
      "2058 - Loss_train: 0.08327094196002785, Loss_test: 0.082131590542441\n",
      "2059 - Loss_train: 0.08327080976117307, Loss_test: 0.08213177091130332\n",
      "2060 - Loss_train: 0.08327067763083167, Loss_test: 0.08213195121706994\n",
      "2061 - Loss_train: 0.0832705455692687, Loss_test: 0.08213213155325999\n",
      "2062 - Loss_train: 0.08327041357651965, Loss_test: 0.08213231185707279\n",
      "2063 - Loss_train: 0.08327028165246764, Loss_test: 0.08213249214761761\n",
      "2064 - Loss_train: 0.08327014979731569, Loss_test: 0.08213267243959212\n",
      "2065 - Loss_train: 0.08327001801080923, Loss_test: 0.08213285270426987\n",
      "2066 - Loss_train: 0.08326988629288774, Loss_test: 0.08213303296531843\n",
      "2067 - Loss_train: 0.08326975464337119, Loss_test: 0.08213321320258321\n",
      "2068 - Loss_train: 0.08326962306248882, Loss_test: 0.08213339343483611\n",
      "2069 - Loss_train: 0.08326949154990933, Loss_test: 0.08213357365878159\n",
      "2070 - Loss_train: 0.08326936010586462, Loss_test: 0.08213375387112754\n",
      "2071 - Loss_train: 0.08326922873021175, Loss_test: 0.08213393405941016\n",
      "2072 - Loss_train: 0.08326909742296684, Loss_test: 0.08213411424701861\n",
      "2073 - Loss_train: 0.08326896618412882, Loss_test: 0.08213429442987281\n",
      "2074 - Loss_train: 0.0832688350138051, Loss_test: 0.08213447457472442\n",
      "2075 - Loss_train: 0.08326870391148239, Loss_test: 0.08213465472215727\n",
      "2076 - Loss_train: 0.08326857287744709, Loss_test: 0.08213483485873534\n",
      "2077 - Loss_train: 0.08326844191186474, Loss_test: 0.08213501498542357\n",
      "2078 - Loss_train: 0.08326831101421789, Loss_test: 0.08213519509238383\n",
      "2079 - Loss_train: 0.08326818018470267, Loss_test: 0.08213537518503036\n",
      "2080 - Loss_train: 0.08326804942332884, Loss_test: 0.08213555527323761\n",
      "2081 - Loss_train: 0.08326791872993632, Loss_test: 0.08213573533585464\n",
      "2082 - Loss_train: 0.08326778810493297, Loss_test: 0.08213591539741572\n",
      "2083 - Loss_train: 0.08326765754757655, Loss_test: 0.08213609543923375\n",
      "2084 - Loss_train: 0.0832675270583942, Loss_test: 0.0821362754714195\n",
      "2085 - Loss_train: 0.08326739663694394, Loss_test: 0.08213645549989962\n",
      "2086 - Loss_train: 0.0832672662833848, Loss_test: 0.08213663551449438\n",
      "2087 - Loss_train: 0.08326713599776772, Loss_test: 0.08213681549915174\n",
      "2088 - Loss_train: 0.08326700577992294, Loss_test: 0.08213699549416337\n",
      "2089 - Loss_train: 0.08326687562993068, Loss_test: 0.08213717545582419\n",
      "2090 - Loss_train: 0.08326674554761117, Loss_test: 0.08213735540937936\n",
      "2091 - Loss_train: 0.083266615533059, Loss_test: 0.08213753536854781\n",
      "2092 - Loss_train: 0.08326648558613638, Loss_test: 0.08213771528237356\n",
      "2093 - Loss_train: 0.08326635570691435, Loss_test: 0.08213789523223014\n",
      "2094 - Loss_train: 0.08326622589522066, Loss_test: 0.08213807511566777\n",
      "2095 - Loss_train: 0.0832660961513888, Loss_test: 0.08213825501765584\n",
      "2096 - Loss_train: 0.0832659664750581, Loss_test: 0.08213843489571465\n",
      "2097 - Loss_train: 0.0832658368662842, Loss_test: 0.08213861476391493\n",
      "2098 - Loss_train: 0.08326570732474882, Loss_test: 0.08213879462333519\n",
      "2099 - Loss_train: 0.083265577850576, Loss_test: 0.08213897446394991\n",
      "2100 - Loss_train: 0.08326544844381939, Loss_test: 0.08213915427998893\n",
      "2101 - Loss_train: 0.08326531910472733, Loss_test: 0.08213933412756676\n",
      "2102 - Loss_train: 0.08326518983269969, Loss_test: 0.08213951391128166\n",
      "2103 - Loss_train: 0.08326506062807543, Loss_test: 0.08213969370304755\n",
      "2104 - Loss_train: 0.0832649314906164, Loss_test: 0.08213987349704789\n",
      "2105 - Loss_train: 0.08326480242061637, Loss_test: 0.08214005323788869\n",
      "2106 - Loss_train: 0.08326467341754179, Loss_test: 0.08214023300200926\n",
      "2107 - Loss_train: 0.08326454448165498, Loss_test: 0.08214041275120021\n",
      "2108 - Loss_train: 0.08326441561288711, Loss_test: 0.08214059246530635\n",
      "2109 - Loss_train: 0.08326428681138288, Loss_test: 0.08214077218119453\n",
      "2110 - Loss_train: 0.0832641580767235, Loss_test: 0.0821409518835201\n",
      "2111 - Loss_train: 0.08326402940932655, Loss_test: 0.08214113157885507\n",
      "2112 - Loss_train: 0.08326390080873795, Loss_test: 0.08214131125320931\n",
      "2113 - Loss_train: 0.08326377227519322, Loss_test: 0.08214149091189721\n",
      "2114 - Loss_train: 0.08326364380862965, Loss_test: 0.0821416705608631\n",
      "2115 - Loss_train: 0.08326351540879469, Loss_test: 0.08214185020079948\n",
      "2116 - Loss_train: 0.08326338707576271, Loss_test: 0.08214202982567724\n",
      "2117 - Loss_train: 0.0832632588094848, Loss_test: 0.08214220943297629\n",
      "2118 - Loss_train: 0.08326313061006915, Loss_test: 0.08214238901753468\n",
      "2119 - Loss_train: 0.08326300247751686, Loss_test: 0.08214256861062737\n",
      "2120 - Loss_train: 0.08326287441157376, Loss_test: 0.08214274818272402\n",
      "2121 - Loss_train: 0.0832627464122652, Loss_test: 0.08214292774928875\n",
      "2122 - Loss_train: 0.08326261847974423, Loss_test: 0.08214310728645705\n",
      "2123 - Loss_train: 0.08326249061367776, Loss_test: 0.08214328681488924\n",
      "2124 - Loss_train: 0.08326236281405729, Loss_test: 0.08214346632460917\n",
      "2125 - Loss_train: 0.08326223508106773, Loss_test: 0.08214364584862838\n",
      "2126 - Loss_train: 0.0832621074144458, Loss_test: 0.08214382531578883\n",
      "2127 - Loss_train: 0.08326197981436699, Loss_test: 0.08214400480830189\n",
      "2128 - Loss_train: 0.08326185228067813, Loss_test: 0.08214418427506871\n",
      "2129 - Loss_train: 0.08326172481362014, Loss_test: 0.08214436373081567\n",
      "2130 - Loss_train: 0.0832615974127141, Loss_test: 0.08214454317054327\n",
      "2131 - Loss_train: 0.08326147007830241, Loss_test: 0.08214472258161218\n",
      "2132 - Loss_train: 0.08326134281023799, Loss_test: 0.08214490200942963\n",
      "2133 - Loss_train: 0.08326121560813085, Loss_test: 0.0821450813925126\n",
      "2134 - Loss_train: 0.08326108847247143, Loss_test: 0.08214526080015577\n",
      "2135 - Loss_train: 0.0832609614030427, Loss_test: 0.082145440160471\n",
      "2136 - Loss_train: 0.08326083439976051, Loss_test: 0.08214561950758989\n",
      "2137 - Loss_train: 0.08326070746236422, Loss_test: 0.08214579887105632\n",
      "2138 - Loss_train: 0.08326058059102948, Loss_test: 0.08214597819724533\n",
      "2139 - Loss_train: 0.08326045378576209, Loss_test: 0.08214615751038812\n",
      "2140 - Loss_train: 0.08326032704654164, Loss_test: 0.08214633682482964\n",
      "2141 - Loss_train: 0.08326020037325638, Loss_test: 0.08214651611747964\n",
      "2142 - Loss_train: 0.08326007376612031, Loss_test: 0.0821466953998602\n",
      "2143 - Loss_train: 0.0832599472246975, Loss_test: 0.08214687466889302\n",
      "2144 - Loss_train: 0.083259820749348, Loss_test: 0.0821470539137214\n",
      "2145 - Loss_train: 0.08325969433983604, Loss_test: 0.08214723314462269\n",
      "2146 - Loss_train: 0.08325956799590355, Loss_test: 0.08214741237912092\n",
      "2147 - Loss_train: 0.08325944171783255, Loss_test: 0.08214759157632803\n",
      "2148 - Loss_train: 0.08325931550575155, Loss_test: 0.08214777078869108\n",
      "2149 - Loss_train: 0.08325918935901629, Loss_test: 0.08214794996403649\n",
      "2150 - Loss_train: 0.08325906327800653, Loss_test: 0.08214812915068592\n",
      "2151 - Loss_train: 0.08325893726286525, Loss_test: 0.08214830829241963\n",
      "2152 - Loss_train: 0.0832588113130646, Loss_test: 0.08214848744623071\n",
      "2153 - Loss_train: 0.08325868542880713, Loss_test: 0.08214866656911615\n",
      "2154 - Loss_train: 0.08325855961011694, Loss_test: 0.08214884568747066\n",
      "2155 - Loss_train: 0.08325843385682195, Loss_test: 0.0821490247751548\n",
      "2156 - Loss_train: 0.08325830816919381, Loss_test: 0.08214920389643267\n",
      "2157 - Loss_train: 0.0832581825469087, Loss_test: 0.08214938296177847\n",
      "2158 - Loss_train: 0.08325805699026, Loss_test: 0.082149562025208\n",
      "2159 - Loss_train: 0.0832579314987396, Loss_test: 0.0821497410596174\n",
      "2160 - Loss_train: 0.08325780607254069, Loss_test: 0.08214992009776102\n",
      "2161 - Loss_train: 0.08325768071183573, Loss_test: 0.08215009912517107\n",
      "2162 - Loss_train: 0.08325755541650423, Loss_test: 0.08215027814963242\n",
      "2163 - Loss_train: 0.08325743018604385, Loss_test: 0.08215045711430152\n",
      "2164 - Loss_train: 0.08325730502075872, Loss_test: 0.08215063611497792\n",
      "2165 - Loss_train: 0.08325717992097953, Loss_test: 0.08215081507907611\n",
      "2166 - Loss_train: 0.0832570548862108, Loss_test: 0.08215099402784576\n",
      "2167 - Loss_train: 0.08325692991641188, Loss_test: 0.08215117296831138\n",
      "2168 - Loss_train: 0.08325680501158404, Loss_test: 0.08215135188544814\n",
      "2169 - Loss_train: 0.08325668017184369, Loss_test: 0.08215153080556002\n",
      "2170 - Loss_train: 0.08325655539734594, Loss_test: 0.08215170970376205\n",
      "2171 - Loss_train: 0.08325643068748428, Loss_test: 0.08215188858163483\n",
      "2172 - Loss_train: 0.08325630604281543, Loss_test: 0.08215206745961565\n",
      "2173 - Loss_train: 0.08325618146291834, Loss_test: 0.08215224630820639\n",
      "2174 - Loss_train: 0.08325605694790769, Loss_test: 0.08215242514746787\n",
      "2175 - Loss_train: 0.08325593249769883, Loss_test: 0.0821526039919468\n",
      "2176 - Loss_train: 0.08325580811240381, Loss_test: 0.08215278279150626\n",
      "2177 - Loss_train: 0.08325568379163062, Loss_test: 0.0821529616032443\n",
      "2178 - Loss_train: 0.0832555595354567, Loss_test: 0.08215314038151986\n",
      "2179 - Loss_train: 0.08325543534434336, Loss_test: 0.08215331916023776\n",
      "2180 - Loss_train: 0.08325531121767704, Loss_test: 0.08215349792152642\n",
      "2181 - Loss_train: 0.08325518715577591, Loss_test: 0.08215367665904959\n",
      "2182 - Loss_train: 0.0832550631582176, Loss_test: 0.08215385538871108\n",
      "2183 - Loss_train: 0.08325493922530428, Loss_test: 0.08215403411121784\n",
      "2184 - Loss_train: 0.08325481535707586, Loss_test: 0.08215421281083421\n",
      "2185 - Loss_train: 0.0832546915530962, Loss_test: 0.08215439150202812\n",
      "2186 - Loss_train: 0.08325456781356855, Loss_test: 0.08215457018370058\n",
      "2187 - Loss_train: 0.08325444413846173, Loss_test: 0.08215474883694723\n",
      "2188 - Loss_train: 0.08325432052775301, Loss_test: 0.08215492748473865\n",
      "2189 - Loss_train: 0.08325419698134966, Loss_test: 0.08215510612363254\n",
      "2190 - Loss_train: 0.08325407349949997, Loss_test: 0.08215528474427689\n",
      "2191 - Loss_train: 0.08325395008195126, Loss_test: 0.08215546333849032\n",
      "2192 - Loss_train: 0.08325382672840849, Loss_test: 0.08215564193222238\n",
      "2193 - Loss_train: 0.08325370343918635, Loss_test: 0.08215582050727856\n",
      "2194 - Loss_train: 0.08325358021407651, Loss_test: 0.08215599907249764\n",
      "2195 - Loss_train: 0.08325345705307491, Loss_test: 0.08215617761661566\n",
      "2196 - Loss_train: 0.08325333395624053, Loss_test: 0.08215635615751041\n",
      "2197 - Loss_train: 0.08325321092348885, Loss_test: 0.08215653468016829\n",
      "2198 - Loss_train: 0.08325308795500355, Loss_test: 0.08215671317278135\n",
      "2199 - Loss_train: 0.08325296505026795, Loss_test: 0.08215689166234091\n",
      "2200 - Loss_train: 0.08325284220956317, Loss_test: 0.08215707015345852\n",
      "2201 - Loss_train: 0.08325271943277546, Loss_test: 0.08215724861054458\n",
      "2202 - Loss_train: 0.08325259671991969, Loss_test: 0.08215742706916294\n",
      "2203 - Loss_train: 0.08325247407110144, Loss_test: 0.08215760547907436\n",
      "2204 - Loss_train: 0.08325235148595785, Loss_test: 0.08215778392415407\n",
      "2205 - Loss_train: 0.08325222896465516, Loss_test: 0.08215796231270614\n",
      "2206 - Loss_train: 0.08325210650718766, Loss_test: 0.08215814071609068\n",
      "2207 - Loss_train: 0.08325198411340155, Loss_test: 0.08215831909422423\n",
      "2208 - Loss_train: 0.08325186178334511, Loss_test: 0.08215849746379852\n",
      "2209 - Loss_train: 0.08325173951691145, Loss_test: 0.0821586758079843\n",
      "2210 - Loss_train: 0.0832516173142101, Loss_test: 0.0821588541458013\n",
      "2211 - Loss_train: 0.08325149517511568, Loss_test: 0.08215903246752078\n",
      "2212 - Loss_train: 0.08325137309956575, Loss_test: 0.08215921077436546\n",
      "2213 - Loss_train: 0.0832512510878183, Loss_test: 0.08215938905287323\n",
      "2214 - Loss_train: 0.08325112913940001, Loss_test: 0.08215956735056845\n",
      "2215 - Loss_train: 0.0832510072544285, Loss_test: 0.0821597456033781\n",
      "2216 - Loss_train: 0.08325088543289554, Loss_test: 0.0821599238479979\n",
      "2217 - Loss_train: 0.08325076367497392, Loss_test: 0.08216010208712225\n",
      "2218 - Loss_train: 0.08325064198026333, Loss_test: 0.0821602803061316\n",
      "2219 - Loss_train: 0.08325052034893524, Loss_test: 0.0821604585145329\n",
      "2220 - Loss_train: 0.08325039878091334, Loss_test: 0.08216063670714402\n",
      "2221 - Loss_train: 0.08325027727639533, Loss_test: 0.08216081488537716\n",
      "2222 - Loss_train: 0.08325015583492573, Loss_test: 0.08216099303659206\n",
      "2223 - Loss_train: 0.08325003445660038, Loss_test: 0.08216117118864273\n",
      "2224 - Loss_train: 0.08324991314176326, Loss_test: 0.08216134932066345\n",
      "2225 - Loss_train: 0.08324979188980353, Loss_test: 0.08216152742784497\n",
      "2226 - Loss_train: 0.08324967070122812, Loss_test: 0.08216170554602537\n",
      "2227 - Loss_train: 0.08324954957546317, Loss_test: 0.08216188362989266\n",
      "2228 - Loss_train: 0.08324942851277888, Loss_test: 0.08216206170329572\n",
      "2229 - Loss_train: 0.08324930751311958, Loss_test: 0.08216223976150604\n",
      "2230 - Loss_train: 0.08324918657675112, Loss_test: 0.08216241781608286\n",
      "2231 - Loss_train: 0.08324906570302562, Loss_test: 0.08216259584771299\n",
      "2232 - Loss_train: 0.08324894489231599, Loss_test: 0.08216277386013067\n",
      "2233 - Loss_train: 0.08324882414451831, Loss_test: 0.08216295185645928\n",
      "2234 - Loss_train: 0.08324870345962058, Loss_test: 0.0821631298603037\n",
      "2235 - Loss_train: 0.08324858283738715, Loss_test: 0.08216330781428446\n",
      "2236 - Loss_train: 0.0832484622779636, Loss_test: 0.08216348578931688\n",
      "2237 - Loss_train: 0.08324834178136471, Loss_test: 0.08216366372875489\n",
      "2238 - Loss_train: 0.08324822134742504, Loss_test: 0.0821638416533588\n",
      "2239 - Loss_train: 0.08324810097627344, Loss_test: 0.08216401956942829\n",
      "2240 - Loss_train: 0.0832479806677672, Loss_test: 0.08216419746261167\n",
      "2241 - Loss_train: 0.08324786042190406, Loss_test: 0.08216437535945958\n",
      "2242 - Loss_train: 0.08324774023863421, Loss_test: 0.08216455322533864\n",
      "2243 - Loss_train: 0.08324762011792063, Loss_test: 0.08216473106732827\n",
      "2244 - Loss_train: 0.08324750005978206, Loss_test: 0.08216490892342086\n",
      "2245 - Loss_train: 0.08324738006411896, Loss_test: 0.08216508673276303\n",
      "2246 - Loss_train: 0.08324726013093889, Loss_test: 0.082165264558629\n",
      "2247 - Loss_train: 0.08324714026020746, Loss_test: 0.08216544234389218\n",
      "2248 - Loss_train: 0.08324702045190631, Loss_test: 0.08216562013628113\n",
      "2249 - Loss_train: 0.08324690070595075, Loss_test: 0.08216579789011522\n",
      "2250 - Loss_train: 0.08324678102250213, Loss_test: 0.08216597566149834\n",
      "2251 - Loss_train: 0.08324666140119126, Loss_test: 0.08216615338623592\n",
      "2252 - Loss_train: 0.083246541842037, Loss_test: 0.08216633110938763\n",
      "2253 - Loss_train: 0.08324642234526204, Loss_test: 0.08216650881080267\n",
      "2254 - Loss_train: 0.08324630291065398, Loss_test: 0.08216668651253169\n",
      "2255 - Loss_train: 0.08324618353824097, Loss_test: 0.08216686418480151\n",
      "2256 - Loss_train: 0.08324606422795938, Loss_test: 0.0821670418457084\n",
      "2257 - Loss_train: 0.08324594497975216, Loss_test: 0.08216721949397553\n",
      "2258 - Loss_train: 0.08324582579376302, Loss_test: 0.08216739712208768\n",
      "2259 - Loss_train: 0.0832457066699984, Loss_test: 0.08216757474084417\n",
      "2260 - Loss_train: 0.08324558760827012, Loss_test: 0.0821677523549641\n",
      "2261 - Loss_train: 0.08324546860847222, Loss_test: 0.08216792993975355\n",
      "2262 - Loss_train: 0.083245349670556, Loss_test: 0.08216810749607698\n",
      "2263 - Loss_train: 0.0832452307944467, Loss_test: 0.08216828504746285\n",
      "2264 - Loss_train: 0.08324511198019181, Loss_test: 0.08216846261089923\n",
      "2265 - Loss_train: 0.08324499322795106, Loss_test: 0.08216864012640342\n",
      "2266 - Loss_train: 0.08324487453725371, Loss_test: 0.08216881763476763\n",
      "2267 - Loss_train: 0.08324475590858418, Loss_test: 0.08216899514302549\n",
      "2268 - Loss_train: 0.0832446373416583, Loss_test: 0.08216917263024649\n",
      "2269 - Loss_train: 0.08324451883631062, Loss_test: 0.08216935008810772\n",
      "2270 - Loss_train: 0.0832444003928779, Loss_test: 0.08216952753898048\n",
      "2271 - Loss_train: 0.08324428201114167, Loss_test: 0.08216970498293394\n",
      "2272 - Loss_train: 0.08324416369086539, Loss_test: 0.08216988240410653\n",
      "2273 - Loss_train: 0.08324404543213626, Loss_test: 0.08217005980213106\n",
      "2274 - Loss_train: 0.08324392723492803, Loss_test: 0.08217023720352427\n",
      "2275 - Loss_train: 0.08324380909927673, Loss_test: 0.08217041457234942\n",
      "2276 - Loss_train: 0.08324369102513164, Loss_test: 0.08217059192974445\n",
      "2277 - Loss_train: 0.08324357301253238, Loss_test: 0.08217076928284009\n",
      "2278 - Loss_train: 0.08324345506136803, Loss_test: 0.08217094661555883\n",
      "2279 - Loss_train: 0.08324333717147975, Loss_test: 0.08217112393016984\n",
      "2280 - Loss_train: 0.08324321934303427, Loss_test: 0.08217130122608736\n",
      "2281 - Loss_train: 0.08324310157620399, Loss_test: 0.08217147852079813\n",
      "2282 - Loss_train: 0.08324298387049157, Loss_test: 0.0821716557887922\n",
      "2283 - Loss_train: 0.08324286622601929, Loss_test: 0.08217183303305291\n",
      "2284 - Loss_train: 0.0832427486427769, Loss_test: 0.0821720102921574\n",
      "2285 - Loss_train: 0.08324263112095118, Loss_test: 0.08217218750796362\n",
      "2286 - Loss_train: 0.08324251366009353, Loss_test: 0.08217236470226931\n",
      "2287 - Loss_train: 0.08324239626038316, Loss_test: 0.08217254191169163\n",
      "2288 - Loss_train: 0.08324227892197093, Loss_test: 0.0821727190833749\n",
      "2289 - Loss_train: 0.08324216164472666, Loss_test: 0.08217289623793388\n",
      "2290 - Loss_train: 0.08324204442828124, Loss_test: 0.0821730734100036\n",
      "2291 - Loss_train: 0.08324192727280129, Loss_test: 0.08217325051979778\n",
      "2292 - Loss_train: 0.08324181017839796, Loss_test: 0.08217342764849007\n",
      "2293 - Loss_train: 0.08324169314485805, Loss_test: 0.08217360475231526\n",
      "2294 - Loss_train: 0.08324157617234873, Loss_test: 0.08217378182721514\n",
      "2295 - Loss_train: 0.0832414592607112, Loss_test: 0.08217395890114107\n",
      "2296 - Loss_train: 0.08324134240992972, Loss_test: 0.08217413596650903\n",
      "2297 - Loss_train: 0.08324122561995695, Loss_test: 0.08217431300477154\n",
      "2298 - Loss_train: 0.08324110889080169, Loss_test: 0.08217449003255231\n",
      "2299 - Loss_train: 0.08324099222244058, Loss_test: 0.08217466703272282\n",
      "2300 - Loss_train: 0.08324087561483082, Loss_test: 0.08217484403143788\n",
      "2301 - Loss_train: 0.08324075906792913, Loss_test: 0.08217502101663979\n",
      "2302 - Loss_train: 0.0832406425817246, Loss_test: 0.08217519796599318\n",
      "2303 - Loss_train: 0.0832405261561256, Loss_test: 0.08217537492150796\n",
      "2304 - Loss_train: 0.08324040979109246, Loss_test: 0.08217555185492861\n",
      "2305 - Loss_train: 0.08324029348691063, Loss_test: 0.08217572876519182\n",
      "2306 - Loss_train: 0.08324017724329096, Loss_test: 0.08217590566412225\n",
      "2307 - Loss_train: 0.08324006106010597, Loss_test: 0.08217608255030467\n",
      "2308 - Loss_train: 0.08323994493727298, Loss_test: 0.08217625942527507\n",
      "2309 - Loss_train: 0.08323982887487238, Loss_test: 0.08217643626519705\n",
      "2310 - Loss_train: 0.08323971287292262, Loss_test: 0.08217661312496068\n",
      "2311 - Loss_train: 0.08323959693134984, Loss_test: 0.082176789943347\n",
      "2312 - Loss_train: 0.08323948105024044, Loss_test: 0.08217696674069501\n",
      "2313 - Loss_train: 0.08323936522952124, Loss_test: 0.08217714353690474\n",
      "2314 - Loss_train: 0.08323924946891227, Loss_test: 0.08217732030887358\n",
      "2315 - Loss_train: 0.08323913376852565, Loss_test: 0.08217749707253437\n",
      "2316 - Loss_train: 0.08323901812843248, Loss_test: 0.08217767382305788\n",
      "2317 - Loss_train: 0.08323890254870507, Loss_test: 0.08217785054951991\n",
      "2318 - Loss_train: 0.08323878702906848, Loss_test: 0.08217802727851167\n",
      "2319 - Loss_train: 0.08323867156967293, Loss_test: 0.08217820395228977\n",
      "2320 - Loss_train: 0.08323855617018519, Loss_test: 0.08217838064170974\n",
      "2321 - Loss_train: 0.08323844083084246, Loss_test: 0.08217855731620777\n",
      "2322 - Loss_train: 0.08323832555149545, Loss_test: 0.0821787339593425\n",
      "2323 - Loss_train: 0.08323821033218742, Loss_test: 0.08217891060085909\n",
      "2324 - Loss_train: 0.08323809517274904, Loss_test: 0.08217908720695007\n",
      "2325 - Loss_train: 0.08323798007358127, Loss_test: 0.08217926382742434\n",
      "2326 - Loss_train: 0.08323786503429685, Loss_test: 0.0821794404117608\n",
      "2327 - Loss_train: 0.08323775005474664, Loss_test: 0.08217961698713822\n",
      "2328 - Loss_train: 0.08323763513501668, Loss_test: 0.08217979352918492\n",
      "2329 - Loss_train: 0.08323752027516784, Loss_test: 0.08217997008783456\n",
      "2330 - Loss_train: 0.08323740547503707, Loss_test: 0.08218014661196779\n",
      "2331 - Loss_train: 0.08323729073471485, Loss_test: 0.08218032310513818\n",
      "2332 - Loss_train: 0.08323717605409248, Loss_test: 0.08218049960961643\n",
      "2333 - Loss_train: 0.08323706143338495, Loss_test: 0.08218067609140058\n",
      "2334 - Loss_train: 0.08323694687236112, Loss_test: 0.08218085253606883\n",
      "2335 - Loss_train: 0.08323683237078999, Loss_test: 0.082181028995457\n",
      "2336 - Loss_train: 0.08323671792894836, Loss_test: 0.08218120543483191\n",
      "2337 - Loss_train: 0.08323660354663362, Loss_test: 0.08218138182664106\n",
      "2338 - Loss_train: 0.08323648922380006, Loss_test: 0.08218155823432158\n",
      "2339 - Loss_train: 0.08323637496044621, Loss_test: 0.0821817346099055\n",
      "2340 - Loss_train: 0.0832362607565975, Loss_test: 0.08218191098055351\n",
      "2341 - Loss_train: 0.08323614661225005, Loss_test: 0.08218208733971018\n",
      "2342 - Loss_train: 0.0832360325273145, Loss_test: 0.08218226367285754\n",
      "2343 - Loss_train: 0.08323591850175487, Loss_test: 0.0821824399839178\n",
      "2344 - Loss_train: 0.08323580453572535, Loss_test: 0.08218261628613406\n",
      "2345 - Loss_train: 0.08323569062887341, Loss_test: 0.0821827925739153\n",
      "2346 - Loss_train: 0.08323557678130993, Loss_test: 0.0821829688462956\n",
      "2347 - Loss_train: 0.08323546299304464, Loss_test: 0.08218314510575819\n",
      "2348 - Loss_train: 0.08323534926404717, Loss_test: 0.08218332134276306\n",
      "2349 - Loss_train: 0.08323523559418734, Loss_test: 0.08218349756788164\n",
      "2350 - Loss_train: 0.08323512198355992, Loss_test: 0.08218367376757026\n",
      "2351 - Loss_train: 0.08323500843204094, Loss_test: 0.08218384994656533\n",
      "2352 - Loss_train: 0.08323489493986233, Loss_test: 0.08218402612807836\n",
      "2353 - Loss_train: 0.0832347815065625, Loss_test: 0.08218420229530997\n",
      "2354 - Loss_train: 0.08323466813235629, Loss_test: 0.08218437844602657\n",
      "2355 - Loss_train: 0.083234554817073, Loss_test: 0.08218455454373345\n",
      "2356 - Loss_train: 0.08323444156088647, Loss_test: 0.08218473068301234\n",
      "2357 - Loss_train: 0.08323432836381357, Loss_test: 0.08218490675972065\n",
      "2358 - Loss_train: 0.08323421522544688, Loss_test: 0.08218508284497622\n",
      "2359 - Loss_train: 0.08323410214602077, Loss_test: 0.08218525890046648\n",
      "2360 - Loss_train: 0.08323398912567832, Loss_test: 0.08218543494997178\n",
      "2361 - Loss_train: 0.08323387616394852, Loss_test: 0.0821856109789934\n",
      "2362 - Loss_train: 0.08323376326107562, Loss_test: 0.0821857869914222\n",
      "2363 - Loss_train: 0.0832336504169936, Loss_test: 0.08218596298517263\n",
      "2364 - Loss_train: 0.08323353763173129, Loss_test: 0.08218613897883909\n",
      "2365 - Loss_train: 0.08323342490513765, Loss_test: 0.08218631492825304\n",
      "2366 - Loss_train: 0.08323331223734418, Loss_test: 0.08218649088360229\n",
      "2367 - Loss_train: 0.08323319962798546, Loss_test: 0.08218666681045235\n",
      "2368 - Loss_train: 0.08323308707753957, Loss_test: 0.08218684274373449\n",
      "2369 - Loss_train: 0.08323297458544006, Loss_test: 0.0821870186167225\n",
      "2370 - Loss_train: 0.08323286215195723, Loss_test: 0.08218719452616911\n",
      "2371 - Loss_train: 0.08323274977713278, Loss_test: 0.0821873703791512\n",
      "2372 - Loss_train: 0.08323263746064709, Loss_test: 0.0821875462317307\n",
      "2373 - Loss_train: 0.08323252520285987, Loss_test: 0.08218772207691423\n",
      "2374 - Loss_train: 0.08323241300348745, Loss_test: 0.08218789788467547\n",
      "2375 - Loss_train: 0.08323230086238666, Loss_test: 0.08218807369519812\n",
      "2376 - Loss_train: 0.08323218877961935, Loss_test: 0.08218824946034628\n",
      "2377 - Loss_train: 0.08323207675531749, Loss_test: 0.08218842525951622\n",
      "2378 - Loss_train: 0.08323196478923274, Loss_test: 0.08218860098998938\n",
      "2379 - Loss_train: 0.08323185288145465, Loss_test: 0.08218877673999735\n",
      "2380 - Loss_train: 0.0832317410320352, Loss_test: 0.08218895246339798\n",
      "2381 - Loss_train: 0.08323162924088484, Loss_test: 0.0821891281601487\n",
      "2382 - Loss_train: 0.08323151750777737, Loss_test: 0.08218930384426142\n",
      "2383 - Loss_train: 0.08323140583282443, Loss_test: 0.08218947951218132\n",
      "2384 - Loss_train: 0.08323129421600274, Loss_test: 0.08218965518169215\n",
      "2385 - Loss_train: 0.08323118265728853, Loss_test: 0.08218983081346108\n",
      "2386 - Loss_train: 0.0832310711567301, Loss_test: 0.08219000643639714\n",
      "2387 - Loss_train: 0.08323095971443092, Loss_test: 0.08219018204295533\n",
      "2388 - Loss_train: 0.08323084832994122, Loss_test: 0.08219035762887833\n",
      "2389 - Loss_train: 0.08323073700362237, Loss_test: 0.0821905331949189\n",
      "2390 - Loss_train: 0.08323062573505267, Loss_test: 0.08219070875740556\n",
      "2391 - Loss_train: 0.08323051452437341, Loss_test: 0.08219088429630685\n",
      "2392 - Loss_train: 0.08323040337182618, Loss_test: 0.08219105981340205\n",
      "2393 - Loss_train: 0.08323029227697651, Loss_test: 0.08219123532336735\n",
      "2394 - Loss_train: 0.08323018123994282, Loss_test: 0.0821914108106701\n",
      "2395 - Loss_train: 0.08323007026072013, Loss_test: 0.08219158630569785\n",
      "2396 - Loss_train: 0.08322995933925785, Loss_test: 0.0821917617474708\n",
      "2397 - Loss_train: 0.08322984847559053, Loss_test: 0.08219193717884876\n",
      "2398 - Loss_train: 0.08322973766988048, Loss_test: 0.08219211262279502\n",
      "2399 - Loss_train: 0.0832296269215733, Loss_test: 0.08219228801448814\n",
      "2400 - Loss_train: 0.08322951623092804, Loss_test: 0.08219246341167265\n",
      "2401 - Loss_train: 0.0832294055979926, Loss_test: 0.08219263878460296\n",
      "2402 - Loss_train: 0.0832292950225347, Loss_test: 0.08219281412931544\n",
      "2403 - Loss_train: 0.08322918450469428, Loss_test: 0.08219298948963169\n",
      "2404 - Loss_train: 0.08322907404437017, Loss_test: 0.08219316479204516\n",
      "2405 - Loss_train: 0.08322896364150853, Loss_test: 0.08219334011958492\n",
      "2406 - Loss_train: 0.08322885329619786, Loss_test: 0.08219351540704994\n",
      "2407 - Loss_train: 0.08322874300836329, Loss_test: 0.08219369066882268\n",
      "2408 - Loss_train: 0.08322863277794788, Loss_test: 0.08219386594646651\n",
      "2409 - Loss_train: 0.08322852260500617, Loss_test: 0.08219404116413971\n",
      "2410 - Loss_train: 0.08322841248928443, Loss_test: 0.08219421641734724\n",
      "2411 - Loss_train: 0.08322830243090222, Loss_test: 0.082194391608201\n",
      "2412 - Loss_train: 0.08322819242984092, Loss_test: 0.08219456681175229\n",
      "2413 - Loss_train: 0.08322808248631094, Loss_test: 0.08219474197392587\n",
      "2414 - Loss_train: 0.08322797259977675, Loss_test: 0.08219491713028533\n",
      "2415 - Loss_train: 0.0832278627706666, Loss_test: 0.08219509227401994\n",
      "2416 - Loss_train: 0.08322775299850804, Loss_test: 0.08219526739733148\n",
      "2417 - Loss_train: 0.08322764328356096, Loss_test: 0.08219544250280654\n",
      "2418 - Loss_train: 0.08322753362563172, Loss_test: 0.08219561760446221\n",
      "2419 - Loss_train: 0.08322742402499456, Loss_test: 0.08219579267545905\n",
      "2420 - Loss_train: 0.08322731448131933, Loss_test: 0.08219596773049193\n",
      "2421 - Loss_train: 0.08322720499460863, Loss_test: 0.08219614276289092\n",
      "2422 - Loss_train: 0.08322709556511183, Loss_test: 0.08219631778549825\n",
      "2423 - Loss_train: 0.08322698619236012, Loss_test: 0.08219649279957517\n",
      "2424 - Loss_train: 0.08322687687658964, Loss_test: 0.08219666778697757\n",
      "2425 - Loss_train: 0.083226767617764, Loss_test: 0.08219684275826587\n",
      "2426 - Loss_train: 0.08322665841576415, Loss_test: 0.0821970177138005\n",
      "2427 - Loss_train: 0.08322654927069156, Loss_test: 0.08219719264900664\n",
      "2428 - Loss_train: 0.08322644018255347, Loss_test: 0.08219736756767577\n",
      "2429 - Loss_train: 0.08322633115127952, Loss_test: 0.0821975424970236\n",
      "2430 - Loss_train: 0.08322622217670564, Loss_test: 0.08219771736628566\n",
      "2431 - Loss_train: 0.08322611325884495, Loss_test: 0.08219789223831357\n",
      "2432 - Loss_train: 0.08322600439778126, Loss_test: 0.08219806708717972\n",
      "2433 - Loss_train: 0.08322589559339251, Loss_test: 0.08219824193210869\n",
      "2434 - Loss_train: 0.08322578684539644, Loss_test: 0.08219841673846194\n",
      "2435 - Loss_train: 0.08322567815421596, Loss_test: 0.08219859155914479\n",
      "2436 - Loss_train: 0.08322556951943744, Loss_test: 0.08219876634224293\n",
      "2437 - Loss_train: 0.08322546094124884, Loss_test: 0.08219894111077379\n",
      "2438 - Loss_train: 0.08322535241963497, Loss_test: 0.08219911586690286\n",
      "2439 - Loss_train: 0.08322524395453942, Loss_test: 0.08219929059268848\n",
      "2440 - Loss_train: 0.08322513554584005, Loss_test: 0.08219946531631901\n",
      "2441 - Loss_train: 0.08322502719363797, Loss_test: 0.08219964002641061\n",
      "2442 - Loss_train: 0.08322491889798724, Loss_test: 0.08219981469225413\n",
      "2443 - Loss_train: 0.08322481065851652, Loss_test: 0.08219998936819996\n",
      "2444 - Loss_train: 0.0832247024753669, Loss_test: 0.08220016402765379\n",
      "2445 - Loss_train: 0.08322459434876835, Loss_test: 0.08220033865151176\n",
      "2446 - Loss_train: 0.08322448627826542, Loss_test: 0.08220051327335803\n",
      "2447 - Loss_train: 0.08322437826401617, Loss_test: 0.08220068786867187\n",
      "2448 - Loss_train: 0.08322427030606805, Loss_test: 0.08220086246067822\n",
      "2449 - Loss_train: 0.08322416240428511, Loss_test: 0.08220103701502585\n",
      "2450 - Loss_train: 0.08322405455868788, Loss_test: 0.08220121156246833\n",
      "2451 - Loss_train: 0.08322394676926122, Loss_test: 0.08220138610096052\n",
      "2452 - Loss_train: 0.0832238390359147, Loss_test: 0.08220156061720214\n",
      "2453 - Loss_train: 0.0832237313588593, Loss_test: 0.0822017351050332\n",
      "2454 - Loss_train: 0.08322362373769834, Loss_test: 0.082201909589896\n",
      "2455 - Loss_train: 0.08322351617270467, Loss_test: 0.08220208405381589\n",
      "2456 - Loss_train: 0.08322340866354357, Loss_test: 0.08220225849465669\n",
      "2457 - Loss_train: 0.0832233012103417, Loss_test: 0.08220243292055869\n",
      "2458 - Loss_train: 0.08322319381312852, Loss_test: 0.08220260732711061\n",
      "2459 - Loss_train: 0.08322308647201916, Loss_test: 0.08220278172186422\n",
      "2460 - Loss_train: 0.08322297918663232, Loss_test: 0.08220295610772396\n",
      "2461 - Loss_train: 0.08322287195726116, Loss_test: 0.08220313046115763\n",
      "2462 - Loss_train: 0.0832227647835143, Loss_test: 0.0822033047969213\n",
      "2463 - Loss_train: 0.08322265766554629, Loss_test: 0.08220347912852305\n",
      "2464 - Loss_train: 0.0832225506035208, Loss_test: 0.08220365343848267\n",
      "2465 - Loss_train: 0.08322244359709298, Loss_test: 0.08220382772711667\n",
      "2466 - Loss_train: 0.08322233664638064, Loss_test: 0.08220400199595458\n",
      "2467 - Loss_train: 0.08322222975137035, Loss_test: 0.0822041762504811\n",
      "2468 - Loss_train: 0.0832221229120805, Loss_test: 0.08220435049583105\n",
      "2469 - Loss_train: 0.08322201612841762, Loss_test: 0.08220452471062241\n",
      "2470 - Loss_train: 0.08322190940029843, Loss_test: 0.0822046989239966\n",
      "2471 - Loss_train: 0.08322180272771786, Loss_test: 0.08220487309462744\n",
      "2472 - Loss_train: 0.08322169611060042, Loss_test: 0.08220504726835731\n",
      "2473 - Loss_train: 0.08322158954899597, Loss_test: 0.08220522141226168\n",
      "2474 - Loss_train: 0.08322148304316883, Loss_test: 0.08220539556091178\n",
      "2475 - Loss_train: 0.08322137659251776, Loss_test: 0.08220556967161663\n",
      "2476 - Loss_train: 0.0832212701973353, Loss_test: 0.08220574377408135\n",
      "2477 - Loss_train: 0.08322116385754007, Loss_test: 0.08220591785676995\n",
      "2478 - Loss_train: 0.08322105757326026, Loss_test: 0.0822060919242822\n",
      "2479 - Loss_train: 0.08322095134431666, Loss_test: 0.08220626596522887\n",
      "2480 - Loss_train: 0.08322084517053227, Loss_test: 0.08220643999615714\n",
      "2481 - Loss_train: 0.08322073905196738, Loss_test: 0.08220661400352743\n",
      "2482 - Loss_train: 0.0832206329889606, Loss_test: 0.08220678801257515\n",
      "2483 - Loss_train: 0.0832205269808706, Loss_test: 0.08220696197921465\n",
      "2484 - Loss_train: 0.08322042102796029, Loss_test: 0.08220713593989021\n",
      "2485 - Loss_train: 0.08322031513045855, Loss_test: 0.08220730988584667\n",
      "2486 - Loss_train: 0.08322020928808684, Loss_test: 0.08220748380109016\n",
      "2487 - Loss_train: 0.08322010350070351, Loss_test: 0.08220765770256558\n",
      "2488 - Loss_train: 0.08321999776827838, Loss_test: 0.08220783160066389\n",
      "2489 - Loss_train: 0.08321989209089424, Loss_test: 0.08220800546879538\n",
      "2490 - Loss_train: 0.08321978646855861, Loss_test: 0.08220817933079744\n",
      "2491 - Loss_train: 0.08321968090126694, Loss_test: 0.08220835316387594\n",
      "2492 - Loss_train: 0.08321957538911422, Loss_test: 0.0822085269951002\n",
      "2493 - Loss_train: 0.0832194699317061, Loss_test: 0.08220870079095124\n",
      "2494 - Loss_train: 0.0832193645294102, Loss_test: 0.08220887458207639\n",
      "2495 - Loss_train: 0.08321925918180183, Loss_test: 0.0822090483417165\n",
      "2496 - Loss_train: 0.08321915388924228, Loss_test: 0.08220922209117598\n",
      "2497 - Loss_train: 0.08321904865149432, Loss_test: 0.08220939582714022\n",
      "2498 - Loss_train: 0.08321894346835051, Loss_test: 0.08220956954201826\n",
      "2499 - Loss_train: 0.08321883833992494, Loss_test: 0.08220974323448445\n",
      "2500 - Loss_train: 0.08321873326645, Loss_test: 0.08220991691933183\n",
      "2501 - Loss_train: 0.08321862824769598, Loss_test: 0.08221009057502475\n",
      "2502 - Loss_train: 0.08321852328341352, Loss_test: 0.08221026422546288\n",
      "2503 - Loss_train: 0.0832184183737987, Loss_test: 0.08221043784219248\n",
      "2504 - Loss_train: 0.08321831351896994, Loss_test: 0.08221061145214366\n",
      "2505 - Loss_train: 0.08321820871873822, Loss_test: 0.08221078505039796\n",
      "2506 - Loss_train: 0.08321810397290703, Loss_test: 0.08221095862975489\n",
      "2507 - Loss_train: 0.0832179992816293, Loss_test: 0.0822111321824774\n",
      "2508 - Loss_train: 0.08321789464488286, Loss_test: 0.08221130572314327\n",
      "2509 - Loss_train: 0.08321779006261475, Loss_test: 0.08221147923502109\n",
      "2510 - Loss_train: 0.083217685534833, Loss_test: 0.08221165273113383\n",
      "2511 - Loss_train: 0.0832175810616457, Loss_test: 0.0822118262311865\n",
      "2512 - Loss_train: 0.08321747664276263, Loss_test: 0.08221199969491509\n",
      "2513 - Loss_train: 0.08321737227804277, Loss_test: 0.08221217313735368\n",
      "2514 - Loss_train: 0.08321726796769305, Loss_test: 0.08221234657315449\n",
      "2515 - Loss_train: 0.08321716371180726, Loss_test: 0.0822125199784057\n",
      "2516 - Loss_train: 0.0832170595100383, Loss_test: 0.082212693397376\n",
      "2517 - Loss_train: 0.08321695536262907, Loss_test: 0.08221286675354132\n",
      "2518 - Loss_train: 0.0832168512693012, Loss_test: 0.08221304011989945\n",
      "2519 - Loss_train: 0.08321674723023152, Loss_test: 0.08221321346196865\n",
      "2520 - Loss_train: 0.08321664324553392, Loss_test: 0.08221338678782798\n",
      "2521 - Loss_train: 0.08321653931471117, Loss_test: 0.08221356009290325\n",
      "2522 - Loss_train: 0.0832164354382414, Loss_test: 0.08221373338788399\n",
      "2523 - Loss_train: 0.08321633161573523, Loss_test: 0.08221390665283299\n",
      "2524 - Loss_train: 0.08321622784720446, Loss_test: 0.08221407991351003\n",
      "2525 - Loss_train: 0.08321612413274866, Loss_test: 0.08221425314949699\n",
      "2526 - Loss_train: 0.08321602047227898, Loss_test: 0.08221442637540002\n",
      "2527 - Loss_train: 0.08321591686600009, Loss_test: 0.08221459955554923\n",
      "2528 - Loss_train: 0.0832158133133831, Loss_test: 0.08221477275106462\n",
      "2529 - Loss_train: 0.08321570981472214, Loss_test: 0.08221494590979017\n",
      "2530 - Loss_train: 0.08321560636994839, Loss_test: 0.08221511906254628\n",
      "2531 - Loss_train: 0.08321550297927097, Loss_test: 0.08221529218040097\n",
      "2532 - Loss_train: 0.08321539964215784, Loss_test: 0.08221546529735266\n",
      "2533 - Loss_train: 0.08321529635909837, Loss_test: 0.08221563838731161\n",
      "2534 - Loss_train: 0.08321519312958442, Loss_test: 0.08221581145957042\n",
      "2535 - Loss_train: 0.0832150899537934, Loss_test: 0.08221598451578055\n",
      "2536 - Loss_train: 0.08321498683175232, Loss_test: 0.08221615755901157\n",
      "2537 - Loss_train: 0.08321488376344709, Loss_test: 0.08221633057933234\n",
      "2538 - Loss_train: 0.08321478074877188, Loss_test: 0.08221650357870657\n",
      "2539 - Loss_train: 0.08321467778774354, Loss_test: 0.08221667656250008\n",
      "2540 - Loss_train: 0.08321457488027541, Loss_test: 0.08221684953153265\n",
      "2541 - Loss_train: 0.08321447202639838, Loss_test: 0.08221702249309513\n",
      "2542 - Loss_train: 0.08321436922605324, Loss_test: 0.08221719540633188\n",
      "2543 - Loss_train: 0.08321426647929148, Loss_test: 0.08221736832237324\n",
      "2544 - Loss_train: 0.0832141637859287, Loss_test: 0.0822175412073386\n",
      "2545 - Loss_train: 0.08321406114618972, Loss_test: 0.0822177140990118\n",
      "2546 - Loss_train: 0.08321395855979798, Loss_test: 0.08221788696692993\n",
      "2547 - Loss_train: 0.08321385602678888, Loss_test: 0.0822180597872842\n",
      "2548 - Loss_train: 0.08321375354731758, Loss_test: 0.08221823261440943\n",
      "2549 - Loss_train: 0.08321365112110685, Loss_test: 0.08221840541939354\n",
      "2550 - Loss_train: 0.0832135487481864, Loss_test: 0.0822185782143492\n",
      "2551 - Loss_train: 0.08321344642874937, Loss_test: 0.08221875096989394\n",
      "2552 - Loss_train: 0.08321334416242367, Loss_test: 0.08221892372013281\n",
      "2553 - Loss_train: 0.08321324194931791, Loss_test: 0.08221909645219277\n",
      "2554 - Loss_train: 0.08321313978960766, Loss_test: 0.08221926917694443\n",
      "2555 - Loss_train: 0.08321303768310094, Loss_test: 0.08221944187307093\n",
      "2556 - Loss_train: 0.08321293562957816, Loss_test: 0.08221961454247564\n",
      "2557 - Loss_train: 0.08321283362919117, Loss_test: 0.08221978719936988\n",
      "2558 - Loss_train: 0.08321273168186676, Loss_test: 0.0822199598479168\n",
      "2559 - Loss_train: 0.08321262978768014, Loss_test: 0.08222013246712727\n",
      "2560 - Loss_train: 0.0832125279466886, Loss_test: 0.08222030508383084\n",
      "2561 - Loss_train: 0.0832124261586416, Loss_test: 0.0822204776578412\n",
      "2562 - Loss_train: 0.08321232442351675, Loss_test: 0.08222065023174213\n",
      "2563 - Loss_train: 0.08321222274162522, Loss_test: 0.08222082278329779\n",
      "2564 - Loss_train: 0.08321212111241294, Loss_test: 0.08222099530325319\n",
      "2565 - Loss_train: 0.08321201953627848, Loss_test: 0.08222116783124957\n",
      "2566 - Loss_train: 0.08321191801284479, Loss_test: 0.08222134031238039\n",
      "2567 - Loss_train: 0.08321181654234211, Loss_test: 0.08222151279722573\n",
      "2568 - Loss_train: 0.08321171512491005, Loss_test: 0.08222168525082749\n",
      "2569 - Loss_train: 0.08321161375995537, Loss_test: 0.08222185769553966\n",
      "2570 - Loss_train: 0.0832115124477917, Loss_test: 0.08222203011758071\n",
      "2571 - Loss_train: 0.08321141118838062, Loss_test: 0.0822222025210889\n",
      "2572 - Loss_train: 0.08321130998177072, Loss_test: 0.08222237491656696\n",
      "2573 - Loss_train: 0.08321120882777022, Loss_test: 0.08222254727745865\n",
      "2574 - Loss_train: 0.08321110772646721, Loss_test: 0.08222271962775585\n",
      "2575 - Loss_train: 0.0832110066778524, Loss_test: 0.08222289196600062\n",
      "2576 - Loss_train: 0.08321090568182987, Loss_test: 0.08222306427493314\n",
      "2577 - Loss_train: 0.08321080473838793, Loss_test: 0.08222323655868377\n",
      "2578 - Loss_train: 0.08321070384750974, Loss_test: 0.08222340884634999\n",
      "2579 - Loss_train: 0.0832106030091062, Loss_test: 0.08222358108922345\n",
      "2580 - Loss_train: 0.08321050222329533, Loss_test: 0.0822237533303879\n",
      "2581 - Loss_train: 0.08321040148986816, Loss_test: 0.08222392555778046\n",
      "2582 - Loss_train: 0.08321030080892368, Loss_test: 0.0822240977659453\n",
      "2583 - Loss_train: 0.08321020018040462, Loss_test: 0.0822242699404098\n",
      "2584 - Loss_train: 0.0832100996042184, Loss_test: 0.08222444211269296\n",
      "2585 - Loss_train: 0.08320999908043682, Loss_test: 0.08222461425493438\n",
      "2586 - Loss_train: 0.08320989860915239, Loss_test: 0.08222478638453118\n",
      "2587 - Loss_train: 0.08320979819018694, Loss_test: 0.08222495849627702\n",
      "2588 - Loss_train: 0.08320969782330583, Loss_test: 0.08222513059359071\n",
      "2589 - Loss_train: 0.08320959750867586, Loss_test: 0.0822253026536785\n",
      "2590 - Loss_train: 0.0832094972464881, Loss_test: 0.08222547472614926\n",
      "2591 - Loss_train: 0.08320939703634879, Loss_test: 0.08222564674727942\n",
      "2592 - Loss_train: 0.08320929687835939, Loss_test: 0.08222581876309829\n",
      "2593 - Loss_train: 0.08320919677273175, Loss_test: 0.08222599078103897\n",
      "2594 - Loss_train: 0.08320909671906979, Loss_test: 0.08222616275334715\n",
      "2595 - Loss_train: 0.08320899671760763, Loss_test: 0.08222633470935885\n",
      "2596 - Loss_train: 0.08320889676808013, Loss_test: 0.08222650666224993\n",
      "2597 - Loss_train: 0.08320879687058223, Loss_test: 0.08222667858438591\n",
      "2598 - Loss_train: 0.08320869702509603, Loss_test: 0.08222685048845799\n",
      "2599 - Loss_train: 0.08320859723157589, Loss_test: 0.08222702238092267\n",
      "2600 - Loss_train: 0.08320849749003002, Loss_test: 0.08222719424280815\n",
      "2601 - Loss_train: 0.08320839780036936, Loss_test: 0.08222736609889364\n",
      "2602 - Loss_train: 0.08320829816286678, Loss_test: 0.08222753793213165\n",
      "2603 - Loss_train: 0.08320819857722644, Loss_test: 0.08222770975395925\n",
      "2604 - Loss_train: 0.0832080990434283, Loss_test: 0.08222788155128895\n",
      "2605 - Loss_train: 0.08320799956128674, Loss_test: 0.08222805331506766\n",
      "2606 - Loss_train: 0.08320790013094775, Loss_test: 0.082228225084105\n",
      "2607 - Loss_train: 0.08320780075237631, Loss_test: 0.08222839681544804\n",
      "2608 - Loss_train: 0.08320770142570758, Loss_test: 0.08222856853861944\n",
      "2609 - Loss_train: 0.0832076021505763, Loss_test: 0.08222874024406286\n",
      "2610 - Loss_train: 0.0832075029271538, Loss_test: 0.08222891193654956\n",
      "2611 - Loss_train: 0.08320740375558451, Loss_test: 0.0822290835921413\n",
      "2612 - Loss_train: 0.08320730463543949, Loss_test: 0.08222925524151335\n",
      "2613 - Loss_train: 0.08320720556706365, Loss_test: 0.08222942686195359\n",
      "2614 - Loss_train: 0.08320710655006817, Loss_test: 0.08222959847213687\n",
      "2615 - Loss_train: 0.08320700758481692, Loss_test: 0.08222977006770499\n",
      "2616 - Loss_train: 0.08320690867095705, Loss_test: 0.08222994163649963\n",
      "2617 - Loss_train: 0.0832068098086088, Loss_test: 0.08223011318675935\n",
      "2618 - Loss_train: 0.08320671099785229, Loss_test: 0.08223028471603003\n",
      "2619 - Loss_train: 0.08320661223840937, Loss_test: 0.08223045623554702\n",
      "2620 - Loss_train: 0.08320651353058844, Loss_test: 0.08223062774108367\n",
      "2621 - Loss_train: 0.08320641487397432, Loss_test: 0.082230799216305\n",
      "2622 - Loss_train: 0.08320631626863648, Loss_test: 0.082230970663634\n",
      "2623 - Loss_train: 0.08320621771469086, Loss_test: 0.08223114211425898\n",
      "2624 - Loss_train: 0.08320611921225227, Loss_test: 0.08223131353888062\n",
      "2625 - Loss_train: 0.08320602076089247, Loss_test: 0.08223148493735687\n",
      "2626 - Loss_train: 0.08320592236079599, Loss_test: 0.0822316563223187\n",
      "2627 - Loss_train: 0.08320582401198204, Loss_test: 0.08223182769613505\n",
      "2628 - Loss_train: 0.08320572571436885, Loss_test: 0.08223199905710533\n",
      "2629 - Loss_train: 0.08320562746798969, Loss_test: 0.08223217036364096\n",
      "2630 - Loss_train: 0.08320552927269682, Loss_test: 0.08223234167976942\n",
      "2631 - Loss_train: 0.0832054311285349, Loss_test: 0.08223251297447404\n",
      "2632 - Loss_train: 0.0832053330354377, Loss_test: 0.08223268423987305\n",
      "2633 - Loss_train: 0.08320523499363555, Loss_test: 0.08223285549824288\n",
      "2634 - Loss_train: 0.08320513700261085, Loss_test: 0.08223302672523987\n",
      "2635 - Loss_train: 0.0832050390626691, Loss_test: 0.08223319793137879\n",
      "2636 - Loss_train: 0.08320494117373038, Loss_test: 0.08223336914719445\n",
      "2637 - Loss_train: 0.08320484333598259, Loss_test: 0.0822335403202726\n",
      "2638 - Loss_train: 0.08320474554898272, Loss_test: 0.08223371147164446\n",
      "2639 - Loss_train: 0.08320464781286696, Loss_test: 0.08223388261638859\n",
      "2640 - Loss_train: 0.08320455012781304, Loss_test: 0.08223405373931682\n",
      "2641 - Loss_train: 0.08320445249345505, Loss_test: 0.08223422483493771\n",
      "2642 - Loss_train: 0.08320435490994886, Loss_test: 0.08223439591684417\n",
      "2643 - Loss_train: 0.08320425737725805, Loss_test: 0.08223456698849685\n",
      "2644 - Loss_train: 0.08320415989535027, Loss_test: 0.08223473802690734\n",
      "2645 - Loss_train: 0.08320406246414894, Loss_test: 0.08223490907104146\n",
      "2646 - Loss_train: 0.08320396508374045, Loss_test: 0.0822350800642007\n",
      "2647 - Loss_train: 0.0832038677539837, Loss_test: 0.08223525105326919\n",
      "2648 - Loss_train: 0.08320377047493359, Loss_test: 0.08223542203017117\n",
      "2649 - Loss_train: 0.08320367324648999, Loss_test: 0.08223559297973015\n",
      "2650 - Loss_train: 0.08320357606873618, Loss_test: 0.08223576389954106\n",
      "2651 - Loss_train: 0.08320347894158442, Loss_test: 0.08223593481794479\n",
      "2652 - Loss_train: 0.08320338186513308, Loss_test: 0.08223610570925381\n",
      "2653 - Loss_train: 0.08320328483925522, Loss_test: 0.08223627657710163\n",
      "2654 - Loss_train: 0.08320318786391903, Loss_test: 0.08223644744653008\n",
      "2655 - Loss_train: 0.08320309093884816, Loss_test: 0.08223661827290991\n",
      "2656 - Loss_train: 0.08320299406444974, Loss_test: 0.08223678910073426\n",
      "2657 - Loss_train: 0.08320289724025813, Loss_test: 0.0822369598915917\n",
      "2658 - Loss_train: 0.08320280046666229, Loss_test: 0.08223713067319999\n",
      "2659 - Loss_train: 0.08320270374354907, Loss_test: 0.08223730142686855\n",
      "2660 - Loss_train: 0.08320260707059804, Loss_test: 0.08223747218147132\n",
      "2661 - Loss_train: 0.0832025104481215, Loss_test: 0.08223764289682808\n",
      "2662 - Loss_train: 0.08320241387581076, Loss_test: 0.08223781360409729\n",
      "2663 - Loss_train: 0.08320231735378568, Loss_test: 0.08223798428269341\n",
      "2664 - Loss_train: 0.08320222088202162, Loss_test: 0.08223815495708464\n",
      "2665 - Loss_train: 0.08320212446048188, Loss_test: 0.08223832560038596\n",
      "2666 - Loss_train: 0.0832020280891015, Loss_test: 0.08223849622285646\n",
      "2667 - Loss_train: 0.08320193176794186, Loss_test: 0.08223866683062328\n",
      "2668 - Loss_train: 0.08320183549693395, Loss_test: 0.08223883742226573\n",
      "2669 - Loss_train: 0.08320173927605717, Loss_test: 0.0822390079866173\n",
      "2670 - Loss_train: 0.08320164310524467, Loss_test: 0.08223917854580375\n",
      "2671 - Loss_train: 0.08320154698467354, Loss_test: 0.08223934906936636\n",
      "2672 - Loss_train: 0.08320145091401689, Loss_test: 0.08223951959177261\n",
      "2673 - Loss_train: 0.08320135489335365, Loss_test: 0.08223969006529766\n",
      "2674 - Loss_train: 0.08320125892284122, Loss_test: 0.08223986055869328\n",
      "2675 - Loss_train: 0.08320116300220734, Loss_test: 0.08224003098882143\n",
      "2676 - Loss_train: 0.08320106713150378, Loss_test: 0.08224020145135082\n",
      "2677 - Loss_train: 0.08320097131065754, Loss_test: 0.08224037184898685\n",
      "2678 - Loss_train: 0.08320087553987228, Loss_test: 0.08224054226108787\n",
      "2679 - Loss_train: 0.08320077981900488, Loss_test: 0.08224071262631744\n",
      "2680 - Loss_train: 0.08320068414777905, Loss_test: 0.08224088299158278\n",
      "2681 - Loss_train: 0.08320058852630316, Loss_test: 0.0822410533255456\n",
      "2682 - Loss_train: 0.08320049295466286, Loss_test: 0.0822412236441545\n",
      "2683 - Loss_train: 0.08320039743281889, Loss_test: 0.08224139395330335\n",
      "2684 - Loss_train: 0.0832003019607144, Loss_test: 0.08224156423015833\n",
      "2685 - Loss_train: 0.08320020653831156, Loss_test: 0.08224173448058886\n",
      "2686 - Loss_train: 0.0832001111657077, Loss_test: 0.08224190473154874\n",
      "2687 - Loss_train: 0.08320001584263532, Loss_test: 0.0822420749546197\n",
      "2688 - Loss_train: 0.08319992056916457, Loss_test: 0.08224224515428798\n",
      "2689 - Loss_train: 0.08319982534539629, Loss_test: 0.08224241536549946\n",
      "2690 - Loss_train: 0.08319973017112996, Loss_test: 0.08224258550300874\n",
      "2691 - Loss_train: 0.08319963504655206, Loss_test: 0.08224275565792068\n",
      "2692 - Loss_train: 0.08319953997154796, Loss_test: 0.08224292577298298\n",
      "2693 - Loss_train: 0.08319944494600388, Loss_test: 0.08224309591320528\n",
      "2694 - Loss_train: 0.08319934996978356, Loss_test: 0.08224326597621343\n",
      "2695 - Loss_train: 0.0831992550431057, Loss_test: 0.08224343605357362\n",
      "2696 - Loss_train: 0.08319916016584136, Loss_test: 0.08224360609707645\n",
      "2697 - Loss_train: 0.08319906533810842, Loss_test: 0.08224377612040397\n",
      "2698 - Loss_train: 0.08319897055954602, Loss_test: 0.08224394614673887\n",
      "2699 - Loss_train: 0.08319887583035777, Loss_test: 0.08224411612985932\n",
      "2700 - Loss_train: 0.08319878115041349, Loss_test: 0.08224428609353428\n",
      "2701 - Loss_train: 0.08319868651985887, Loss_test: 0.08224445606561796\n",
      "2702 - Loss_train: 0.0831985919385713, Loss_test: 0.0822446259863992\n",
      "2703 - Loss_train: 0.0831984974065082, Loss_test: 0.08224479590945633\n",
      "2704 - Loss_train: 0.08319840292386851, Loss_test: 0.08224496579980396\n",
      "2705 - Loss_train: 0.0831983084903117, Loss_test: 0.08224513567390627\n",
      "2706 - Loss_train: 0.08319821410606679, Loss_test: 0.08224530553170747\n",
      "2707 - Loss_train: 0.08319811977079566, Loss_test: 0.08224547537196926\n",
      "2708 - Loss_train: 0.08319802548461218, Loss_test: 0.08224564518453711\n",
      "2709 - Loss_train: 0.08319793124752697, Loss_test: 0.08224581498871412\n",
      "2710 - Loss_train: 0.08319783705972686, Loss_test: 0.08224598474805146\n",
      "2711 - Loss_train: 0.08319774292078903, Loss_test: 0.0822461545291435\n",
      "2712 - Loss_train: 0.08319764883081977, Loss_test: 0.08224632426048084\n",
      "2713 - Loss_train: 0.08319755478984771, Loss_test: 0.08224649397306306\n",
      "2714 - Loss_train: 0.08319746079784394, Loss_test: 0.08224666368469948\n",
      "2715 - Loss_train: 0.08319736685481872, Loss_test: 0.08224683335469488\n",
      "2716 - Loss_train: 0.08319727296083897, Loss_test: 0.0822470030196934\n",
      "2717 - Loss_train: 0.08319717911559306, Loss_test: 0.08224717267052174\n",
      "2718 - Loss_train: 0.08319708531920773, Loss_test: 0.08224734227746833\n",
      "2719 - Loss_train: 0.08319699157163345, Loss_test: 0.0822475118819999\n",
      "2720 - Loss_train: 0.08319689787289604, Loss_test: 0.08224768146967446\n",
      "2721 - Loss_train: 0.08319680422297299, Loss_test: 0.08224785102573932\n",
      "2722 - Loss_train: 0.08319671062178646, Loss_test: 0.0822480205678569\n",
      "2723 - Loss_train: 0.08319661706951716, Loss_test: 0.08224819010053323\n",
      "2724 - Loss_train: 0.08319652356583465, Loss_test: 0.08224835960948056\n",
      "2725 - Loss_train: 0.08319643011083262, Loss_test: 0.08224852908049675\n",
      "2726 - Loss_train: 0.08319633670441666, Loss_test: 0.08224869855150128\n",
      "2727 - Loss_train: 0.08319624334670232, Loss_test: 0.08224886799779053\n",
      "2728 - Loss_train: 0.08319615003757631, Loss_test: 0.0822490374280462\n",
      "2729 - Loss_train: 0.08319605677701497, Loss_test: 0.08224920683132711\n",
      "2730 - Loss_train: 0.08319596356508106, Loss_test: 0.08224937622877737\n",
      "2731 - Loss_train: 0.08319587040160652, Loss_test: 0.08224954557866054\n",
      "2732 - Loss_train: 0.08319577728665843, Loss_test: 0.08224971493393177\n",
      "2733 - Loss_train: 0.08319568422015591, Loss_test: 0.082249884261628\n",
      "2734 - Loss_train: 0.08319559120231898, Loss_test: 0.08225005357754446\n",
      "2735 - Loss_train: 0.08319549823268851, Loss_test: 0.08225022284327498\n",
      "2736 - Loss_train: 0.08319540531156476, Loss_test: 0.08225039213362222\n",
      "2737 - Loss_train: 0.08319531243875339, Loss_test: 0.08225056137026236\n",
      "2738 - Loss_train: 0.08319521961426225, Loss_test: 0.08225073062211058\n",
      "2739 - Loss_train: 0.08319512683820025, Loss_test: 0.08225089980672681\n",
      "2740 - Loss_train: 0.08319503411044799, Loss_test: 0.08225106902232636\n",
      "2741 - Loss_train: 0.08319494143106856, Loss_test: 0.08225123817192487\n",
      "2742 - Loss_train: 0.08319484879982368, Loss_test: 0.0822514073324821\n",
      "2743 - Loss_train: 0.08319475621680457, Loss_test: 0.0822515764658052\n",
      "2744 - Loss_train: 0.08319466368198389, Loss_test: 0.08225174558091808\n",
      "2745 - Loss_train: 0.083194571195311, Loss_test: 0.0822519146644382\n",
      "2746 - Loss_train: 0.08319447875681374, Loss_test: 0.08225208374505028\n",
      "2747 - Loss_train: 0.08319438636664059, Loss_test: 0.08225225279141467\n",
      "2748 - Loss_train: 0.08319429402457539, Loss_test: 0.0822524218201103\n",
      "2749 - Loss_train: 0.08319420173057342, Loss_test: 0.08225259083854163\n",
      "2750 - Loss_train: 0.08319410948442188, Loss_test: 0.08225275982652763\n",
      "2751 - Loss_train: 0.08319401728643036, Loss_test: 0.08225292880874482\n",
      "2752 - Loss_train: 0.08319392513639214, Loss_test: 0.08225309774628933\n",
      "2753 - Loss_train: 0.08319383303449235, Loss_test: 0.08225326669661563\n",
      "2754 - Loss_train: 0.08319374098045178, Loss_test: 0.08225343560531083\n",
      "2755 - Loss_train: 0.08319364897429371, Loss_test: 0.08225360449303032\n",
      "2756 - Loss_train: 0.08319355701599647, Loss_test: 0.08225377337652086\n",
      "2757 - Loss_train: 0.08319346510564876, Loss_test: 0.08225394222248228\n",
      "2758 - Loss_train: 0.08319337324310841, Loss_test: 0.08225411106205494\n",
      "2759 - Loss_train: 0.08319328142855459, Loss_test: 0.08225427988281715\n",
      "2760 - Loss_train: 0.08319318966165831, Loss_test: 0.08225444866418805\n",
      "2761 - Loss_train: 0.08319309794273891, Loss_test: 0.08225461744575892\n",
      "2762 - Loss_train: 0.08319300627139944, Loss_test: 0.08225478619857489\n",
      "2763 - Loss_train: 0.08319291464772045, Loss_test: 0.08225495494044517\n",
      "2764 - Loss_train: 0.08319282307177615, Loss_test: 0.08225512365724598\n",
      "2765 - Loss_train: 0.08319273154353618, Loss_test: 0.08225529235221059\n",
      "2766 - Loss_train: 0.08319264006293711, Loss_test: 0.08225546103344808\n",
      "2767 - Loss_train: 0.08319254863016474, Loss_test: 0.08225562968941003\n",
      "2768 - Loss_train: 0.08319245724482344, Loss_test: 0.08225579831363992\n",
      "2769 - Loss_train: 0.08319236590723403, Loss_test: 0.08225596693199626\n",
      "2770 - Loss_train: 0.08319227461722999, Loss_test: 0.0822561355342488\n",
      "2771 - Loss_train: 0.08319218337455325, Loss_test: 0.0822563040980015\n",
      "2772 - Loss_train: 0.08319209217942466, Loss_test: 0.08225647265363759\n",
      "2773 - Loss_train: 0.08319200103176182, Loss_test: 0.08225664119960964\n",
      "2774 - Loss_train: 0.08319190993177102, Loss_test: 0.08225680971700124\n",
      "2775 - Loss_train: 0.08319181887899164, Loss_test: 0.08225697821581594\n",
      "2776 - Loss_train: 0.08319172787380633, Loss_test: 0.08225714668469061\n",
      "2777 - Loss_train: 0.08319163691580722, Loss_test: 0.0822573151540804\n",
      "2778 - Loss_train: 0.08319154600532476, Loss_test: 0.08225748357221871\n",
      "2779 - Loss_train: 0.08319145514202902, Loss_test: 0.08225765200234195\n",
      "2780 - Loss_train: 0.0831913643262269, Loss_test: 0.08225782037459299\n",
      "2781 - Loss_train: 0.08319127355758439, Loss_test: 0.08225798878888872\n",
      "2782 - Loss_train: 0.08319118283615742, Loss_test: 0.08225815712154481\n",
      "2783 - Loss_train: 0.08319109216199747, Loss_test: 0.08225832546806013\n",
      "2784 - Loss_train: 0.08319100153526397, Loss_test: 0.08225849378681652\n",
      "2785 - Loss_train: 0.08319091095567645, Loss_test: 0.08225866206422643\n",
      "2786 - Loss_train: 0.08319082042321665, Loss_test: 0.08225883035642832\n",
      "2787 - Loss_train: 0.08319072993768847, Loss_test: 0.08225899860524781\n",
      "2788 - Loss_train: 0.08319063949937758, Loss_test: 0.0822591668502139\n",
      "2789 - Loss_train: 0.08319054910815099, Loss_test: 0.08225933505587019\n",
      "2790 - Loss_train: 0.08319045876397535, Loss_test: 0.08225950326728074\n",
      "2791 - Loss_train: 0.08319036846681571, Loss_test: 0.08225967142652334\n",
      "2792 - Loss_train: 0.08319027821663187, Loss_test: 0.08225983959122575\n",
      "2793 - Loss_train: 0.08319018801369012, Loss_test: 0.08226000771604217\n",
      "2794 - Loss_train: 0.0831900978575188, Loss_test: 0.08226017585237015\n",
      "2795 - Loss_train: 0.08319000774832402, Loss_test: 0.08226034394182752\n",
      "2796 - Loss_train: 0.08318991768603383, Loss_test: 0.08226051202927524\n",
      "2797 - Loss_train: 0.08318982767079289, Loss_test: 0.08226068006528024\n",
      "2798 - Loss_train: 0.08318973770231348, Loss_test: 0.08226084810967099\n",
      "2799 - Loss_train: 0.08318964778064089, Loss_test: 0.08226101612216201\n",
      "2800 - Loss_train: 0.08318955790590452, Loss_test: 0.08226118410968553\n",
      "2801 - Loss_train: 0.08318946807796315, Loss_test: 0.08226135209279184\n",
      "2802 - Loss_train: 0.08318937829660461, Loss_test: 0.08226152004838438\n",
      "2803 - Loss_train: 0.0831892885620076, Loss_test: 0.08226168798085702\n",
      "2804 - Loss_train: 0.08318919887441344, Loss_test: 0.08226185590296184\n",
      "2805 - Loss_train: 0.0831891092332173, Loss_test: 0.08226202379082052\n",
      "2806 - Loss_train: 0.08318901963891047, Loss_test: 0.08226219165608686\n",
      "2807 - Loss_train: 0.08318893009106482, Loss_test: 0.08226235951990127\n",
      "2808 - Loss_train: 0.08318884058998607, Loss_test: 0.08226252734723062\n",
      "2809 - Loss_train: 0.08318875113545393, Loss_test: 0.08226269517072254\n",
      "2810 - Loss_train: 0.08318866172747787, Loss_test: 0.08226286295804888\n",
      "2811 - Loss_train: 0.08318857236619454, Loss_test: 0.08226303073087886\n",
      "2812 - Loss_train: 0.08318848305133103, Loss_test: 0.08226319850100099\n",
      "2813 - Loss_train: 0.08318839378296572, Loss_test: 0.08226336622308603\n",
      "2814 - Loss_train: 0.08318830456122718, Loss_test: 0.08226353392870193\n",
      "2815 - Loss_train: 0.0831882153860298, Loss_test: 0.08226370163460697\n",
      "2816 - Loss_train: 0.08318812625696226, Loss_test: 0.08226386929371093\n",
      "2817 - Loss_train: 0.08318803717451585, Loss_test: 0.08226403695700622\n",
      "2818 - Loss_train: 0.08318794813830234, Loss_test: 0.08226420457614667\n",
      "2819 - Loss_train: 0.08318785914845912, Loss_test: 0.08226437219970581\n",
      "2820 - Loss_train: 0.0831877702049582, Loss_test: 0.08226453978564967\n",
      "2821 - Loss_train: 0.08318768130792985, Loss_test: 0.08226470735961659\n",
      "2822 - Loss_train: 0.08318759245700669, Loss_test: 0.08226487490480798\n",
      "2823 - Loss_train: 0.08318750365252035, Loss_test: 0.08226504243490694\n",
      "2824 - Loss_train: 0.08318741489427475, Loss_test: 0.08226520995815513\n",
      "2825 - Loss_train: 0.08318732618207739, Loss_test: 0.08226537743466812\n",
      "2826 - Loss_train: 0.0831872375160435, Loss_test: 0.08226554490635195\n",
      "2827 - Loss_train: 0.08318714889617045, Loss_test: 0.0822657123535625\n",
      "2828 - Loss_train: 0.08318706032265122, Loss_test: 0.08226587980033533\n",
      "2829 - Loss_train: 0.08318697179507545, Loss_test: 0.08226604718717612\n",
      "2830 - Loss_train: 0.08318688331374575, Loss_test: 0.08226621458792938\n",
      "2831 - Loss_train: 0.08318679487830977, Loss_test: 0.08226638193899369\n",
      "2832 - Loss_train: 0.08318670648915812, Loss_test: 0.08226654930979604\n",
      "2833 - Loss_train: 0.08318661814598968, Loss_test: 0.08226671661701028\n",
      "2834 - Loss_train: 0.08318652984870233, Loss_test: 0.08226688393257785\n",
      "2835 - Loss_train: 0.08318644159755324, Loss_test: 0.08226705122316623\n",
      "2836 - Loss_train: 0.08318635339229079, Loss_test: 0.0822672184766283\n",
      "2837 - Loss_train: 0.0831862652329809, Loss_test: 0.08226738572867419\n",
      "2838 - Loss_train: 0.08318617711940705, Loss_test: 0.08226755295235781\n",
      "2839 - Loss_train: 0.0831860890517481, Loss_test: 0.08226772015304801\n",
      "2840 - Loss_train: 0.08318600103016752, Loss_test: 0.08226788734358799\n",
      "2841 - Loss_train: 0.08318591305439399, Loss_test: 0.08226805449695584\n",
      "2842 - Loss_train: 0.08318582512435546, Loss_test: 0.08226822164717024\n",
      "2843 - Loss_train: 0.08318573723995772, Loss_test: 0.08226838877659777\n",
      "2844 - Loss_train: 0.0831856494013738, Loss_test: 0.08226855587395279\n",
      "2845 - Loss_train: 0.08318556160866968, Loss_test: 0.08226872295392412\n",
      "2846 - Loss_train: 0.08318547386148283, Loss_test: 0.08226889002695081\n",
      "2847 - Loss_train: 0.08318538616001714, Loss_test: 0.08226905705067637\n",
      "2848 - Loss_train: 0.08318529850436474, Loss_test: 0.08226922408698861\n",
      "2849 - Loss_train: 0.0831852108942121, Loss_test: 0.08226939107771752\n",
      "2850 - Loss_train: 0.08318512332964337, Loss_test: 0.08226955804801746\n",
      "2851 - Loss_train: 0.08318503581073465, Loss_test: 0.08226972502630912\n",
      "2852 - Loss_train: 0.08318494833755927, Loss_test: 0.08226989196474524\n",
      "2853 - Loss_train: 0.08318486090966623, Loss_test: 0.08227005887568317\n",
      "2854 - Loss_train: 0.08318477352749601, Loss_test: 0.0822702257731466\n",
      "2855 - Loss_train: 0.08318468619063699, Loss_test: 0.08227039265079275\n",
      "2856 - Loss_train: 0.08318459889945751, Loss_test: 0.08227055950715806\n",
      "2857 - Loss_train: 0.083184511653614, Loss_test: 0.08227072634168454\n",
      "2858 - Loss_train: 0.08318442445316722, Loss_test: 0.08227089316180461\n",
      "2859 - Loss_train: 0.08318433729807112, Loss_test: 0.08227105995388191\n",
      "2860 - Loss_train: 0.08318425018861869, Loss_test: 0.08227122673965508\n",
      "2861 - Loss_train: 0.08318416312450597, Loss_test: 0.08227139348764599\n",
      "2862 - Loss_train: 0.08318407610555892, Loss_test: 0.08227156023086905\n",
      "2863 - Loss_train: 0.08318398913186602, Loss_test: 0.08227172693030738\n",
      "2864 - Loss_train: 0.08318390220363804, Loss_test: 0.08227189362931607\n",
      "2865 - Loss_train: 0.08318381532057655, Loss_test: 0.0822720602964291\n",
      "2866 - Loss_train: 0.08318372848287031, Loss_test: 0.08227222695220768\n",
      "2867 - Loss_train: 0.08318364169020817, Loss_test: 0.08227239358263258\n",
      "2868 - Loss_train: 0.08318355494272889, Loss_test: 0.08227256017948452\n",
      "2869 - Loss_train: 0.08318346824045304, Loss_test: 0.08227272678499235\n",
      "2870 - Loss_train: 0.08318338158326058, Loss_test: 0.08227289334533397\n",
      "2871 - Loss_train: 0.08318329497124478, Loss_test: 0.08227305990297662\n",
      "2872 - Loss_train: 0.08318320840448387, Loss_test: 0.08227322642161822\n",
      "2873 - Loss_train: 0.08318312188264804, Loss_test: 0.08227339292622877\n",
      "2874 - Loss_train: 0.08318303540587561, Loss_test: 0.08227355942083482\n",
      "2875 - Loss_train: 0.0831829489741214, Loss_test: 0.08227372587622522\n",
      "2876 - Loss_train: 0.08318286258755725, Loss_test: 0.08227389233774036\n",
      "2877 - Loss_train: 0.08318277624597264, Loss_test: 0.08227405875817526\n",
      "2878 - Loss_train: 0.08318268994918106, Loss_test: 0.08227422515676305\n",
      "2879 - Loss_train: 0.08318260369751142, Loss_test: 0.08227439154589584\n",
      "2880 - Loss_train: 0.0831825174907352, Loss_test: 0.08227455791088818\n",
      "2881 - Loss_train: 0.08318243132858262, Loss_test: 0.0822747242376083\n",
      "2882 - Loss_train: 0.08318234521153511, Loss_test: 0.0822748905746819\n",
      "2883 - Loss_train: 0.08318225913935974, Loss_test: 0.08227505686464283\n",
      "2884 - Loss_train: 0.083182173111997, Loss_test: 0.08227522314177552\n",
      "2885 - Loss_train: 0.08318208712926244, Loss_test: 0.08227538940131543\n",
      "2886 - Loss_train: 0.08318200119145262, Loss_test: 0.08227555564805618\n",
      "2887 - Loss_train: 0.08318191529843226, Loss_test: 0.08227572185199092\n",
      "2888 - Loss_train: 0.0831818294500898, Loss_test: 0.08227588806646853\n",
      "2889 - Loss_train: 0.083181743646278, Loss_test: 0.08227605423951001\n",
      "2890 - Loss_train: 0.08318165788717351, Loss_test: 0.08227622038786833\n",
      "2891 - Loss_train: 0.08318157217273259, Loss_test: 0.08227638652376022\n",
      "2892 - Loss_train: 0.0831814865029147, Loss_test: 0.08227655264492437\n",
      "2893 - Loss_train: 0.083181400877694, Loss_test: 0.08227671873045342\n",
      "2894 - Loss_train: 0.0831813152970747, Loss_test: 0.08227688480567236\n",
      "2895 - Loss_train: 0.08318122976114885, Loss_test: 0.08227705085396723\n",
      "2896 - Loss_train: 0.08318114426964471, Loss_test: 0.08227721688139136\n",
      "2897 - Loss_train: 0.08318105882264251, Loss_test: 0.08227738290793112\n",
      "2898 - Loss_train: 0.0831809734201016, Loss_test: 0.08227754888078902\n",
      "2899 - Loss_train: 0.0831808880621923, Loss_test: 0.08227771485835049\n",
      "2900 - Loss_train: 0.08318080274865802, Loss_test: 0.08227788080338515\n",
      "2901 - Loss_train: 0.08318071747943973, Loss_test: 0.08227804672961461\n",
      "2902 - Loss_train: 0.08318063225473425, Loss_test: 0.08227821264546879\n",
      "2903 - Loss_train: 0.08318054707437761, Loss_test: 0.08227837850964535\n",
      "2904 - Loss_train: 0.08318046193839766, Loss_test: 0.08227854439549015\n",
      "2905 - Loss_train: 0.08318037684677781, Loss_test: 0.08227871022899617\n",
      "2906 - Loss_train: 0.08318029179958837, Loss_test: 0.082278876058709\n",
      "2907 - Loss_train: 0.08318020679665919, Loss_test: 0.08227904186331955\n",
      "2908 - Loss_train: 0.08318012183781168, Loss_test: 0.08227920763083746\n",
      "2909 - Loss_train: 0.0831800369232075, Loss_test: 0.08227937339871114\n",
      "2910 - Loss_train: 0.08317995205296097, Loss_test: 0.08227953914014811\n",
      "2911 - Loss_train: 0.0831798672267632, Loss_test: 0.08227970485624138\n",
      "2912 - Loss_train: 0.08317978244472744, Loss_test: 0.0822798705518248\n",
      "2913 - Loss_train: 0.08317969770698277, Loss_test: 0.08228003622810746\n",
      "2914 - Loss_train: 0.08317961301345798, Loss_test: 0.08228020189398405\n",
      "2915 - Loss_train: 0.08317952836380436, Loss_test: 0.08228036751435054\n",
      "2916 - Loss_train: 0.08317944375829248, Loss_test: 0.08228053314007781\n",
      "2917 - Loss_train: 0.08317935919683198, Loss_test: 0.0822806987391189\n",
      "2918 - Loss_train: 0.08317927467942474, Loss_test: 0.08228086430193868\n",
      "2919 - Loss_train: 0.08317919020597006, Loss_test: 0.08228102985317032\n",
      "2920 - Loss_train: 0.08317910577647109, Loss_test: 0.08228119537794923\n",
      "2921 - Loss_train: 0.08317902139098697, Loss_test: 0.08228136090468408\n",
      "2922 - Loss_train: 0.08317893704940833, Loss_test: 0.08228152637298652\n",
      "2923 - Loss_train: 0.08317885275173173, Loss_test: 0.08228169185414233\n",
      "2924 - Loss_train: 0.08317876849804977, Loss_test: 0.08228185729862922\n",
      "2925 - Loss_train: 0.083178684288232, Loss_test: 0.08228202273018184\n",
      "2926 - Loss_train: 0.08317860012230571, Loss_test: 0.08228218812428864\n",
      "2927 - Loss_train: 0.08317851600008515, Loss_test: 0.08228235350917576\n",
      "2928 - Loss_train: 0.08317843192164183, Loss_test: 0.08228251887601624\n",
      "2929 - Loss_train: 0.08317834788701271, Loss_test: 0.082282684217932\n",
      "2930 - Loss_train: 0.08317826389613175, Loss_test: 0.08228284953333243\n",
      "2931 - Loss_train: 0.08317817994902148, Loss_test: 0.08228301483457454\n",
      "2932 - Loss_train: 0.08317809604560504, Loss_test: 0.08228318012262711\n",
      "2933 - Loss_train: 0.08317801218602931, Loss_test: 0.08228334537641345\n",
      "2934 - Loss_train: 0.08317792836992842, Loss_test: 0.0822835106148086\n",
      "2935 - Loss_train: 0.08317784459765037, Loss_test: 0.08228367583038641\n",
      "2936 - Loss_train: 0.08317776086897842, Loss_test: 0.08228384101386477\n",
      "2937 - Loss_train: 0.08317767718395219, Loss_test: 0.08228400618705488\n",
      "2938 - Loss_train: 0.08317759354231305, Loss_test: 0.08228417133993374\n",
      "2939 - Loss_train: 0.08317750994419897, Loss_test: 0.08228433647583257\n",
      "2940 - Loss_train: 0.08317742638970853, Loss_test: 0.08228450158392953\n",
      "2941 - Loss_train: 0.08317734287864519, Loss_test: 0.08228466668856649\n",
      "2942 - Loss_train: 0.08317725941109856, Loss_test: 0.08228483174383593\n",
      "2943 - Loss_train: 0.08317717598705156, Loss_test: 0.08228499678722637\n",
      "2944 - Loss_train: 0.08317709260640872, Loss_test: 0.08228516181836533\n",
      "2945 - Loss_train: 0.08317700926934597, Loss_test: 0.08228532683511552\n",
      "2946 - Loss_train: 0.08317692597565879, Loss_test: 0.08228549180810672\n",
      "2947 - Loss_train: 0.08317684272518927, Loss_test: 0.08228565677945136\n",
      "2948 - Loss_train: 0.08317675951808537, Loss_test: 0.08228582171853928\n",
      "2949 - Loss_train: 0.08317667635432174, Loss_test: 0.08228598664638069\n",
      "2950 - Loss_train: 0.08317659323385, Loss_test: 0.08228615154284415\n",
      "2951 - Loss_train: 0.08317651015666735, Loss_test: 0.08228631642008162\n",
      "2952 - Loss_train: 0.08317642712293528, Loss_test: 0.08228648127540276\n",
      "2953 - Loss_train: 0.0831763441322227, Loss_test: 0.08228664612042146\n",
      "2954 - Loss_train: 0.08317626118472032, Loss_test: 0.08228681092162848\n",
      "2955 - Loss_train: 0.08317617828041886, Loss_test: 0.08228697573290371\n",
      "2956 - Loss_train: 0.08317609541930915, Loss_test: 0.0822871404947085\n",
      "2957 - Loss_train: 0.08317601260150734, Loss_test: 0.08228730524335132\n",
      "2958 - Loss_train: 0.08317592982665625, Loss_test: 0.08228746999102628\n",
      "2959 - Loss_train: 0.08317584709490151, Loss_test: 0.08228763468775427\n",
      "2960 - Loss_train: 0.08317576440642455, Loss_test: 0.08228779938139413\n",
      "2961 - Loss_train: 0.08317568176084719, Loss_test: 0.08228796404774777\n",
      "2962 - Loss_train: 0.08317559915844891, Loss_test: 0.08228812869355197\n",
      "2963 - Loss_train: 0.08317551659908978, Loss_test: 0.08228829331695332\n",
      "2964 - Loss_train: 0.08317543408272897, Loss_test: 0.0822884579259914\n",
      "2965 - Loss_train: 0.08317535160907225, Loss_test: 0.08228862250470063\n",
      "2966 - Loss_train: 0.0831752691783928, Loss_test: 0.08228878706375665\n",
      "2967 - Loss_train: 0.08317518679063132, Loss_test: 0.08228895160907979\n",
      "2968 - Loss_train: 0.08317510444582939, Loss_test: 0.08228911613047504\n",
      "2969 - Loss_train: 0.08317502214391591, Loss_test: 0.08228928062799225\n",
      "2970 - Loss_train: 0.08317493988486332, Loss_test: 0.08228944511400371\n",
      "2971 - Loss_train: 0.08317485766877172, Loss_test: 0.08228960955373975\n",
      "2972 - Loss_train: 0.08317477549535808, Loss_test: 0.08228977400963165\n",
      "2973 - Loss_train: 0.08317469336473625, Loss_test: 0.08228993838678422\n",
      "2974 - Loss_train: 0.0831746112768352, Loss_test: 0.08229010281125611\n",
      "2975 - Loss_train: 0.08317452923189922, Loss_test: 0.08229026716348808\n",
      "2976 - Loss_train: 0.08317444722964334, Loss_test: 0.08229043152396062\n",
      "2977 - Loss_train: 0.08317436526989641, Loss_test: 0.08229059584353855\n",
      "2978 - Loss_train: 0.08317428335303244, Loss_test: 0.08229076015739828\n",
      "2979 - Loss_train: 0.08317420147869553, Loss_test: 0.08229092443410625\n",
      "2980 - Loss_train: 0.08317411964702127, Loss_test: 0.08229108870197695\n",
      "2981 - Loss_train: 0.08317403785786957, Loss_test: 0.08229125293902724\n",
      "2982 - Loss_train: 0.08317395611137769, Loss_test: 0.08229141717558304\n",
      "2983 - Loss_train: 0.08317387440742068, Loss_test: 0.08229158136450142\n",
      "2984 - Loss_train: 0.08317379274602552, Loss_test: 0.08229174555719068\n",
      "2985 - Loss_train: 0.08317371112713501, Loss_test: 0.08229190970320287\n",
      "2986 - Loss_train: 0.08317362955074015, Loss_test: 0.08229207385302546\n",
      "2987 - Loss_train: 0.08317354801705223, Loss_test: 0.08229223796811885\n",
      "2988 - Loss_train: 0.08317346652558276, Loss_test: 0.08229240204425936\n",
      "2989 - Loss_train: 0.08317338507674106, Loss_test: 0.0822925661300081\n",
      "2990 - Loss_train: 0.08317330367036067, Loss_test: 0.08229273017424102\n",
      "2991 - Loss_train: 0.08317322230608823, Loss_test: 0.08229289420178158\n",
      "2992 - Loss_train: 0.08317314098438319, Loss_test: 0.08229305821601188\n",
      "2993 - Loss_train: 0.08317305970489682, Loss_test: 0.08229322219906075\n",
      "2994 - Loss_train: 0.08317297846777826, Loss_test: 0.08229338617285803\n",
      "2995 - Loss_train: 0.08317289727291484, Loss_test: 0.08229355010978487\n",
      "2996 - Loss_train: 0.08317281612037629, Loss_test: 0.08229371404127077\n",
      "2997 - Loss_train: 0.08317273501007041, Loss_test: 0.08229387793683003\n",
      "2998 - Loss_train: 0.08317265394197122, Loss_test: 0.08229404181976921\n",
      "2999 - Loss_train: 0.08317257291612748, Loss_test: 0.08229420567812355\n",
      "3000 - Loss_train: 0.08317249193245753, Loss_test: 0.08229436951806686\n",
      "3001 - Loss_train: 0.08317241099093692, Loss_test: 0.08229453332515706\n",
      "3002 - Loss_train: 0.08317233009174908, Loss_test: 0.08229469712274691\n",
      "3003 - Loss_train: 0.08317224923468025, Loss_test: 0.08229486089616485\n",
      "3004 - Loss_train: 0.08317216841956743, Loss_test: 0.08229502465256014\n",
      "3005 - Loss_train: 0.08317208764653077, Loss_test: 0.0822951883865133\n",
      "3006 - Loss_train: 0.08317200691556349, Loss_test: 0.08229535208839475\n",
      "3007 - Loss_train: 0.08317192622678476, Loss_test: 0.08229551578443933\n",
      "3008 - Loss_train: 0.08317184558004675, Loss_test: 0.08229567944295897\n",
      "3009 - Loss_train: 0.08317176497527458, Loss_test: 0.0822958430922528\n",
      "3010 - Loss_train: 0.08317168441243865, Loss_test: 0.08229600671447625\n",
      "3011 - Loss_train: 0.08317160389136427, Loss_test: 0.0822961703148626\n",
      "3012 - Loss_train: 0.0831715234122848, Loss_test: 0.0822963338964331\n",
      "3013 - Loss_train: 0.08317144297528323, Loss_test: 0.08229649746079952\n",
      "3014 - Loss_train: 0.08317136258002675, Loss_test: 0.08229666098890043\n",
      "3015 - Loss_train: 0.08317128222685258, Loss_test: 0.08229682450198948\n",
      "3016 - Loss_train: 0.08317120191529125, Loss_test: 0.08229698799593303\n",
      "3017 - Loss_train: 0.08317112164549803, Loss_test: 0.08229715147025943\n",
      "3018 - Loss_train: 0.08317104141760895, Loss_test: 0.08229731493038912\n",
      "3019 - Loss_train: 0.08317096123143052, Loss_test: 0.08229747835381034\n",
      "3020 - Loss_train: 0.08317088108699748, Loss_test: 0.0822976417645685\n",
      "3021 - Loss_train: 0.08317080098434283, Loss_test: 0.08229780515262837\n",
      "3022 - Loss_train: 0.08317072092352938, Loss_test: 0.08229796852583124\n",
      "3023 - Loss_train: 0.08317064090441825, Loss_test: 0.08229813187204282\n",
      "3024 - Loss_train: 0.0831705609268427, Loss_test: 0.0822982951821591\n",
      "3025 - Loss_train: 0.08317048099106951, Loss_test: 0.08229845848863501\n",
      "3026 - Loss_train: 0.08317040109672416, Loss_test: 0.08229862176074672\n",
      "3027 - Loss_train: 0.08317032124404439, Loss_test: 0.08229878502571243\n",
      "3028 - Loss_train: 0.08317024143290704, Loss_test: 0.0822989482600866\n",
      "3029 - Loss_train: 0.08317016166330621, Loss_test: 0.0822991114656398\n",
      "3030 - Loss_train: 0.08317008193541094, Loss_test: 0.08229927467769502\n",
      "3031 - Loss_train: 0.08317000224901999, Loss_test: 0.08229943783542802\n",
      "3032 - Loss_train: 0.08316992260409574, Loss_test: 0.08229960100492306\n",
      "3033 - Loss_train: 0.08316984300050469, Loss_test: 0.0822997641101977\n",
      "3034 - Loss_train: 0.08316976343838318, Loss_test: 0.08229992722686927\n",
      "3035 - Loss_train: 0.08316968391786454, Loss_test: 0.0823000903147757\n",
      "3036 - Loss_train: 0.08316960443866053, Loss_test: 0.08230025338003977\n",
      "3037 - Loss_train: 0.08316952500071362, Loss_test: 0.08230041642384539\n",
      "3038 - Loss_train: 0.08316944560416024, Loss_test: 0.08230057943869434\n",
      "3039 - Loss_train: 0.08316936624906586, Loss_test: 0.08230074244471787\n",
      "3040 - Loss_train: 0.08316928693519057, Loss_test: 0.0823009054122846\n",
      "3041 - Loss_train: 0.08316920766266125, Loss_test: 0.08230106837760888\n",
      "3042 - Loss_train: 0.08316912843156107, Loss_test: 0.0823012313071607\n",
      "3043 - Loss_train: 0.08316904924159638, Loss_test: 0.08230139422411796\n",
      "3044 - Loss_train: 0.08316897009287702, Loss_test: 0.08230155711162089\n",
      "3045 - Loss_train: 0.0831688909854543, Loss_test: 0.08230171998690754\n",
      "3046 - Loss_train: 0.08316881191912888, Loss_test: 0.08230188282365208\n",
      "3047 - Loss_train: 0.08316873289389975, Loss_test: 0.0823020456610554\n",
      "3048 - Loss_train: 0.08316865390985509, Loss_test: 0.08230220846296621\n",
      "3049 - Loss_train: 0.0831685749669472, Loss_test: 0.08230237124416709\n",
      "3050 - Loss_train: 0.08316849606515551, Loss_test: 0.08230253399593838\n",
      "3051 - Loss_train: 0.08316841720445255, Loss_test: 0.08230269675127751\n",
      "3052 - Loss_train: 0.08316833838497928, Loss_test: 0.08230285946345331\n",
      "3053 - Loss_train: 0.08316825960647636, Loss_test: 0.08230302217779464\n",
      "3054 - Loss_train: 0.08316818086892382, Loss_test: 0.08230318483003832\n",
      "3055 - Loss_train: 0.08316810217235031, Loss_test: 0.08230334749501615\n",
      "3056 - Loss_train: 0.08316802351681668, Loss_test: 0.08230351012156369\n",
      "3057 - Loss_train: 0.0831679449023306, Loss_test: 0.08230367272280203\n",
      "3058 - Loss_train: 0.08316786632860362, Loss_test: 0.08230383531988107\n",
      "3059 - Loss_train: 0.08316778779578779, Loss_test: 0.08230399787705087\n",
      "3060 - Loss_train: 0.08316770930392173, Loss_test: 0.08230416043900916\n",
      "3061 - Loss_train: 0.08316763085294566, Loss_test: 0.08230432293321607\n",
      "3062 - Loss_train: 0.08316755244275198, Loss_test: 0.08230448546692681\n",
      "3063 - Loss_train: 0.08316747407341549, Loss_test: 0.0823046479371965\n",
      "3064 - Loss_train: 0.08316739574488739, Loss_test: 0.08230481039761804\n",
      "3065 - Loss_train: 0.08316731745724383, Loss_test: 0.08230497281888771\n",
      "3066 - Loss_train: 0.08316723921031827, Loss_test: 0.08230513523939482\n",
      "3067 - Loss_train: 0.08316716100427603, Loss_test: 0.082305297632321\n",
      "3068 - Loss_train: 0.08316708283879598, Loss_test: 0.08230546000841428\n",
      "3069 - Loss_train: 0.08316700471418273, Loss_test: 0.08230562234761528\n",
      "3070 - Loss_train: 0.08316692663024597, Loss_test: 0.08230578468267445\n",
      "3071 - Loss_train: 0.08316684858680495, Loss_test: 0.08230594698107155\n",
      "3072 - Loss_train: 0.08316677058406596, Loss_test: 0.082306109274985\n",
      "3073 - Loss_train: 0.08316669262189803, Loss_test: 0.08230627153035824\n",
      "3074 - Loss_train: 0.08316661470043077, Loss_test: 0.0823064337662411\n",
      "3075 - Loss_train: 0.0831665368195613, Loss_test: 0.08230659599303175\n",
      "3076 - Loss_train: 0.08316645897910936, Loss_test: 0.08230675818324501\n",
      "3077 - Loss_train: 0.0831663811792452, Loss_test: 0.08230692035990846\n",
      "3078 - Loss_train: 0.08316630341981696, Loss_test: 0.08230708250225399\n",
      "3079 - Loss_train: 0.0831662257009577, Loss_test: 0.08230724465644702\n",
      "3080 - Loss_train: 0.08316614802250795, Loss_test: 0.08230740674633787\n",
      "3081 - Loss_train: 0.0831660703847403, Loss_test: 0.08230756884303086\n",
      "3082 - Loss_train: 0.08316599278733988, Loss_test: 0.08230773090553795\n",
      "3083 - Loss_train: 0.08316591523020618, Loss_test: 0.08230789295676434\n",
      "3084 - Loss_train: 0.08316583771344134, Loss_test: 0.08230805496987097\n",
      "3085 - Loss_train: 0.08316576023718031, Loss_test: 0.08230821697865494\n",
      "3086 - Loss_train: 0.08316568280129354, Loss_test: 0.08230837894305494\n",
      "3087 - Loss_train: 0.08316560540558793, Loss_test: 0.082308540912781\n",
      "3088 - Loss_train: 0.08316552805015162, Loss_test: 0.08230870283709907\n",
      "3089 - Loss_train: 0.08316545073499228, Loss_test: 0.08230886475234879\n",
      "3090 - Loss_train: 0.08316537346011707, Loss_test: 0.0823090266504696\n",
      "3091 - Loss_train: 0.0831652962255644, Loss_test: 0.08230918852378986\n",
      "3092 - Loss_train: 0.08316521903123712, Loss_test: 0.08230935036337161\n",
      "3093 - Loss_train: 0.08316514187703344, Loss_test: 0.08230951218427408\n",
      "3094 - Loss_train: 0.08316506476317037, Loss_test: 0.08230967398978695\n",
      "3095 - Loss_train: 0.08316498768945375, Loss_test: 0.08230983578175052\n",
      "3096 - Loss_train: 0.08316491065573044, Loss_test: 0.08230999753409222\n",
      "3097 - Loss_train: 0.08316483366226073, Loss_test: 0.08231015927265868\n",
      "3098 - Loss_train: 0.0831647567088869, Loss_test: 0.08231032098171863\n",
      "3099 - Loss_train: 0.08316467979543701, Loss_test: 0.0823104826865867\n",
      "3100 - Loss_train: 0.08316460292197733, Loss_test: 0.08231064436202644\n",
      "3101 - Loss_train: 0.08316452608881332, Loss_test: 0.08231080599489873\n",
      "3102 - Loss_train: 0.0831644492956266, Loss_test: 0.0823109676299376\n",
      "3103 - Loss_train: 0.08316437254242255, Loss_test: 0.08231112923918243\n",
      "3104 - Loss_train: 0.0831642958292089, Loss_test: 0.08231129081933637\n",
      "3105 - Loss_train: 0.08316421915588786, Loss_test: 0.08231145238496808\n",
      "3106 - Loss_train: 0.08316414252253271, Loss_test: 0.08231161393127862\n",
      "3107 - Loss_train: 0.08316406592891387, Loss_test: 0.08231177544518081\n",
      "3108 - Loss_train: 0.08316398937512952, Loss_test: 0.08231193694204463\n",
      "3109 - Loss_train: 0.08316391286123537, Loss_test: 0.0823120984200121\n",
      "3110 - Loss_train: 0.08316383638722225, Loss_test: 0.08231225987170795\n",
      "3111 - Loss_train: 0.08316375995322223, Loss_test: 0.08231242130736798\n",
      "3112 - Loss_train: 0.08316368355884873, Loss_test: 0.0823125827166927\n",
      "3113 - Loss_train: 0.08316360720419684, Loss_test: 0.08231274409503825\n",
      "3114 - Loss_train: 0.08316353088946656, Loss_test: 0.08231290545998254\n",
      "3115 - Loss_train: 0.08316345461445962, Loss_test: 0.08231306681563509\n",
      "3116 - Loss_train: 0.0831633783790711, Loss_test: 0.08231322811953115\n",
      "3117 - Loss_train: 0.08316330218354615, Loss_test: 0.08231338943250786\n",
      "3118 - Loss_train: 0.08316322602756539, Loss_test: 0.08231355069762517\n",
      "3119 - Loss_train: 0.08316314991136846, Loss_test: 0.08231371195341412\n",
      "3120 - Loss_train: 0.0831630738346727, Loss_test: 0.08231387318388377\n",
      "3121 - Loss_train: 0.08316299779758901, Loss_test: 0.08231403438989712\n",
      "3122 - Loss_train: 0.08316292180021621, Loss_test: 0.082314195594095\n",
      "3123 - Loss_train: 0.08316284584230993, Loss_test: 0.08231435675639515\n",
      "3124 - Loss_train: 0.08316276992394955, Loss_test: 0.08231451789509939\n",
      "3125 - Loss_train: 0.08316269404527397, Loss_test: 0.0823146790235371\n",
      "3126 - Loss_train: 0.08316261820592026, Loss_test: 0.08231484012938704\n",
      "3127 - Loss_train: 0.08316254240610176, Loss_test: 0.08231500120143974\n",
      "3128 - Loss_train: 0.08316246664577275, Loss_test: 0.08231516225552099\n",
      "3129 - Loss_train: 0.08316239092490246, Loss_test: 0.08231532329554192\n",
      "3130 - Loss_train: 0.08316231524342658, Loss_test: 0.08231548430768122\n",
      "3131 - Loss_train: 0.08316223960139155, Loss_test: 0.08231564529747919\n",
      "3132 - Loss_train: 0.08316216399871049, Loss_test: 0.08231580627018768\n",
      "3133 - Loss_train: 0.08316208843558456, Loss_test: 0.08231596721834034\n",
      "3134 - Loss_train: 0.08316201291169055, Loss_test: 0.08231612814202999\n",
      "3135 - Loss_train: 0.08316193742707927, Loss_test: 0.08231628904767872\n",
      "3136 - Loss_train: 0.08316186198183631, Loss_test: 0.08231644992463048\n",
      "3137 - Loss_train: 0.0831617865758255, Loss_test: 0.08231661078662875\n",
      "3138 - Loss_train: 0.08316171120907609, Loss_test: 0.08231677162239057\n",
      "3139 - Loss_train: 0.08316163588157136, Loss_test: 0.08231693242555858\n",
      "3140 - Loss_train: 0.08316156059334776, Loss_test: 0.08231709322674761\n",
      "3141 - Loss_train: 0.08316148534429603, Loss_test: 0.08231725399671444\n",
      "3142 - Loss_train: 0.08316141013447793, Loss_test: 0.08231741474133085\n",
      "3143 - Loss_train: 0.08316133496391005, Loss_test: 0.08231757547564784\n",
      "3144 - Loss_train: 0.0831612598323353, Loss_test: 0.08231773617332246\n",
      "3145 - Loss_train: 0.08316118473990788, Loss_test: 0.08231789685339255\n",
      "3146 - Loss_train: 0.08316110968666517, Loss_test: 0.08231805750898193\n",
      "3147 - Loss_train: 0.08316103467239057, Loss_test: 0.08231821816180256\n",
      "3148 - Loss_train: 0.08316095969719883, Loss_test: 0.08231837877080184\n",
      "3149 - Loss_train: 0.08316088476109255, Loss_test: 0.08231853936163716\n",
      "3150 - Loss_train: 0.08316080986392682, Loss_test: 0.08231869992773942\n",
      "3151 - Loss_train: 0.08316073500578254, Loss_test: 0.08231886047773329\n",
      "3152 - Loss_train: 0.0831606601867807, Loss_test: 0.08231902101375826\n",
      "3153 - Loss_train: 0.08316058540679326, Loss_test: 0.08231918151763327\n",
      "3154 - Loss_train: 0.0831605106655616, Loss_test: 0.08231934198933477\n",
      "3155 - Loss_train: 0.08316043596327972, Loss_test: 0.08231950245398022\n",
      "3156 - Loss_train: 0.08316036129994712, Loss_test: 0.08231966288751766\n",
      "3157 - Loss_train: 0.08316028667545454, Loss_test: 0.08231982331100463\n",
      "3158 - Loss_train: 0.08316021208983071, Loss_test: 0.08231998370714182\n",
      "3159 - Loss_train: 0.08316013754317159, Loss_test: 0.08232014407865572\n",
      "3160 - Loss_train: 0.08316006303513397, Loss_test: 0.08232030442273022\n",
      "3161 - Loss_train: 0.08315998856597533, Loss_test: 0.08232046475879022\n",
      "3162 - Loss_train: 0.08315991413552519, Loss_test: 0.08232062504778032\n",
      "3163 - Loss_train: 0.08315983974390796, Loss_test: 0.08232078535030485\n",
      "3164 - Loss_train: 0.08315976539095472, Loss_test: 0.08232094559554402\n",
      "3165 - Loss_train: 0.08315969107667101, Loss_test: 0.08232110583364918\n",
      "3166 - Loss_train: 0.08315961680115633, Loss_test: 0.08232126605428627\n",
      "3167 - Loss_train: 0.08315954256448341, Loss_test: 0.08232142624697084\n",
      "3168 - Loss_train: 0.08315946836632788, Loss_test: 0.08232158642021234\n",
      "3169 - Loss_train: 0.08315939420682983, Loss_test: 0.08232174656959336\n",
      "3170 - Loss_train: 0.08315932008590383, Loss_test: 0.08232190670418447\n",
      "3171 - Loss_train: 0.08315924600379375, Loss_test: 0.08232206679491014\n",
      "3172 - Loss_train: 0.08315917196001874, Loss_test: 0.08232222688370446\n",
      "3173 - Loss_train: 0.08315909795489425, Loss_test: 0.08232238694202795\n",
      "3174 - Loss_train: 0.08315902398824829, Loss_test: 0.08232254699046228\n",
      "3175 - Loss_train: 0.08315895006014029, Loss_test: 0.08232270701092702\n",
      "3176 - Loss_train: 0.08315887617057147, Loss_test: 0.08232286699515665\n",
      "3177 - Loss_train: 0.08315880231941643, Loss_test: 0.08232302696941707\n",
      "3178 - Loss_train: 0.08315872850687121, Loss_test: 0.08232318691205438\n",
      "3179 - Loss_train: 0.08315865473278981, Loss_test: 0.08232334684652166\n",
      "3180 - Loss_train: 0.08315858099694717, Loss_test: 0.08232350674168369\n",
      "3181 - Loss_train: 0.08315850729967733, Loss_test: 0.08232366663677947\n",
      "3182 - Loss_train: 0.08315843364065573, Loss_test: 0.08232382648722394\n",
      "3183 - Loss_train: 0.08315836001991023, Loss_test: 0.08232398633167691\n",
      "3184 - Loss_train: 0.08315828643756713, Loss_test: 0.08232414614022415\n",
      "3185 - Loss_train: 0.08315821289359286, Loss_test: 0.08232430593570893\n",
      "3186 - Loss_train: 0.08315813938790438, Loss_test: 0.08232446571086244\n",
      "3187 - Loss_train: 0.08315806592047936, Loss_test: 0.08232462546519358\n",
      "3188 - Loss_train: 0.08315799249133453, Loss_test: 0.08232478519152257\n",
      "3189 - Loss_train: 0.08315791910038829, Loss_test: 0.08232494488941516\n",
      "3190 - Loss_train: 0.08315784574780757, Loss_test: 0.0823251045654069\n",
      "3191 - Loss_train: 0.08315777243329976, Loss_test: 0.08232526423228707\n",
      "3192 - Loss_train: 0.08315769915698265, Loss_test: 0.08232542385650277\n",
      "3193 - Loss_train: 0.08315762591894184, Loss_test: 0.08232558349224404\n",
      "3194 - Loss_train: 0.0831575527188592, Loss_test: 0.08232574306522725\n",
      "3195 - Loss_train: 0.08315747955696402, Loss_test: 0.08232590264187084\n",
      "3196 - Loss_train: 0.08315740643329796, Loss_test: 0.08232606219536322\n",
      "3197 - Loss_train: 0.08315733334754988, Loss_test: 0.0823262217166149\n",
      "3198 - Loss_train: 0.08315726030006138, Loss_test: 0.08232638122008956\n",
      "3199 - Loss_train: 0.08315718729060737, Loss_test: 0.08232654069881509\n",
      "3200 - Loss_train: 0.0831571143190469, Loss_test: 0.08232670015979612\n",
      "3201 - Loss_train: 0.08315704138542744, Loss_test: 0.08232685959769011\n",
      "3202 - Loss_train: 0.08315696848974953, Loss_test: 0.08232701900380367\n",
      "3203 - Loss_train: 0.08315689563207704, Loss_test: 0.0823271784195958\n",
      "3204 - Loss_train: 0.08315682281234461, Loss_test: 0.08232733777549227\n",
      "3205 - Loss_train: 0.08315675003055147, Loss_test: 0.0823274971111373\n",
      "3206 - Loss_train: 0.08315667728681123, Loss_test: 0.08232765644434586\n",
      "3207 - Loss_train: 0.08315660458078185, Loss_test: 0.08232781573954603\n",
      "3208 - Loss_train: 0.0831565319126678, Loss_test: 0.08232797502819882\n",
      "3209 - Loss_train: 0.08315645928236443, Loss_test: 0.08232813427946142\n",
      "3210 - Loss_train: 0.083156386690012, Loss_test: 0.08232829352088383\n",
      "3211 - Loss_train: 0.08315631413536026, Loss_test: 0.08232845273142025\n",
      "3212 - Loss_train: 0.08315624161845421, Loss_test: 0.08232861192024254\n",
      "3213 - Loss_train: 0.08315616913932115, Loss_test: 0.08232877109156032\n",
      "3214 - Loss_train: 0.08315609669790368, Loss_test: 0.08232893023650917\n",
      "3215 - Loss_train: 0.08315602429425169, Loss_test: 0.08232908934586874\n",
      "3216 - Loss_train: 0.08315595192833128, Loss_test: 0.08232924846209544\n",
      "3217 - Loss_train: 0.08315587960002735, Loss_test: 0.08232940753393286\n",
      "3218 - Loss_train: 0.0831558073095608, Loss_test: 0.08232956659176453\n",
      "3219 - Loss_train: 0.0831557350567315, Loss_test: 0.08232972562533486\n",
      "3220 - Loss_train: 0.08315566284137262, Loss_test: 0.08232988461937366\n",
      "3221 - Loss_train: 0.08315559066370892, Loss_test: 0.0823300436273857\n",
      "3222 - Loss_train: 0.08315551852370365, Loss_test: 0.08233020257545022\n",
      "3223 - Loss_train: 0.08315544642117022, Loss_test: 0.08233036154412046\n",
      "3224 - Loss_train: 0.08315537435633295, Loss_test: 0.08233052045373401\n",
      "3225 - Loss_train: 0.08315530232880428, Loss_test: 0.08233067934217765\n",
      "3226 - Loss_train: 0.08315523033878315, Loss_test: 0.08233083822810093\n",
      "3227 - Loss_train: 0.08315515838626984, Loss_test: 0.08233099707538852\n",
      "3228 - Loss_train: 0.08315508647121829, Loss_test: 0.0823311559043508\n",
      "3229 - Loss_train: 0.08315501459378247, Loss_test: 0.08233131472709668\n",
      "3230 - Loss_train: 0.08315494275359099, Loss_test: 0.08233147350193322\n",
      "3231 - Loss_train: 0.08315487095099632, Loss_test: 0.08233163227028957\n",
      "3232 - Loss_train: 0.0831547991856615, Loss_test: 0.08233179101811511\n",
      "3233 - Loss_train: 0.08315472745766125, Loss_test: 0.08233194971774802\n",
      "3234 - Loss_train: 0.08315465576724063, Loss_test: 0.08233210842372513\n",
      "3235 - Loss_train: 0.08315458411394393, Loss_test: 0.08233226710143562\n",
      "3236 - Loss_train: 0.0831545124979656, Loss_test: 0.08233242574495125\n",
      "3237 - Loss_train: 0.08315444091932253, Loss_test: 0.08233258438331696\n",
      "3238 - Loss_train: 0.08315436937786545, Loss_test: 0.08233274297286204\n",
      "3239 - Loss_train: 0.08315429787379795, Loss_test: 0.0823329015756404\n",
      "3240 - Loss_train: 0.0831542264068903, Loss_test: 0.0823330601247248\n",
      "3241 - Loss_train: 0.0831541549772602, Loss_test: 0.08233321866261972\n",
      "3242 - Loss_train: 0.08315408358480088, Loss_test: 0.08233337719104633\n",
      "3243 - Loss_train: 0.08315401222954061, Loss_test: 0.08233353568525936\n",
      "3244 - Loss_train: 0.08315394091158523, Loss_test: 0.08233369414107933\n",
      "3245 - Loss_train: 0.08315386963071797, Loss_test: 0.08233385258955582\n",
      "3246 - Loss_train: 0.08315379838680792, Loss_test: 0.08233401101651655\n",
      "3247 - Loss_train: 0.08315372718004858, Loss_test: 0.0823341694332913\n",
      "3248 - Loss_train: 0.0831536560105066, Loss_test: 0.08233432779412705\n",
      "3249 - Loss_train: 0.08315358487786158, Loss_test: 0.08233448616716464\n",
      "3250 - Loss_train: 0.08315351378227565, Loss_test: 0.08233464449469491\n",
      "3251 - Loss_train: 0.08315344272382572, Loss_test: 0.0823348028086156\n",
      "3252 - Loss_train: 0.08315337170240605, Loss_test: 0.08233496110596268\n",
      "3253 - Loss_train: 0.08315330071782848, Loss_test: 0.08233511936147493\n",
      "3254 - Loss_train: 0.08315322977023175, Loss_test: 0.08233527762121057\n",
      "3255 - Loss_train: 0.08315315885956823, Loss_test: 0.08233543583588812\n",
      "3256 - Loss_train: 0.08315308798597976, Loss_test: 0.08233559404309373\n",
      "3257 - Loss_train: 0.08315301714917425, Loss_test: 0.082335752205725\n",
      "3258 - Loss_train: 0.08315294634934407, Loss_test: 0.08233591035654766\n",
      "3259 - Loss_train: 0.08315287558629633, Loss_test: 0.08233606850623858\n",
      "3260 - Loss_train: 0.08315280486020167, Loss_test: 0.0823362266126224\n",
      "3261 - Loss_train: 0.08315273417082447, Loss_test: 0.08233638470504862\n",
      "3262 - Loss_train: 0.08315266351828357, Loss_test: 0.08233654276289507\n",
      "3263 - Loss_train: 0.0831525929024898, Loss_test: 0.08233670080757033\n",
      "3264 - Loss_train: 0.08315252232352048, Loss_test: 0.0823368588210318\n",
      "3265 - Loss_train: 0.08315245178134871, Loss_test: 0.08233701682007752\n",
      "3266 - Loss_train: 0.08315238127583702, Loss_test: 0.08233717479960552\n",
      "3267 - Loss_train: 0.08315231080721253, Loss_test: 0.08233733274639052\n",
      "3268 - Loss_train: 0.08315224037523401, Loss_test: 0.0823374906822313\n",
      "3269 - Loss_train: 0.08315216997982718, Loss_test: 0.08233764858699028\n",
      "3270 - Loss_train: 0.08315209962117817, Loss_test: 0.0823378064582948\n",
      "3271 - Loss_train: 0.0831520292990524, Loss_test: 0.082337964339378\n",
      "3272 - Loss_train: 0.0831519590137186, Loss_test: 0.08233812216388856\n",
      "3273 - Loss_train: 0.08315188876484954, Loss_test: 0.0823382799844876\n",
      "3274 - Loss_train: 0.08315181855256398, Loss_test: 0.08233843776994995\n",
      "3275 - Loss_train: 0.08315174837682165, Loss_test: 0.08233859554571779\n",
      "3276 - Loss_train: 0.08315167823780452, Loss_test: 0.08233875330723181\n",
      "3277 - Loss_train: 0.08315160813509381, Loss_test: 0.08233891101775319\n",
      "3278 - Loss_train: 0.08315153806917383, Loss_test: 0.08233906872516007\n",
      "3279 - Loss_train: 0.08315146803966522, Loss_test: 0.08233922640640802\n",
      "3280 - Loss_train: 0.08315139804666127, Loss_test: 0.08233938406111237\n",
      "3281 - Loss_train: 0.08315132809008902, Loss_test: 0.08233954169700965\n",
      "3282 - Loss_train: 0.08315125816989288, Loss_test: 0.0823396993100494\n",
      "3283 - Loss_train: 0.0831511882860738, Loss_test: 0.08233985690245588\n",
      "3284 - Loss_train: 0.08315111843864029, Loss_test: 0.08234001446665556\n",
      "3285 - Loss_train: 0.08315104862757014, Loss_test: 0.08234017200268885\n",
      "3286 - Loss_train: 0.08315097885284449, Loss_test: 0.08234032953281088\n",
      "3287 - Loss_train: 0.08315090911436893, Loss_test: 0.08234048702108555\n",
      "3288 - Loss_train: 0.08315083941220859, Loss_test: 0.08234064449551842\n",
      "3289 - Loss_train: 0.08315076974650665, Loss_test: 0.08234080194802314\n",
      "3290 - Loss_train: 0.0831507001171155, Loss_test: 0.08234095938986542\n",
      "3291 - Loss_train: 0.08315063052380503, Loss_test: 0.0823411167877795\n",
      "3292 - Loss_train: 0.08315056096676061, Loss_test: 0.08234127417386886\n",
      "3293 - Loss_train: 0.08315049144595864, Loss_test: 0.08234143153789045\n",
      "3294 - Loss_train: 0.08315042196143141, Loss_test: 0.08234158888025636\n",
      "3295 - Loss_train: 0.08315035251319423, Loss_test: 0.08234174619871507\n",
      "3296 - Loss_train: 0.08315028310108681, Loss_test: 0.08234190349590297\n",
      "3297 - Loss_train: 0.08315021372505582, Loss_test: 0.0823420607628238\n",
      "3298 - Loss_train: 0.08315014438503836, Loss_test: 0.08234221800081228\n",
      "3299 - Loss_train: 0.08315007508120735, Loss_test: 0.08234237523901289\n",
      "3300 - Loss_train: 0.08315000581342052, Loss_test: 0.08234253244049426\n",
      "3301 - Loss_train: 0.0831499365817203, Loss_test: 0.08234268961850674\n",
      "3302 - Loss_train: 0.08314986738614116, Loss_test: 0.08234284677236761\n",
      "3303 - Loss_train: 0.0831497982266176, Loss_test: 0.0823430039214376\n",
      "3304 - Loss_train: 0.08314972910308809, Loss_test: 0.08234316102692212\n",
      "3305 - Loss_train: 0.08314966001562715, Loss_test: 0.0823433181072411\n",
      "3306 - Loss_train: 0.08314959096398929, Loss_test: 0.082343475185348\n",
      "3307 - Loss_train: 0.08314952194836502, Loss_test: 0.08234363220916731\n",
      "3308 - Loss_train: 0.08314945296866996, Loss_test: 0.08234378924987187\n",
      "3309 - Loss_train: 0.08314938402489878, Loss_test: 0.08234394624984524\n",
      "3310 - Loss_train: 0.083149315117017, Loss_test: 0.08234410322295536\n",
      "3311 - Loss_train: 0.08314924624508085, Loss_test: 0.0823442601751077\n",
      "3312 - Loss_train: 0.08314917740892389, Loss_test: 0.08234441709021376\n",
      "3313 - Loss_train: 0.08314910860859948, Loss_test: 0.08234457401874291\n",
      "3314 - Loss_train: 0.08314903984412941, Loss_test: 0.08234473090023137\n",
      "3315 - Loss_train: 0.08314897111552795, Loss_test: 0.08234488774620483\n",
      "3316 - Loss_train: 0.08314890242265846, Loss_test: 0.08234504459705796\n",
      "3317 - Loss_train: 0.08314883376573268, Loss_test: 0.08234520141338172\n",
      "3318 - Loss_train: 0.08314876514445188, Loss_test: 0.08234535820603656\n",
      "3319 - Loss_train: 0.08314869655889531, Loss_test: 0.08234551496747923\n",
      "3320 - Loss_train: 0.08314862800919812, Loss_test: 0.08234567172457602\n",
      "3321 - Loss_train: 0.08314855949503536, Loss_test: 0.08234582844587679\n",
      "3322 - Loss_train: 0.0831484910166857, Loss_test: 0.08234598514757158\n",
      "3323 - Loss_train: 0.08314842257388914, Loss_test: 0.08234614182752219\n",
      "3324 - Loss_train: 0.08314835416671178, Loss_test: 0.0823462984914619\n",
      "3325 - Loss_train: 0.08314828579535224, Loss_test: 0.08234645511608395\n",
      "3326 - Loss_train: 0.08314821745945276, Loss_test: 0.08234661173034036\n",
      "3327 - Loss_train: 0.0831481491591542, Loss_test: 0.0823467683145259\n",
      "3328 - Loss_train: 0.08314808089435595, Loss_test: 0.08234692487969522\n",
      "3329 - Loss_train: 0.08314801266515032, Loss_test: 0.0823470814232339\n",
      "3330 - Loss_train: 0.08314794447145435, Loss_test: 0.08234723794495158\n",
      "3331 - Loss_train: 0.08314787631324758, Loss_test: 0.08234739443831787\n",
      "3332 - Loss_train: 0.08314780819054905, Loss_test: 0.08234755092174799\n",
      "3333 - Loss_train: 0.08314774010334518, Loss_test: 0.0823477073563454\n",
      "3334 - Loss_train: 0.08314767205160685, Loss_test: 0.08234786378641021\n",
      "3335 - Loss_train: 0.0831476040352779, Loss_test: 0.08234802020396899\n",
      "3336 - Loss_train: 0.0831475360543964, Loss_test: 0.08234817656814795\n",
      "3337 - Loss_train: 0.0831474681090737, Loss_test: 0.08234833294267335\n",
      "3338 - Loss_train: 0.08314740019897417, Loss_test: 0.08234848927075036\n",
      "3339 - Loss_train: 0.0831473323242566, Loss_test: 0.08234864558755603\n",
      "3340 - Loss_train: 0.08314726448489512, Loss_test: 0.08234880187029735\n",
      "3341 - Loss_train: 0.08314719668082747, Loss_test: 0.08234895814866089\n",
      "3342 - Loss_train: 0.08314712891224489, Loss_test: 0.08234911439529269\n",
      "3343 - Loss_train: 0.08314706117877708, Loss_test: 0.08234927061699014\n",
      "3344 - Loss_train: 0.08314699348063828, Loss_test: 0.0823494268162196\n",
      "3345 - Loss_train: 0.08314692581769736, Loss_test: 0.0823495829828943\n",
      "3346 - Loss_train: 0.08314685819002025, Loss_test: 0.08234973915532694\n",
      "3347 - Loss_train: 0.08314679059752582, Loss_test: 0.08234989527683167\n",
      "3348 - Loss_train: 0.08314672304018195, Loss_test: 0.08235005137582409\n",
      "3349 - Loss_train: 0.0831466555180885, Loss_test: 0.08235020745853504\n",
      "3350 - Loss_train: 0.08314658803134958, Loss_test: 0.08235036353119128\n",
      "3351 - Loss_train: 0.08314652057957705, Loss_test: 0.0823505195650829\n",
      "3352 - Loss_train: 0.08314645316304979, Loss_test: 0.08235067557856621\n",
      "3353 - Loss_train: 0.08314638578151788, Loss_test: 0.08235083158341425\n",
      "3354 - Loss_train: 0.08314631843505879, Loss_test: 0.08235098754765807\n",
      "3355 - Loss_train: 0.08314625112378635, Loss_test: 0.08235114349120524\n",
      "3356 - Loss_train: 0.08314618384761163, Loss_test: 0.08235129941777222\n",
      "3357 - Loss_train: 0.08314611660636174, Loss_test: 0.08235145531667801\n",
      "3358 - Loss_train: 0.08314604940006945, Loss_test: 0.08235161119924399\n",
      "3359 - Loss_train: 0.0831459822288266, Loss_test: 0.08235176704780846\n",
      "3360 - Loss_train: 0.08314591509257639, Loss_test: 0.08235192289079532\n",
      "3361 - Loss_train: 0.08314584799129286, Loss_test: 0.08235207869021752\n",
      "3362 - Loss_train: 0.08314578092489981, Loss_test: 0.08235223447846568\n",
      "3363 - Loss_train: 0.0831457138934573, Loss_test: 0.08235239024056855\n",
      "3364 - Loss_train: 0.08314564689692992, Loss_test: 0.08235254597975464\n",
      "3365 - Loss_train: 0.08314557993528053, Loss_test: 0.0823527017027089\n",
      "3366 - Loss_train: 0.0831455130085429, Loss_test: 0.08235285739757027\n",
      "3367 - Loss_train: 0.08314544611662432, Loss_test: 0.08235301306179851\n",
      "3368 - Loss_train: 0.08314537925953719, Loss_test: 0.08235316872707533\n",
      "3369 - Loss_train: 0.0831453124373423, Loss_test: 0.08235332435531262\n",
      "3370 - Loss_train: 0.0831452456500794, Loss_test: 0.08235347993217737\n",
      "3371 - Loss_train: 0.08314517889742087, Loss_test: 0.08235363552598046\n",
      "3372 - Loss_train: 0.08314511217952117, Loss_test: 0.08235379108118186\n",
      "3373 - Loss_train: 0.08314504549637376, Loss_test: 0.0823539466187565\n",
      "3374 - Loss_train: 0.08314497884811264, Loss_test: 0.08235410212487826\n",
      "3375 - Loss_train: 0.0831449122344363, Loss_test: 0.08235425761099192\n",
      "3376 - Loss_train: 0.08314484565539983, Loss_test: 0.0823544130716712\n",
      "3377 - Loss_train: 0.08314477911119313, Loss_test: 0.08235456852291807\n",
      "3378 - Loss_train: 0.08314471260153068, Loss_test: 0.08235472393994919\n",
      "3379 - Loss_train: 0.08314464612651977, Loss_test: 0.08235487934363894\n",
      "3380 - Loss_train: 0.08314457968609457, Loss_test: 0.08235503470788133\n",
      "3381 - Loss_train: 0.0831445132802394, Loss_test: 0.08235519004928776\n",
      "3382 - Loss_train: 0.0831444469089913, Loss_test: 0.0823553453825855\n",
      "3383 - Loss_train: 0.0831443805723153, Loss_test: 0.08235550069390611\n",
      "3384 - Loss_train: 0.0831443142703027, Loss_test: 0.08235565597032225\n",
      "3385 - Loss_train: 0.08314424800269286, Loss_test: 0.08235581122071517\n",
      "3386 - Loss_train: 0.08314418176966953, Loss_test: 0.08235596647453453\n",
      "3387 - Loss_train: 0.08314411557119979, Loss_test: 0.08235612167501294\n",
      "3388 - Loss_train: 0.08314404940707802, Loss_test: 0.08235627687197555\n",
      "3389 - Loss_train: 0.08314398327735686, Loss_test: 0.0823564320278493\n",
      "3390 - Loss_train: 0.0831439171820984, Loss_test: 0.08235658717190225\n",
      "3391 - Loss_train: 0.08314385112127567, Loss_test: 0.08235674229828582\n",
      "3392 - Loss_train: 0.08314378509489115, Loss_test: 0.08235689739273587\n",
      "3393 - Loss_train: 0.08314371910300415, Loss_test: 0.08235705245596665\n",
      "3394 - Loss_train: 0.08314365314540194, Loss_test: 0.0823572075094511\n",
      "3395 - Loss_train: 0.0831435872222585, Loss_test: 0.08235736253754386\n",
      "3396 - Loss_train: 0.083143521333292, Loss_test: 0.08235751755140455\n",
      "3397 - Loss_train: 0.08314345547868714, Loss_test: 0.08235767252563814\n",
      "3398 - Loss_train: 0.08314338965837126, Loss_test: 0.08235782747621107\n",
      "3399 - Loss_train: 0.08314332387242716, Loss_test: 0.08235798242073615\n",
      "3400 - Loss_train: 0.08314325812077346, Loss_test: 0.08235813732275493\n",
      "3401 - Loss_train: 0.08314319240321065, Loss_test: 0.08235829222885822\n",
      "3402 - Loss_train: 0.08314312672000551, Loss_test: 0.08235844707782913\n",
      "3403 - Loss_train: 0.08314306107084224, Loss_test: 0.0823586019248618\n",
      "3404 - Loss_train: 0.08314299545593305, Loss_test: 0.08235875674782284\n",
      "3405 - Loss_train: 0.08314292987525651, Loss_test: 0.0823589115442427\n",
      "3406 - Loss_train: 0.0831428643288049, Loss_test: 0.08235906631339233\n",
      "3407 - Loss_train: 0.0831427988163485, Loss_test: 0.08235922107043643\n",
      "3408 - Loss_train: 0.08314273333797582, Loss_test: 0.08235937579487752\n",
      "3409 - Loss_train: 0.08314266789370997, Loss_test: 0.08235953049613885\n",
      "3410 - Loss_train: 0.08314260248356758, Loss_test: 0.08235968518021895\n",
      "3411 - Loss_train: 0.0831425371074672, Loss_test: 0.08235983984057672\n",
      "3412 - Loss_train: 0.08314247176538306, Loss_test: 0.08235999446022936\n",
      "3413 - Loss_train: 0.0831424064573395, Loss_test: 0.08236014909093155\n",
      "3414 - Loss_train: 0.08314234118324684, Loss_test: 0.08236030367145167\n",
      "3415 - Loss_train: 0.08314227594318245, Loss_test: 0.08236045824250564\n",
      "3416 - Loss_train: 0.08314221073719627, Loss_test: 0.08236061278867453\n",
      "3417 - Loss_train: 0.08314214556507495, Loss_test: 0.08236076729889284\n",
      "3418 - Loss_train: 0.08314208042690285, Loss_test: 0.08236092179211653\n",
      "3419 - Loss_train: 0.08314201532264619, Loss_test: 0.08236107626896759\n",
      "3420 - Loss_train: 0.08314195025234712, Loss_test: 0.08236123071830143\n",
      "3421 - Loss_train: 0.08314188521585672, Loss_test: 0.08236138514575345\n",
      "3422 - Loss_train: 0.0831418202132932, Loss_test: 0.08236153956149987\n",
      "3423 - Loss_train: 0.08314175524471358, Loss_test: 0.08236169392675623\n",
      "3424 - Loss_train: 0.08314169030996393, Loss_test: 0.08236184829538469\n",
      "3425 - Loss_train: 0.08314162540894102, Loss_test: 0.0823620026170055\n",
      "3426 - Loss_train: 0.08314156054178157, Loss_test: 0.08236215693252777\n",
      "3427 - Loss_train: 0.08314149570841382, Loss_test: 0.08236231122509102\n",
      "3428 - Loss_train: 0.08314143090876783, Loss_test: 0.0823624654824847\n",
      "3429 - Loss_train: 0.08314136614295901, Loss_test: 0.08236261972727311\n",
      "3430 - Loss_train: 0.08314130141081626, Loss_test: 0.08236277394733063\n",
      "3431 - Loss_train: 0.08314123671234007, Loss_test: 0.08236292814752694\n",
      "3432 - Loss_train: 0.08314117204746758, Loss_test: 0.08236308231584583\n",
      "3433 - Loss_train: 0.08314110741636574, Loss_test: 0.08236323647137839\n",
      "3434 - Loss_train: 0.08314104281897815, Loss_test: 0.08236339059925932\n",
      "3435 - Loss_train: 0.08314097825526062, Loss_test: 0.08236354469685433\n",
      "3436 - Loss_train: 0.08314091372513613, Loss_test: 0.08236369876694276\n",
      "3437 - Loss_train: 0.08314084922852069, Loss_test: 0.08236385283038239\n",
      "3438 - Loss_train: 0.08314078476565265, Loss_test: 0.0823640068596137\n",
      "3439 - Loss_train: 0.08314072033636494, Loss_test: 0.08236416087405589\n",
      "3440 - Loss_train: 0.08314065594043464, Loss_test: 0.08236431485987955\n",
      "3441 - Loss_train: 0.08314059157802892, Loss_test: 0.08236446881924142\n",
      "3442 - Loss_train: 0.08314052724931545, Loss_test: 0.08236462277684418\n",
      "3443 - Loss_train: 0.08314046295398224, Loss_test: 0.08236477666870291\n",
      "3444 - Loss_train: 0.08314039869212549, Loss_test: 0.08236493056859645\n",
      "3445 - Loss_train: 0.08314033446373191, Loss_test: 0.08236508443691141\n",
      "3446 - Loss_train: 0.08314027026884839, Loss_test: 0.08236523830321359\n",
      "3447 - Loss_train: 0.08314020610740118, Loss_test: 0.08236539211275412\n",
      "3448 - Loss_train: 0.08314014197925619, Loss_test: 0.08236554592217532\n",
      "3449 - Loss_train: 0.0831400778846049, Loss_test: 0.08236569969299694\n",
      "3450 - Loss_train: 0.08314001382327144, Loss_test: 0.08236585346250633\n",
      "3451 - Loss_train: 0.08313994979533929, Loss_test: 0.08236600718989782\n",
      "3452 - Loss_train: 0.08313988580075583, Loss_test: 0.08236616089842073\n",
      "3453 - Loss_train: 0.08313982183960639, Loss_test: 0.08236631457939332\n",
      "3454 - Loss_train: 0.0831397579116385, Loss_test: 0.08236646823542172\n",
      "3455 - Loss_train: 0.08313969401709263, Loss_test: 0.0823666218795435\n",
      "3456 - Loss_train: 0.08313963015574691, Loss_test: 0.08236677550080537\n",
      "3457 - Loss_train: 0.08313956632762566, Loss_test: 0.08236692908792914\n",
      "3458 - Loss_train: 0.0831395025327421, Loss_test: 0.08236708265798282\n",
      "3459 - Loss_train: 0.08313943877120765, Loss_test: 0.0823672362023211\n",
      "3460 - Loss_train: 0.08313937504274135, Loss_test: 0.08236738972598678\n",
      "3461 - Loss_train: 0.08313931134752839, Loss_test: 0.08236754322633169\n",
      "3462 - Loss_train: 0.083139247685499, Loss_test: 0.08236769670921609\n",
      "3463 - Loss_train: 0.08313918405652694, Loss_test: 0.08236785016025727\n",
      "3464 - Loss_train: 0.08313912046072881, Loss_test: 0.08236800358484934\n",
      "3465 - Loss_train: 0.0831390568980503, Loss_test: 0.0823681569957113\n",
      "3466 - Loss_train: 0.08313899336848925, Loss_test: 0.08236831037168751\n",
      "3467 - Loss_train: 0.08313892987201783, Loss_test: 0.08236846372958598\n",
      "3468 - Loss_train: 0.08313886640862786, Loss_test: 0.08236861707744048\n",
      "3469 - Loss_train: 0.083138802978385, Loss_test: 0.082368770377164\n",
      "3470 - Loss_train: 0.08313873958120843, Loss_test: 0.0823689236736761\n",
      "3471 - Loss_train: 0.0831386762169361, Loss_test: 0.08236907694374071\n",
      "3472 - Loss_train: 0.08313861288563311, Loss_test: 0.08236923018436026\n",
      "3473 - Loss_train: 0.08313854958724258, Loss_test: 0.08236938341599806\n",
      "3474 - Loss_train: 0.08313848632191434, Loss_test: 0.08236953659776024\n",
      "3475 - Loss_train: 0.08313842308955018, Loss_test: 0.08236968977422997\n",
      "3476 - Loss_train: 0.08313835989001625, Loss_test: 0.08236984292619992\n",
      "3477 - Loss_train: 0.08313829672338087, Loss_test: 0.08236999605458711\n",
      "3478 - Loss_train: 0.08313823358966999, Loss_test: 0.08237014915747204\n",
      "3479 - Loss_train: 0.08313817048886865, Loss_test: 0.08237030223502291\n",
      "3480 - Loss_train: 0.08313810742092294, Loss_test: 0.08237045530087464\n",
      "3481 - Loss_train: 0.08313804438594807, Loss_test: 0.08237060832943105\n",
      "3482 - Loss_train: 0.08313798138368629, Loss_test: 0.08237076134396365\n",
      "3483 - Loss_train: 0.08313791841428185, Loss_test: 0.08237091432265062\n",
      "3484 - Loss_train: 0.08313785547777239, Loss_test: 0.08237106729165047\n",
      "3485 - Loss_train: 0.083137792573917, Loss_test: 0.08237122023710565\n",
      "3486 - Loss_train: 0.0831377297027986, Loss_test: 0.08237137315789284\n",
      "3487 - Loss_train: 0.08313766686440048, Loss_test: 0.082371526051183\n",
      "3488 - Loss_train: 0.0831376040589389, Loss_test: 0.08237167891343435\n",
      "3489 - Loss_train: 0.08313754128598859, Loss_test: 0.08237183176238054\n",
      "3490 - Loss_train: 0.08313747854579603, Loss_test: 0.08237198458817999\n",
      "3491 - Loss_train: 0.08313741583823242, Loss_test: 0.08237213739533378\n",
      "3492 - Loss_train: 0.08313735316338784, Loss_test: 0.08237229017686913\n",
      "3493 - Loss_train: 0.08313729052119573, Loss_test: 0.08237244291756625\n",
      "3494 - Loss_train: 0.08313722791156065, Loss_test: 0.0823725956465034\n",
      "3495 - Loss_train: 0.08313716533451754, Loss_test: 0.08237274836338392\n",
      "3496 - Loss_train: 0.08313710279023949, Loss_test: 0.08237290104824463\n",
      "3497 - Loss_train: 0.08313704027840713, Loss_test: 0.08237305370337061\n",
      "3498 - Loss_train: 0.08313697779919027, Loss_test: 0.08237320633919121\n",
      "3499 - Loss_train: 0.08313691535249948, Loss_test: 0.08237335896684313\n",
      "3500 - Loss_train: 0.0831368529383966, Loss_test: 0.08237351155394865\n",
      "3501 - Loss_train: 0.0831367905566242, Loss_test: 0.0823736641101244\n",
      "3502 - Loss_train: 0.08313672820743546, Loss_test: 0.08237381666379161\n",
      "3503 - Loss_train: 0.08313666589069287, Loss_test: 0.08237396916826013\n",
      "3504 - Loss_train: 0.08313660360635229, Loss_test: 0.0823741216774648\n",
      "3505 - Loss_train: 0.0831365413544104, Loss_test: 0.08237427414708554\n",
      "3506 - Loss_train: 0.08313647913503487, Loss_test: 0.08237442661291429\n",
      "3507 - Loss_train: 0.08313641694796152, Loss_test: 0.08237457902290188\n",
      "3508 - Loss_train: 0.08313635479342568, Loss_test: 0.082374731444032\n",
      "3509 - Loss_train: 0.08313629267109446, Loss_test: 0.08237488381795771\n",
      "3510 - Loss_train: 0.08313623058116643, Loss_test: 0.08237503617708385\n",
      "3511 - Loss_train: 0.08313616852354615, Loss_test: 0.08237518850848251\n",
      "3512 - Loss_train: 0.08313610649823147, Loss_test: 0.08237534081051592\n",
      "3513 - Loss_train: 0.08313604450523142, Loss_test: 0.08237549310339548\n",
      "3514 - Loss_train: 0.08313598254457881, Loss_test: 0.08237564536942306\n",
      "3515 - Loss_train: 0.0831359206161813, Loss_test: 0.08237579760950851\n",
      "3516 - Loss_train: 0.0831358587200058, Loss_test: 0.08237594982047458\n",
      "3517 - Loss_train: 0.08313579685610577, Loss_test: 0.08237610202071904\n",
      "3518 - Loss_train: 0.0831357350245521, Loss_test: 0.08237625420178502\n",
      "3519 - Loss_train: 0.08313567322502145, Loss_test: 0.08237640632988205\n",
      "3520 - Loss_train: 0.08313561145767553, Loss_test: 0.08237655846191348\n",
      "3521 - Loss_train: 0.0831355497225456, Loss_test: 0.08237671055862343\n",
      "3522 - Loss_train: 0.08313548801956051, Loss_test: 0.08237686264211254\n",
      "3523 - Loss_train: 0.08313542634880494, Loss_test: 0.08237701469896681\n",
      "3524 - Loss_train: 0.08313536471022043, Loss_test: 0.0823771667215723\n",
      "3525 - Loss_train: 0.08313530310357607, Loss_test: 0.08237731873457817\n",
      "3526 - Loss_train: 0.08313524152909105, Loss_test: 0.08237747070898527\n",
      "3527 - Loss_train: 0.08313517998659803, Loss_test: 0.08237762266789515\n",
      "3528 - Loss_train: 0.08313511847616988, Loss_test: 0.0823777746063805\n",
      "3529 - Loss_train: 0.08313505699787713, Loss_test: 0.08237792652084538\n",
      "3530 - Loss_train: 0.08313499555151625, Loss_test: 0.08237807840460067\n",
      "3531 - Loss_train: 0.08313493413712567, Loss_test: 0.08237823027889635\n",
      "3532 - Loss_train: 0.08313487275472826, Loss_test: 0.08237838211946308\n",
      "3533 - Loss_train: 0.08313481140427764, Loss_test: 0.08237853394397383\n",
      "3534 - Loss_train: 0.08313475008590035, Loss_test: 0.08237868574085966\n",
      "3535 - Loss_train: 0.08313468879935144, Loss_test: 0.08237883751575045\n",
      "3536 - Loss_train: 0.08313462754475269, Loss_test: 0.08237898924388526\n",
      "3537 - Loss_train: 0.08313456632220684, Loss_test: 0.08237914098266655\n",
      "3538 - Loss_train: 0.0831345051313487, Loss_test: 0.08237929266975139\n",
      "3539 - Loss_train: 0.08313444397241535, Loss_test: 0.08237944437274752\n",
      "3540 - Loss_train: 0.08313438284534264, Loss_test: 0.0823795960149184\n",
      "3541 - Loss_train: 0.08313432175002751, Loss_test: 0.08237974765019096\n",
      "3542 - Loss_train: 0.08313426068654373, Loss_test: 0.08237989925918782\n",
      "3543 - Loss_train: 0.08313419965487139, Loss_test: 0.0823800508427521\n",
      "3544 - Loss_train: 0.08313413865500724, Loss_test: 0.08238020240399255\n",
      "3545 - Loss_train: 0.08313407768694223, Loss_test: 0.0823803539405705\n",
      "3546 - Loss_train: 0.08313401675060639, Loss_test: 0.08238050545888059\n",
      "3547 - Loss_train: 0.08313395584602126, Loss_test: 0.08238065694920758\n",
      "3548 - Loss_train: 0.08313389497326595, Loss_test: 0.08238080841622562\n",
      "3549 - Loss_train: 0.08313383413218801, Loss_test: 0.08238095985820947\n",
      "3550 - Loss_train: 0.08313377332271969, Loss_test: 0.08238111128844132\n",
      "3551 - Loss_train: 0.08313371254501042, Loss_test: 0.08238126267201568\n",
      "3552 - Loss_train: 0.08313365179890854, Loss_test: 0.08238141406165136\n",
      "3553 - Loss_train: 0.08313359108437078, Loss_test: 0.0823815653990844\n",
      "3554 - Loss_train: 0.08313353040142629, Loss_test: 0.08238171673101057\n",
      "3555 - Loss_train: 0.08313346975007749, Loss_test: 0.08238186803171453\n",
      "3556 - Loss_train: 0.083133409130352, Loss_test: 0.08238201931634043\n",
      "3557 - Loss_train: 0.08313334854217119, Loss_test: 0.08238217056766298\n",
      "3558 - Loss_train: 0.08313328798575664, Loss_test: 0.08238232180723314\n",
      "3559 - Loss_train: 0.08313322746083898, Loss_test: 0.08238247301522841\n",
      "3560 - Loss_train: 0.0831331669674022, Loss_test: 0.08238262419165027\n",
      "3561 - Loss_train: 0.0831331065053875, Loss_test: 0.08238277536275097\n",
      "3562 - Loss_train: 0.08313304607482627, Loss_test: 0.08238292650079579\n",
      "3563 - Loss_train: 0.08313298567571135, Loss_test: 0.0823830776203739\n",
      "3564 - Loss_train: 0.08313292530816803, Loss_test: 0.08238322870342203\n",
      "3565 - Loss_train: 0.08313286497206429, Loss_test: 0.08238337978131238\n",
      "3566 - Loss_train: 0.0831328046674866, Loss_test: 0.0823835308118736\n",
      "3567 - Loss_train: 0.0831327443941695, Loss_test: 0.08238368185128403\n",
      "3568 - Loss_train: 0.0831326841523281, Loss_test: 0.08238383284327021\n",
      "3569 - Loss_train: 0.08313262394187845, Loss_test: 0.08238398380229789\n",
      "3570 - Loss_train: 0.08313256376266057, Loss_test: 0.082384134767113\n",
      "3571 - Loss_train: 0.08313250361480355, Loss_test: 0.08238428570477174\n",
      "3572 - Loss_train: 0.0831324434982708, Loss_test: 0.08238443660060858\n",
      "3573 - Loss_train: 0.08313238341318337, Loss_test: 0.08238458747875024\n",
      "3574 - Loss_train: 0.08313232335925316, Loss_test: 0.08238473833548782\n",
      "3575 - Loss_train: 0.08313226333654307, Loss_test: 0.08238488917037462\n",
      "3576 - Loss_train: 0.08313220334515312, Loss_test: 0.08238503998346164\n",
      "3577 - Loss_train: 0.08313214338503302, Loss_test: 0.08238519076271572\n",
      "3578 - Loss_train: 0.08313208345607045, Loss_test: 0.08238534153303839\n",
      "3579 - Loss_train: 0.08313202355837472, Loss_test: 0.08238549226929942\n",
      "3580 - Loss_train: 0.08313196369199743, Loss_test: 0.0823856429944037\n",
      "3581 - Loss_train: 0.08313190385679016, Loss_test: 0.08238579367462076\n",
      "3582 - Loss_train: 0.08313184405278294, Loss_test: 0.08238594435673403\n",
      "3583 - Loss_train: 0.08313178427979462, Loss_test: 0.08238609498944055\n",
      "3584 - Loss_train: 0.0831317245378954, Loss_test: 0.08238624562031203\n",
      "3585 - Loss_train: 0.083131664827125, Loss_test: 0.08238639621494517\n",
      "3586 - Loss_train: 0.0831316051473696, Loss_test: 0.08238654679490283\n",
      "3587 - Loss_train: 0.08313154549877469, Loss_test: 0.08238669734798519\n",
      "3588 - Loss_train: 0.08313148588126912, Loss_test: 0.08238684787306616\n",
      "3589 - Loss_train: 0.08313142629479144, Loss_test: 0.08238699837145688\n",
      "3590 - Loss_train: 0.08313136673938325, Loss_test: 0.08238714885507237\n",
      "3591 - Loss_train: 0.0831313072149501, Loss_test: 0.08238729931549588\n",
      "3592 - Loss_train: 0.08313124772151348, Loss_test: 0.08238744973682186\n",
      "3593 - Loss_train: 0.08313118825907144, Loss_test: 0.08238760015689786\n",
      "3594 - Loss_train: 0.08313112882759044, Loss_test: 0.08238775053295949\n",
      "3595 - Loss_train: 0.08313106942707991, Loss_test: 0.08238790090561349\n",
      "3596 - Loss_train: 0.08313101005751652, Loss_test: 0.08238805124147071\n",
      "3597 - Loss_train: 0.08313095071897716, Loss_test: 0.08238820156513178\n",
      "3598 - Loss_train: 0.08313089141121355, Loss_test: 0.08238835184191906\n",
      "3599 - Loss_train: 0.08313083213432736, Loss_test: 0.08238850212724678\n",
      "3600 - Loss_train: 0.08313077288834106, Loss_test: 0.08238865235004578\n",
      "3601 - Loss_train: 0.08313071367319841, Loss_test: 0.08238880259846551\n",
      "3602 - Loss_train: 0.08313065448890528, Loss_test: 0.08238895278445352\n",
      "3603 - Loss_train: 0.08313059533543926, Loss_test: 0.08238910295576973\n",
      "3604 - Loss_train: 0.08313053621279833, Loss_test: 0.08238925311772854\n",
      "3605 - Loss_train: 0.08313047712095656, Loss_test: 0.08238940322929703\n",
      "3606 - Loss_train: 0.08313041805997276, Loss_test: 0.08238955334797293\n",
      "3607 - Loss_train: 0.0831303590296363, Loss_test: 0.08238970342221616\n",
      "3608 - Loss_train: 0.08313030003007026, Loss_test: 0.08238985347809669\n",
      "3609 - Loss_train: 0.08313024106121779, Loss_test: 0.08239000351053254\n",
      "3610 - Loss_train: 0.08313018212321882, Loss_test: 0.08239015352012408\n",
      "3611 - Loss_train: 0.0831301232157998, Loss_test: 0.08239030350727758\n",
      "3612 - Loss_train: 0.08313006433921448, Loss_test: 0.08239045346767425\n",
      "3613 - Loss_train: 0.08313000549313986, Loss_test: 0.08239060340537596\n",
      "3614 - Loss_train: 0.0831299466777464, Loss_test: 0.08239075331838587\n",
      "3615 - Loss_train: 0.08312988789289984, Loss_test: 0.08239090320166598\n",
      "3616 - Loss_train: 0.08312982913869775, Loss_test: 0.08239105307494317\n",
      "3617 - Loss_train: 0.08312977041509569, Loss_test: 0.0823912029262156\n",
      "3618 - Loss_train: 0.08312971172206271, Loss_test: 0.08239135274662247\n",
      "3619 - Loss_train: 0.08312965305958087, Loss_test: 0.08239150253758183\n",
      "3620 - Loss_train: 0.08312959442765018, Loss_test: 0.08239165231833206\n",
      "3621 - Loss_train: 0.08312953582630415, Loss_test: 0.08239180206823299\n",
      "3622 - Loss_train: 0.0831294772554652, Loss_test: 0.08239195179312828\n",
      "3623 - Loss_train: 0.08312941871518585, Loss_test: 0.08239210149786518\n",
      "3624 - Loss_train: 0.08312936020529109, Loss_test: 0.0823922511745649\n",
      "3625 - Loss_train: 0.08312930172593017, Loss_test: 0.08239240082361389\n",
      "3626 - Loss_train: 0.08312924327701189, Loss_test: 0.08239255046053401\n",
      "3627 - Loss_train: 0.08312918485855171, Loss_test: 0.08239270006304514\n",
      "3628 - Loss_train: 0.0831291264704814, Loss_test: 0.08239284965027478\n",
      "3629 - Loss_train: 0.08312906811293955, Loss_test: 0.08239299920900224\n",
      "3630 - Loss_train: 0.08312900978570588, Loss_test: 0.08239314875649821\n",
      "3631 - Loss_train: 0.08312895148885754, Loss_test: 0.08239329826066942\n",
      "3632 - Loss_train: 0.08312889322236074, Loss_test: 0.08239344775897413\n",
      "3633 - Loss_train: 0.08312883498618126, Loss_test: 0.08239359722427993\n",
      "3634 - Loss_train: 0.0831287767804311, Loss_test: 0.08239374665257586\n",
      "3635 - Loss_train: 0.08312871860505235, Loss_test: 0.08239389609228003\n",
      "3636 - Loss_train: 0.0831286604598707, Loss_test: 0.08239404547483734\n",
      "3637 - Loss_train: 0.0831286023449653, Loss_test: 0.08239419486076204\n",
      "3638 - Loss_train: 0.08312854426042242, Loss_test: 0.08239434419370582\n",
      "3639 - Loss_train: 0.08312848620618074, Loss_test: 0.08239449353168754\n",
      "3640 - Loss_train: 0.08312842818200383, Loss_test: 0.08239464282042197\n",
      "3641 - Loss_train: 0.08312837018798555, Loss_test: 0.08239479210664108\n",
      "3642 - Loss_train: 0.0831283122243493, Loss_test: 0.08239494136782467\n",
      "3643 - Loss_train: 0.08312825429082311, Loss_test: 0.08239509059506757\n",
      "3644 - Loss_train: 0.08312819638758194, Loss_test: 0.0823952397916254\n",
      "3645 - Loss_train: 0.08312813851429456, Loss_test: 0.0823953889656539\n",
      "3646 - Loss_train: 0.08312808067121603, Loss_test: 0.08239553813435385\n",
      "3647 - Loss_train: 0.08312802285835494, Loss_test: 0.08239568726955891\n",
      "3648 - Loss_train: 0.08312796507549172, Loss_test: 0.08239583637936335\n",
      "3649 - Loss_train: 0.08312790732282885, Loss_test: 0.08239598546587461\n",
      "3650 - Loss_train: 0.08312784960016587, Loss_test: 0.08239613453179405\n",
      "3651 - Loss_train: 0.08312779190752062, Loss_test: 0.08239628358896885\n",
      "3652 - Loss_train: 0.08312773424507516, Loss_test: 0.08239643259401644\n",
      "3653 - Loss_train: 0.0831276766125701, Loss_test: 0.0823965815850871\n",
      "3654 - Loss_train: 0.08312761901008504, Loss_test: 0.08239673055495769\n",
      "3655 - Loss_train: 0.0831275614376067, Loss_test: 0.08239687950261176\n",
      "3656 - Loss_train: 0.08312750389519014, Loss_test: 0.08239702842080013\n",
      "3657 - Loss_train: 0.08312744638263586, Loss_test: 0.08239717732096731\n",
      "3658 - Loss_train: 0.0831273889001316, Loss_test: 0.08239732620070211\n",
      "3659 - Loss_train: 0.08312733144747375, Loss_test: 0.08239747505035004\n",
      "3660 - Loss_train: 0.08312727402473281, Loss_test: 0.0823976238867822\n",
      "3661 - Loss_train: 0.08312721663201271, Loss_test: 0.08239777267410239\n",
      "3662 - Loss_train: 0.08312715926907564, Loss_test: 0.08239792146579408\n",
      "3663 - Loss_train: 0.08312710193616622, Loss_test: 0.082398070207237\n",
      "3664 - Loss_train: 0.0831270446330909, Loss_test: 0.08239821896118571\n",
      "3665 - Loss_train: 0.08312698735974894, Loss_test: 0.0823983676492936\n",
      "3666 - Loss_train: 0.08312693011634606, Loss_test: 0.08239851633714687\n",
      "3667 - Loss_train: 0.08312687290263818, Loss_test: 0.08239866501334667\n",
      "3668 - Loss_train: 0.08312681571869265, Loss_test: 0.08239881364812168\n",
      "3669 - Loss_train: 0.08312675856476873, Loss_test: 0.08239896225920855\n",
      "3670 - Loss_train: 0.0831267014404244, Loss_test: 0.08239911086036565\n",
      "3671 - Loss_train: 0.08312664434598882, Loss_test: 0.08239925941496595\n",
      "3672 - Loss_train: 0.08312658728112403, Loss_test: 0.08239940796795948\n",
      "3673 - Loss_train: 0.08312653024600004, Loss_test: 0.08239955647469925\n",
      "3674 - Loss_train: 0.08312647324064708, Loss_test: 0.08239970497576021\n",
      "3675 - Loss_train: 0.08312641626502568, Loss_test: 0.08239985345579835\n",
      "3676 - Loss_train: 0.08312635931906708, Loss_test: 0.08240000189950285\n",
      "3677 - Loss_train: 0.08312630240265902, Loss_test: 0.08240015032538146\n",
      "3678 - Loss_train: 0.08312624551602364, Loss_test: 0.08240029873015088\n",
      "3679 - Loss_train: 0.08312618865898196, Loss_test: 0.08240044709437765\n",
      "3680 - Loss_train: 0.08312613183148965, Loss_test: 0.08240059545287323\n",
      "3681 - Loss_train: 0.08312607503357473, Loss_test: 0.08240074379678902\n",
      "3682 - Loss_train: 0.0831260182652121, Loss_test: 0.08240089208373855\n",
      "3683 - Loss_train: 0.08312596152639651, Loss_test: 0.08240104038080011\n",
      "3684 - Loss_train: 0.08312590481719072, Loss_test: 0.08240118862588963\n",
      "3685 - Loss_train: 0.08312584813752498, Loss_test: 0.08240133686915631\n",
      "3686 - Loss_train: 0.08312579148733287, Loss_test: 0.08240148508151102\n",
      "3687 - Loss_train: 0.08312573486668383, Loss_test: 0.0824016332791332\n",
      "3688 - Loss_train: 0.0831256782754912, Loss_test: 0.08240178143687253\n",
      "3689 - Loss_train: 0.08312562171387058, Loss_test: 0.08240192957501663\n",
      "3690 - Loss_train: 0.08312556518162288, Loss_test: 0.08240207769715623\n",
      "3691 - Loss_train: 0.08312550867896086, Loss_test: 0.08240222578184078\n",
      "3692 - Loss_train: 0.08312545220559087, Loss_test: 0.0824023738583846\n",
      "3693 - Loss_train: 0.08312539576160943, Loss_test: 0.08240252189467574\n",
      "3694 - Loss_train: 0.08312533934720445, Loss_test: 0.08240266993234088\n",
      "3695 - Loss_train: 0.08312528296200734, Loss_test: 0.08240281791871913\n",
      "3696 - Loss_train: 0.08312522660619293, Loss_test: 0.08240296588728034\n",
      "3697 - Loss_train: 0.08312517027978163, Loss_test: 0.08240311384908015\n",
      "3698 - Loss_train: 0.08312511398263114, Loss_test: 0.08240326176340043\n",
      "3699 - Loss_train: 0.0831250577148635, Loss_test: 0.0824034096876622\n",
      "3700 - Loss_train: 0.08312500147638903, Loss_test: 0.08240355755762094\n",
      "3701 - Loss_train: 0.08312494526716725, Loss_test: 0.08240370541115107\n",
      "3702 - Loss_train: 0.08312488908716646, Loss_test: 0.08240385324322802\n",
      "3703 - Loss_train: 0.08312483293651368, Loss_test: 0.08240400105555006\n",
      "3704 - Loss_train: 0.08312477681527532, Loss_test: 0.08240414884484946\n",
      "3705 - Loss_train: 0.08312472072303848, Loss_test: 0.08240429658929602\n",
      "3706 - Loss_train: 0.08312466466011012, Loss_test: 0.08240444433091655\n",
      "3707 - Loss_train: 0.08312460862636761, Loss_test: 0.08240459205355517\n",
      "3708 - Loss_train: 0.08312455262174884, Loss_test: 0.08240473973763944\n",
      "3709 - Loss_train: 0.08312449664647714, Loss_test: 0.08240488740391666\n",
      "3710 - Loss_train: 0.08312444070019592, Loss_test: 0.08240503504489245\n",
      "3711 - Loss_train: 0.08312438478320669, Loss_test: 0.08240518266955497\n",
      "3712 - Loss_train: 0.08312432889524474, Loss_test: 0.08240533025091416\n",
      "3713 - Loss_train: 0.0831242730363392, Loss_test: 0.08240547782936516\n",
      "3714 - Loss_train: 0.08312421720661112, Loss_test: 0.08240562538898949\n",
      "3715 - Loss_train: 0.08312416140589166, Loss_test: 0.08240577289702423\n",
      "3716 - Loss_train: 0.0831241056343516, Loss_test: 0.08240592040139777\n",
      "3717 - Loss_train: 0.08312404989173472, Loss_test: 0.08240606788087129\n",
      "3718 - Loss_train: 0.08312399417817654, Loss_test: 0.08240621532759464\n",
      "3719 - Loss_train: 0.08312393849372161, Loss_test: 0.08240636275026386\n",
      "3720 - Loss_train: 0.083123882838135, Loss_test: 0.08240651015258779\n",
      "3721 - Loss_train: 0.08312382721164385, Loss_test: 0.08240665753555534\n",
      "3722 - Loss_train: 0.08312377161407151, Loss_test: 0.08240680490038263\n",
      "3723 - Loss_train: 0.08312371604555709, Loss_test: 0.08240695222195793\n",
      "3724 - Loss_train: 0.08312366050584945, Loss_test: 0.08240709953438255\n",
      "3725 - Loss_train: 0.08312360499521694, Loss_test: 0.08240724682560722\n",
      "3726 - Loss_train: 0.08312354951330804, Loss_test: 0.08240739408243872\n",
      "3727 - Loss_train: 0.08312349406035087, Loss_test: 0.08240754132260371\n",
      "3728 - Loss_train: 0.08312343863623918, Loss_test: 0.08240768853093768\n",
      "3729 - Loss_train: 0.08312338324104063, Loss_test: 0.0824078357259354\n",
      "3730 - Loss_train: 0.08312332787468758, Loss_test: 0.08240798289103578\n",
      "3731 - Loss_train: 0.08312327253717265, Loss_test: 0.08240813003866077\n",
      "3732 - Loss_train: 0.08312321722844694, Loss_test: 0.08240827715146469\n",
      "3733 - Loss_train: 0.08312316194855689, Loss_test: 0.08240842425584133\n",
      "3734 - Loss_train: 0.08312310669757428, Loss_test: 0.0824085713169997\n",
      "3735 - Loss_train: 0.08312305147536805, Loss_test: 0.08240871837014364\n",
      "3736 - Loss_train: 0.08312299628184584, Loss_test: 0.08240886539242595\n",
      "3737 - Loss_train: 0.08312294111700959, Loss_test: 0.08240901238605616\n",
      "3738 - Loss_train: 0.08312288598092211, Loss_test: 0.08240915937243344\n",
      "3739 - Loss_train: 0.0831228308735384, Loss_test: 0.08240930632102554\n",
      "3740 - Loss_train: 0.08312277579487883, Loss_test: 0.0824094532545613\n",
      "3741 - Loss_train: 0.08312272074489829, Loss_test: 0.08240960015547064\n",
      "3742 - Loss_train: 0.08312266572362091, Loss_test: 0.08240974702786694\n",
      "3743 - Loss_train: 0.08312261073098078, Loss_test: 0.08240989389843342\n",
      "3744 - Loss_train: 0.083122555766943, Loss_test: 0.0824100407192649\n",
      "3745 - Loss_train: 0.08312250083154751, Loss_test: 0.08241018753506017\n",
      "3746 - Loss_train: 0.08312244592476695, Loss_test: 0.08241033431343238\n",
      "3747 - Loss_train: 0.08312239104673878, Loss_test: 0.08241048109121746\n",
      "3748 - Loss_train: 0.08312233619729222, Loss_test: 0.08241062781878078\n",
      "3749 - Loss_train: 0.08312228137639788, Loss_test: 0.08241077454453917\n",
      "3750 - Loss_train: 0.08312222658406934, Loss_test: 0.08241092123285569\n",
      "3751 - Loss_train: 0.08312217182017016, Loss_test: 0.08241106788982394\n",
      "3752 - Loss_train: 0.08312211708477177, Loss_test: 0.08241121454475422\n",
      "3753 - Loss_train: 0.08312206237787388, Loss_test: 0.08241136114822557\n",
      "3754 - Loss_train: 0.08312200769945693, Loss_test: 0.08241150776698665\n",
      "3755 - Loss_train: 0.08312195304951295, Loss_test: 0.08241165432073887\n",
      "3756 - Loss_train: 0.08312189842817064, Loss_test: 0.08241180087383575\n",
      "3757 - Loss_train: 0.08312184383516817, Loss_test: 0.08241194739563493\n",
      "3758 - Loss_train: 0.0831217892705947, Loss_test: 0.08241209390180096\n",
      "3759 - Loss_train: 0.08312173473445153, Loss_test: 0.08241224038199384\n",
      "3760 - Loss_train: 0.08312168022671322, Loss_test: 0.0824123868289348\n",
      "3761 - Loss_train: 0.0831216257473393, Loss_test: 0.08241253325511014\n",
      "3762 - Loss_train: 0.0831215712963793, Loss_test: 0.08241267965967054\n",
      "3763 - Loss_train: 0.0831215168737376, Loss_test: 0.08241282604021163\n",
      "3764 - Loss_train: 0.08312146247947792, Loss_test: 0.08241297239226617\n",
      "3765 - Loss_train: 0.08312140811350908, Loss_test: 0.08241311872448044\n",
      "3766 - Loss_train: 0.08312135377590611, Loss_test: 0.08241326505230068\n",
      "3767 - Loss_train: 0.08312129946665993, Loss_test: 0.08241341131896965\n",
      "3768 - Loss_train: 0.08312124518563577, Loss_test: 0.08241355759333806\n",
      "3769 - Loss_train: 0.08312119093288362, Loss_test: 0.08241370383942831\n",
      "3770 - Loss_train: 0.08312113670847346, Loss_test: 0.08241385003270787\n",
      "3771 - Loss_train: 0.0831210825123268, Loss_test: 0.0824139962246014\n",
      "3772 - Loss_train: 0.08312102834425529, Loss_test: 0.08241414239619688\n",
      "3773 - Loss_train: 0.08312097420440587, Loss_test: 0.08241428853255879\n",
      "3774 - Loss_train: 0.08312092009276202, Loss_test: 0.08241443466387785\n",
      "3775 - Loss_train: 0.08312086600931255, Loss_test: 0.08241458075082213\n",
      "3776 - Loss_train: 0.08312081195414071, Loss_test: 0.08241472682323811\n",
      "3777 - Loss_train: 0.08312075792710377, Loss_test: 0.08241487287994426\n",
      "3778 - Loss_train: 0.08312070392815646, Loss_test: 0.08241501889358144\n",
      "3779 - Loss_train: 0.08312064995733029, Loss_test: 0.0824151648965488\n",
      "3780 - Loss_train: 0.08312059601461898, Loss_test: 0.08241531087639103\n",
      "3781 - Loss_train: 0.0831205420999215, Loss_test: 0.08241545683042682\n",
      "3782 - Loss_train: 0.08312048821342195, Loss_test: 0.0824156027502039\n",
      "3783 - Loss_train: 0.08312043435510245, Loss_test: 0.08241574865858689\n",
      "3784 - Loss_train: 0.08312038052478822, Loss_test: 0.08241589454833541\n",
      "3785 - Loss_train: 0.08312032672252137, Loss_test: 0.08241604037816044\n",
      "3786 - Loss_train: 0.08312027294818072, Loss_test: 0.08241618622627524\n",
      "3787 - Loss_train: 0.08312021920184255, Loss_test: 0.082416332034569\n",
      "3788 - Loss_train: 0.08312016548341612, Loss_test: 0.08241647781810384\n",
      "3789 - Loss_train: 0.08312011179316794, Loss_test: 0.08241662358167799\n",
      "3790 - Loss_train: 0.0831200581307319, Loss_test: 0.08241676932107843\n",
      "3791 - Loss_train: 0.08312000449624184, Loss_test: 0.08241691502893143\n",
      "3792 - Loss_train: 0.08311995088968274, Loss_test: 0.08241706071572645\n",
      "3793 - Loss_train: 0.08311989731104508, Loss_test: 0.08241720638930877\n",
      "3794 - Loss_train: 0.08311984376031203, Loss_test: 0.08241735202493226\n",
      "3795 - Loss_train: 0.08311979023759156, Loss_test: 0.08241749765470324\n",
      "3796 - Loss_train: 0.08311973674262622, Loss_test: 0.08241764323799744\n",
      "3797 - Loss_train: 0.08311968327550498, Loss_test: 0.08241778880768667\n",
      "3798 - Loss_train: 0.08311962983627144, Loss_test: 0.08241793436291395\n",
      "3799 - Loss_train: 0.0831195764249466, Loss_test: 0.08241807988802138\n",
      "3800 - Loss_train: 0.0831195230413351, Loss_test: 0.0824182253780418\n",
      "3801 - Loss_train: 0.08311946968553721, Loss_test: 0.08241837085481972\n",
      "3802 - Loss_train: 0.08311941635752305, Loss_test: 0.08241851630755974\n",
      "3803 - Loss_train: 0.08311936305727873, Loss_test: 0.08241866172503569\n",
      "3804 - Loss_train: 0.08311930978490775, Loss_test: 0.08241880714082515\n",
      "3805 - Loss_train: 0.08311925654021457, Loss_test: 0.08241895251239004\n",
      "3806 - Loss_train: 0.08311920332321333, Loss_test: 0.0824190978729538\n",
      "3807 - Loss_train: 0.08311915013394809, Loss_test: 0.08241924320230072\n",
      "3808 - Loss_train: 0.08311909697242467, Loss_test: 0.08241938850868656\n",
      "3809 - Loss_train: 0.08311904383855694, Loss_test: 0.08241953377851116\n",
      "3810 - Loss_train: 0.08311899073238452, Loss_test: 0.08241967905429799\n",
      "3811 - Loss_train: 0.08311893765397346, Loss_test: 0.08241982428367851\n",
      "3812 - Loss_train: 0.08311888460319194, Loss_test: 0.08241996950183218\n",
      "3813 - Loss_train: 0.08311883157994951, Loss_test: 0.08242011468670571\n",
      "3814 - Loss_train: 0.08311877858432155, Loss_test: 0.08242025985652783\n",
      "3815 - Loss_train: 0.08311872561641004, Loss_test: 0.08242040498455218\n",
      "3816 - Loss_train: 0.08311867267593519, Loss_test: 0.08242055011343155\n",
      "3817 - Loss_train: 0.08311861976307419, Loss_test: 0.08242069519661989\n",
      "3818 - Loss_train: 0.08311856687789161, Loss_test: 0.08242084027044588\n",
      "3819 - Loss_train: 0.0831185140200986, Loss_test: 0.0824209853095554\n",
      "3820 - Loss_train: 0.08311846118996616, Loss_test: 0.08242113034036609\n",
      "3821 - Loss_train: 0.08311840838720336, Loss_test: 0.08242127532860254\n",
      "3822 - Loss_train: 0.08311835561211962, Loss_test: 0.08242142030956293\n",
      "3823 - Loss_train: 0.08311830286439399, Loss_test: 0.08242156525391806\n",
      "3824 - Loss_train: 0.08311825014414119, Loss_test: 0.0824217101854658\n",
      "3825 - Loss_train: 0.08311819745137707, Loss_test: 0.08242185508134135\n",
      "3826 - Loss_train: 0.08311814478600409, Loss_test: 0.0824219999616997\n",
      "3827 - Loss_train: 0.08311809214822657, Loss_test: 0.08242214481088612\n",
      "3828 - Loss_train: 0.08311803953781698, Loss_test: 0.08242228964896464\n",
      "3829 - Loss_train: 0.08311798695485909, Loss_test: 0.08242243444828397\n",
      "3830 - Loss_train: 0.08311793439914981, Loss_test: 0.08242257921717182\n",
      "3831 - Loss_train: 0.08311788187079609, Loss_test: 0.08242272399925867\n",
      "3832 - Loss_train: 0.08311782936994448, Loss_test: 0.08242286872532628\n",
      "3833 - Loss_train: 0.08311777689627668, Loss_test: 0.08242301342743043\n",
      "3834 - Loss_train: 0.08311772445008614, Loss_test: 0.08242315811876873\n",
      "3835 - Loss_train: 0.08311767203118964, Loss_test: 0.08242330277979176\n",
      "3836 - Loss_train: 0.08311761963948917, Loss_test: 0.08242344742058837\n",
      "3837 - Loss_train: 0.08311756727508393, Loss_test: 0.08242359204331386\n",
      "3838 - Loss_train: 0.08311751493795215, Loss_test: 0.08242373661440974\n",
      "3839 - Loss_train: 0.08311746262812218, Loss_test: 0.0824238811980674\n",
      "3840 - Loss_train: 0.08311741034561042, Loss_test: 0.08242402573725706\n",
      "3841 - Loss_train: 0.08311735809025274, Loss_test: 0.08242417025691573\n",
      "3842 - Loss_train: 0.08311730586207695, Loss_test: 0.08242431473699352\n",
      "3843 - Loss_train: 0.08311725366112986, Loss_test: 0.08242445922523065\n",
      "3844 - Loss_train: 0.08311720148733125, Loss_test: 0.08242460365850315\n",
      "3845 - Loss_train: 0.08311714934074037, Loss_test: 0.0824247481063316\n",
      "3846 - Loss_train: 0.08311709722139585, Loss_test: 0.08242489248617288\n",
      "3847 - Loss_train: 0.0831170451290863, Loss_test: 0.082425036873345\n",
      "3848 - Loss_train: 0.08311699306391636, Loss_test: 0.08242518121870715\n",
      "3849 - Loss_train: 0.08311694102582491, Loss_test: 0.08242532554009398\n",
      "3850 - Loss_train: 0.08311688901500477, Loss_test: 0.08242546985332284\n",
      "3851 - Loss_train: 0.0831168370312329, Loss_test: 0.08242561411969782\n",
      "3852 - Loss_train: 0.08311678507447794, Loss_test: 0.08242575839158019\n",
      "3853 - Loss_train: 0.08311673314490396, Loss_test: 0.0824259026230638\n",
      "3854 - Loss_train: 0.0831166812422626, Loss_test: 0.08242604681929104\n",
      "3855 - Loss_train: 0.08311662936665622, Loss_test: 0.08242619101545452\n",
      "3856 - Loss_train: 0.08311657751798789, Loss_test: 0.08242633516194445\n",
      "3857 - Loss_train: 0.08311652569638159, Loss_test: 0.08242647930460262\n",
      "3858 - Loss_train: 0.08311647390181454, Loss_test: 0.08242662341780374\n",
      "3859 - Loss_train: 0.08311642213416574, Loss_test: 0.08242676750311065\n",
      "3860 - Loss_train: 0.08311637039348568, Loss_test: 0.08242691156696987\n",
      "3861 - Loss_train: 0.08311631867979151, Loss_test: 0.08242705561912558\n",
      "3862 - Loss_train: 0.08311626699311703, Loss_test: 0.08242719962074864\n",
      "3863 - Loss_train: 0.08311621533335181, Loss_test: 0.08242734362066315\n",
      "3864 - Loss_train: 0.08311616370051575, Loss_test: 0.08242748758196601\n",
      "3865 - Loss_train: 0.08311611209457827, Loss_test: 0.08242763154418804\n",
      "3866 - Loss_train: 0.08311606051539118, Loss_test: 0.08242777545305989\n",
      "3867 - Loss_train: 0.08311600896304298, Loss_test: 0.08242791935755055\n",
      "3868 - Loss_train: 0.08311595743755344, Loss_test: 0.08242806322728988\n",
      "3869 - Loss_train: 0.08311590593887695, Loss_test: 0.08242820706613248\n",
      "3870 - Loss_train: 0.08311585446708121, Loss_test: 0.08242835091036385\n",
      "3871 - Loss_train: 0.08311580302211051, Loss_test: 0.08242849469128893\n",
      "3872 - Loss_train: 0.08311575160389609, Loss_test: 0.08242863848927608\n",
      "3873 - Loss_train: 0.08311570021246877, Loss_test: 0.08242878222425334\n",
      "3874 - Loss_train: 0.08311564884792341, Loss_test: 0.08242892596457281\n",
      "3875 - Loss_train: 0.08311559751002924, Loss_test: 0.08242906966479925\n",
      "3876 - Loss_train: 0.08311554619900259, Loss_test: 0.08242921334716531\n",
      "3877 - Loss_train: 0.08311549491460528, Loss_test: 0.08242935700874765\n",
      "3878 - Loss_train: 0.08311544365693745, Loss_test: 0.08242950064954721\n",
      "3879 - Loss_train: 0.08311539242591248, Loss_test: 0.08242964424589205\n",
      "3880 - Loss_train: 0.08311534122169191, Loss_test: 0.08242978783529792\n",
      "3881 - Loss_train: 0.08311529004403925, Loss_test: 0.08242993138511673\n",
      "3882 - Loss_train: 0.08311523889302803, Loss_test: 0.08243007493892268\n",
      "3883 - Loss_train: 0.08311518776866446, Loss_test: 0.08243021845065082\n",
      "3884 - Loss_train: 0.0831151366709586, Loss_test: 0.08243036193308174\n",
      "3885 - Loss_train: 0.08311508559992488, Loss_test: 0.08243050540818678\n",
      "3886 - Loss_train: 0.0831150345554101, Loss_test: 0.08243064883693227\n",
      "3887 - Loss_train: 0.08311498353759751, Loss_test: 0.08243079226619987\n",
      "3888 - Loss_train: 0.08311493254624777, Loss_test: 0.08243093566020425\n",
      "3889 - Loss_train: 0.08311488158143106, Loss_test: 0.08243107903271576\n",
      "3890 - Loss_train: 0.08311483064333054, Loss_test: 0.08243122237769943\n",
      "3891 - Loss_train: 0.08311477973161159, Loss_test: 0.08243136570193182\n",
      "3892 - Loss_train: 0.08311472884637486, Loss_test: 0.082431508994187\n",
      "3893 - Loss_train: 0.08311467798778126, Loss_test: 0.08243165226478175\n",
      "3894 - Loss_train: 0.08311462715557216, Loss_test: 0.08243179552227878\n",
      "3895 - Loss_train: 0.08311457634993742, Loss_test: 0.0824319387549846\n",
      "3896 - Loss_train: 0.0831145255707224, Loss_test: 0.08243208193994021\n",
      "3897 - Loss_train: 0.08311447481803412, Loss_test: 0.08243222513956225\n",
      "3898 - Loss_train: 0.08311442409169097, Loss_test: 0.0824323682771996\n",
      "3899 - Loss_train: 0.08311437339174288, Loss_test: 0.08243251141905397\n",
      "3900 - Loss_train: 0.08311432271833942, Loss_test: 0.08243265451896925\n",
      "3901 - Loss_train: 0.08311427207117597, Loss_test: 0.08243279760016416\n",
      "3902 - Loss_train: 0.08311422145039439, Loss_test: 0.08243294066027537\n",
      "3903 - Loss_train: 0.08311417085594817, Loss_test: 0.08243308369041806\n",
      "3904 - Loss_train: 0.08311412028801654, Loss_test: 0.082433226707067\n",
      "3905 - Loss_train: 0.08311406974628642, Loss_test: 0.08243336969511171\n",
      "3906 - Loss_train: 0.08311401923089956, Loss_test: 0.08243351265251603\n",
      "3907 - Loss_train: 0.08311396874184201, Loss_test: 0.08243365560350581\n",
      "3908 - Loss_train: 0.0831139182791555, Loss_test: 0.08243379851627042\n",
      "3909 - Loss_train: 0.08311386784266872, Loss_test: 0.08243394140306344\n",
      "3910 - Loss_train: 0.08311381743247358, Loss_test: 0.08243408426819661\n",
      "3911 - Loss_train: 0.08311376704845108, Loss_test: 0.0824342270962296\n",
      "3912 - Loss_train: 0.08311371669073521, Loss_test: 0.08243436994283142\n",
      "3913 - Loss_train: 0.08311366635921497, Loss_test: 0.08243451272736672\n",
      "3914 - Loss_train: 0.08311361605395332, Loss_test: 0.08243465549362368\n",
      "3915 - Loss_train: 0.08311356577485808, Loss_test: 0.08243479824772151\n",
      "3916 - Loss_train: 0.08311351552198461, Loss_test: 0.08243494096838701\n",
      "3917 - Loss_train: 0.08311346529540063, Loss_test: 0.08243508368077627\n",
      "3918 - Loss_train: 0.08311341509501226, Loss_test: 0.08243522633821163\n",
      "3919 - Loss_train: 0.08311336492058279, Loss_test: 0.08243536899144481\n",
      "3920 - Loss_train: 0.08311331477232801, Loss_test: 0.08243551163063985\n",
      "3921 - Loss_train: 0.08311326465017493, Loss_test: 0.0824356542223056\n",
      "3922 - Loss_train: 0.08311321455411948, Loss_test: 0.08243579680263258\n",
      "3923 - Loss_train: 0.08311316448412535, Loss_test: 0.08243593935799266\n",
      "3924 - Loss_train: 0.08311311444027811, Loss_test: 0.082436081894376\n",
      "3925 - Loss_train: 0.08311306442250722, Loss_test: 0.08243622440013704\n",
      "3926 - Loss_train: 0.08311301443079949, Loss_test: 0.08243636687427834\n",
      "3927 - Loss_train: 0.08311296446508158, Loss_test: 0.08243650934618918\n",
      "3928 - Loss_train: 0.083112914525431, Loss_test: 0.08243665178937133\n",
      "3929 - Loss_train: 0.08311286461177206, Loss_test: 0.08243679418011136\n",
      "3930 - Loss_train: 0.08311281472411142, Loss_test: 0.08243693658009135\n",
      "3931 - Loss_train: 0.08311276486239495, Loss_test: 0.08243707893705303\n",
      "3932 - Loss_train: 0.08311271502680867, Loss_test: 0.08243722128056642\n",
      "3933 - Loss_train: 0.08311266521707053, Loss_test: 0.0824373635987748\n",
      "3934 - Loss_train: 0.08311261543328184, Loss_test: 0.0824375058931884\n",
      "3935 - Loss_train: 0.08311256567538962, Loss_test: 0.08243764815616028\n",
      "3936 - Loss_train: 0.08311251594340392, Loss_test: 0.08243779039518419\n",
      "3937 - Loss_train: 0.08311246623738157, Loss_test: 0.08243793263271527\n",
      "3938 - Loss_train: 0.08311241655733545, Loss_test: 0.0824380748140598\n",
      "3939 - Loss_train: 0.08311236690305038, Loss_test: 0.08243821699229854\n",
      "3940 - Loss_train: 0.08311231727468824, Loss_test: 0.08243835913818837\n",
      "3941 - Loss_train: 0.08311226767209988, Loss_test: 0.08243850125729982\n",
      "3942 - Loss_train: 0.0831122180954371, Loss_test: 0.08243864335131164\n",
      "3943 - Loss_train: 0.0831121685445057, Loss_test: 0.08243878544544118\n",
      "3944 - Loss_train: 0.08311211901948322, Loss_test: 0.08243892747974488\n",
      "3945 - Loss_train: 0.08311206952012364, Loss_test: 0.08243906951313594\n",
      "3946 - Loss_train: 0.08311202004670722, Loss_test: 0.08243921152819883\n",
      "3947 - Loss_train: 0.08311197059897388, Loss_test: 0.08243935349592864\n",
      "3948 - Loss_train: 0.08311192117694241, Loss_test: 0.08243949544935114\n",
      "3949 - Loss_train: 0.08311187178076171, Loss_test: 0.08243963738090226\n",
      "3950 - Loss_train: 0.08311182241020147, Loss_test: 0.08243977928347514\n",
      "3951 - Loss_train: 0.08311177306549046, Loss_test: 0.08243992118070023\n",
      "3952 - Loss_train: 0.08311172374639945, Loss_test: 0.08244006303462728\n",
      "3953 - Loss_train: 0.08311167445308083, Loss_test: 0.0824402048784632\n",
      "3954 - Loss_train: 0.08311162518542874, Loss_test: 0.08244034667754097\n",
      "3955 - Loss_train: 0.08311157594333707, Loss_test: 0.08244048846418363\n",
      "3956 - Loss_train: 0.08311152672688366, Loss_test: 0.08244063023797904\n",
      "3957 - Loss_train: 0.08311147753607623, Loss_test: 0.0824407719715054\n",
      "3958 - Loss_train: 0.08311142837098251, Loss_test: 0.08244091369321069\n",
      "3959 - Loss_train: 0.083111379231489, Loss_test: 0.08244105538307203\n",
      "3960 - Loss_train: 0.083111330117495, Loss_test: 0.08244119706208913\n",
      "3961 - Loss_train: 0.08311128102909005, Loss_test: 0.08244133869654928\n",
      "3962 - Loss_train: 0.08311123196624554, Loss_test: 0.08244148031012236\n",
      "3963 - Loss_train: 0.08311118292898145, Loss_test: 0.08244162191524325\n",
      "3964 - Loss_train: 0.08311113391734819, Loss_test: 0.08244176348075102\n",
      "3965 - Loss_train: 0.0831110849311332, Loss_test: 0.08244190503147689\n",
      "3966 - Loss_train: 0.08311103597044853, Loss_test: 0.08244204655374154\n",
      "3967 - Loss_train: 0.08311098703535617, Loss_test: 0.08244218805348805\n",
      "3968 - Loss_train: 0.08311093812565296, Loss_test: 0.0824423295207784\n",
      "3969 - Loss_train: 0.08311088924153931, Loss_test: 0.08244247097995955\n",
      "3970 - Loss_train: 0.08311084038274148, Loss_test: 0.08244261241721772\n",
      "3971 - Loss_train: 0.0831107915494715, Loss_test: 0.08244275381182305\n",
      "3972 - Loss_train: 0.08311074274153482, Loss_test: 0.08244289519525465\n",
      "3973 - Loss_train: 0.0831106939590272, Loss_test: 0.08244303654780553\n",
      "3974 - Loss_train: 0.08311064520211732, Loss_test: 0.08244317788305018\n",
      "3975 - Loss_train: 0.08311059647044959, Loss_test: 0.08244331919208842\n",
      "3976 - Loss_train: 0.08311054776416334, Loss_test: 0.08244346047602512\n",
      "3977 - Loss_train: 0.08311049908328566, Loss_test: 0.08244360173920473\n",
      "3978 - Loss_train: 0.08311045042774758, Loss_test: 0.08244374297236896\n",
      "3979 - Loss_train: 0.08311040179763474, Loss_test: 0.08244388417508514\n",
      "3980 - Loss_train: 0.08311035319270202, Loss_test: 0.08244402537182621\n",
      "3981 - Loss_train: 0.083110304613247, Loss_test: 0.08244416652884254\n",
      "3982 - Loss_train: 0.08311025605901684, Loss_test: 0.08244430767167131\n",
      "3983 - Loss_train: 0.0831102075300596, Loss_test: 0.0824444487898968\n",
      "3984 - Loss_train: 0.083110159026471, Loss_test: 0.08244458988814506\n",
      "3985 - Loss_train: 0.08311011054805315, Loss_test: 0.08244473095270484\n",
      "3986 - Loss_train: 0.08311006209490505, Loss_test: 0.08244487200026081\n",
      "3987 - Loss_train: 0.08311001366699654, Loss_test: 0.08244501301392962\n",
      "3988 - Loss_train: 0.08310996526428685, Loss_test: 0.08244515399939081\n",
      "3989 - Loss_train: 0.08310991688676006, Loss_test: 0.08244529498354891\n",
      "3990 - Loss_train: 0.08310986853449406, Loss_test: 0.08244543593219845\n",
      "3991 - Loss_train: 0.08310982020743247, Loss_test: 0.08244557686265443\n",
      "3992 - Loss_train: 0.08310977190550647, Loss_test: 0.08244571775311409\n",
      "3993 - Loss_train: 0.08310972362885324, Loss_test: 0.08244585862989481\n",
      "3994 - Loss_train: 0.08310967537721553, Loss_test: 0.08244599948527459\n",
      "3995 - Loss_train: 0.08310962715074571, Loss_test: 0.08244614031600618\n",
      "3996 - Loss_train: 0.08310957894937668, Loss_test: 0.08244628110687907\n",
      "3997 - Loss_train: 0.08310953077311581, Loss_test: 0.0824464218950233\n",
      "3998 - Loss_train: 0.08310948262191424, Loss_test: 0.082446562645248\n",
      "3999 - Loss_train: 0.08310943449579564, Loss_test: 0.08244670338352968\n",
      "4000 - Loss_train: 0.08310938639483932, Loss_test: 0.0824468440875466\n",
      "4001 - Loss_train: 0.08310933831889206, Loss_test: 0.08244698477270211\n",
      "4002 - Loss_train: 0.08310929026798049, Loss_test: 0.08244712542191915\n",
      "4003 - Loss_train: 0.08310924224220792, Loss_test: 0.08244726607406108\n",
      "4004 - Loss_train: 0.08310919424134681, Loss_test: 0.08244740668404532\n",
      "4005 - Loss_train: 0.08310914626546066, Loss_test: 0.08244754726846733\n",
      "4006 - Loss_train: 0.08310909831458257, Loss_test: 0.08244768783170678\n",
      "4007 - Loss_train: 0.08310905038865225, Loss_test: 0.08244782836994363\n",
      "4008 - Loss_train: 0.08310900248774265, Loss_test: 0.08244796888347555\n",
      "4009 - Loss_train: 0.08310895461173833, Loss_test: 0.08244810938107708\n",
      "4010 - Loss_train: 0.08310890676072641, Loss_test: 0.08244824984433016\n",
      "4011 - Loss_train: 0.08310885893458714, Loss_test: 0.0824483902925118\n",
      "4012 - Loss_train: 0.08310881113332648, Loss_test: 0.08244853071470136\n",
      "4013 - Loss_train: 0.08310876335702774, Loss_test: 0.08244867110162288\n",
      "4014 - Loss_train: 0.083108715605604, Loss_test: 0.08244881148168685\n",
      "4015 - Loss_train: 0.08310866787912387, Loss_test: 0.0824489518240172\n",
      "4016 - Loss_train: 0.0831086201773871, Loss_test: 0.08244909214247617\n",
      "4017 - Loss_train: 0.08310857250052604, Loss_test: 0.08244923244613057\n",
      "4018 - Loss_train: 0.08310852484860698, Loss_test: 0.08244937273033157\n",
      "4019 - Loss_train: 0.08310847722139222, Loss_test: 0.08244951297008175\n",
      "4020 - Loss_train: 0.08310842961901055, Loss_test: 0.08244965320611397\n",
      "4021 - Loss_train: 0.08310838204141535, Loss_test: 0.08244979341255906\n",
      "4022 - Loss_train: 0.08310833448861031, Loss_test: 0.08244993358406362\n",
      "4023 - Loss_train: 0.08310828696057612, Loss_test: 0.08245007374746255\n",
      "4024 - Loss_train: 0.08310823945724209, Loss_test: 0.08245021387909532\n",
      "4025 - Loss_train: 0.08310819197863276, Loss_test: 0.08245035397330618\n",
      "4026 - Loss_train: 0.08310814452484686, Loss_test: 0.08245049406880163\n",
      "4027 - Loss_train: 0.08310809709574997, Loss_test: 0.08245063412133363\n",
      "4028 - Loss_train: 0.08310804969130374, Loss_test: 0.08245077415309914\n",
      "4029 - Loss_train: 0.08310800231154822, Loss_test: 0.08245091416885826\n",
      "4030 - Loss_train: 0.08310795495655539, Loss_test: 0.08245105415314911\n",
      "4031 - Loss_train: 0.08310790762615203, Loss_test: 0.08245119411492378\n",
      "4032 - Loss_train: 0.08310786032043757, Loss_test: 0.0824513340651806\n",
      "4033 - Loss_train: 0.08310781303938643, Loss_test: 0.08245147397359438\n",
      "4034 - Loss_train: 0.08310776578304521, Loss_test: 0.08245161386077461\n",
      "4035 - Loss_train: 0.08310771855122184, Loss_test: 0.0824517537404668\n",
      "4036 - Loss_train: 0.08310767134409787, Loss_test: 0.08245189358054889\n",
      "4037 - Loss_train: 0.08310762416145624, Loss_test: 0.08245203338911508\n",
      "4038 - Loss_train: 0.08310757700349863, Loss_test: 0.08245217319073808\n",
      "4039 - Loss_train: 0.08310752987009677, Loss_test: 0.08245231294951497\n",
      "4040 - Loss_train: 0.08310748276125969, Loss_test: 0.08245245270953042\n",
      "4041 - Loss_train: 0.08310743567685445, Loss_test: 0.08245259243854643\n",
      "4042 - Loss_train: 0.0831073886169443, Loss_test: 0.08245273212165269\n",
      "4043 - Loss_train: 0.0831073415816135, Loss_test: 0.08245287181562874\n",
      "4044 - Loss_train: 0.08310729457086427, Loss_test: 0.08245301143934865\n",
      "4045 - Loss_train: 0.08310724758461008, Loss_test: 0.08245315107924459\n",
      "4046 - Loss_train: 0.08310720062281575, Loss_test: 0.0824532906948358\n",
      "4047 - Loss_train: 0.08310715368537724, Loss_test: 0.08245343027259734\n",
      "4048 - Loss_train: 0.08310710677244815, Loss_test: 0.08245356980449109\n",
      "4049 - Loss_train: 0.08310705988401036, Loss_test: 0.08245370936237777\n",
      "4050 - Loss_train: 0.08310701301998076, Loss_test: 0.0824538488505783\n",
      "4051 - Loss_train: 0.08310696618026034, Loss_test: 0.082453988335209\n",
      "4052 - Loss_train: 0.08310691936490204, Loss_test: 0.08245412779489633\n",
      "4053 - Loss_train: 0.0831068725739721, Loss_test: 0.08245426723821378\n",
      "4054 - Loss_train: 0.08310682580738009, Loss_test: 0.08245440663892349\n",
      "4055 - Loss_train: 0.08310677906516344, Loss_test: 0.08245454602993753\n",
      "4056 - Loss_train: 0.08310673234737065, Loss_test: 0.08245468537872182\n",
      "4057 - Loss_train: 0.08310668565397177, Loss_test: 0.0824548247356331\n",
      "4058 - Loss_train: 0.08310663898476878, Loss_test: 0.08245496403589302\n",
      "4059 - Loss_train: 0.08310659233992594, Loss_test: 0.08245510333575075\n",
      "4060 - Loss_train: 0.08310654571931116, Loss_test: 0.08245524260715166\n",
      "4061 - Loss_train: 0.08310649912310364, Loss_test: 0.08245538184840014\n",
      "4062 - Loss_train: 0.0831064525510618, Loss_test: 0.08245552105510734\n",
      "4063 - Loss_train: 0.08310640600325978, Loss_test: 0.08245566026193829\n",
      "4064 - Loss_train: 0.08310635947979134, Loss_test: 0.08245579942223163\n",
      "4065 - Loss_train: 0.08310631298053509, Loss_test: 0.08245593858344936\n",
      "4066 - Loss_train: 0.08310626650546002, Loss_test: 0.08245607769530622\n",
      "4067 - Loss_train: 0.08310622005465239, Loss_test: 0.08245621679648182\n",
      "4068 - Loss_train: 0.08310617362794327, Loss_test: 0.08245635588091402\n",
      "4069 - Loss_train: 0.08310612722553261, Loss_test: 0.0824564949206062\n",
      "4070 - Loss_train: 0.08310608084720449, Loss_test: 0.08245663394184294\n",
      "4071 - Loss_train: 0.08310603449302637, Loss_test: 0.08245677295105343\n",
      "4072 - Loss_train: 0.08310598816299587, Loss_test: 0.08245691193667964\n",
      "4073 - Loss_train: 0.08310594185712546, Loss_test: 0.08245705088531467\n",
      "4074 - Loss_train: 0.08310589557537869, Loss_test: 0.08245718981851517\n",
      "4075 - Loss_train: 0.08310584931773426, Loss_test: 0.08245732871328265\n",
      "4076 - Loss_train: 0.08310580308411979, Loss_test: 0.08245746759637132\n",
      "4077 - Loss_train: 0.08310575687466319, Loss_test: 0.08245760646316645\n",
      "4078 - Loss_train: 0.08310571068938238, Loss_test: 0.08245774530114909\n",
      "4079 - Loss_train: 0.08310566452813793, Loss_test: 0.08245788410265818\n",
      "4080 - Loss_train: 0.08310561839089824, Loss_test: 0.08245802288915151\n",
      "4081 - Loss_train: 0.08310557227759921, Loss_test: 0.08245816165045093\n",
      "4082 - Loss_train: 0.0831055261883922, Loss_test: 0.08245830038251806\n",
      "4083 - Loss_train: 0.08310548012309447, Loss_test: 0.08245843910394925\n",
      "4084 - Loss_train: 0.08310543408180386, Loss_test: 0.08245857778801056\n",
      "4085 - Loss_train: 0.08310538806450868, Loss_test: 0.08245871645997167\n",
      "4086 - Loss_train: 0.08310534207130071, Loss_test: 0.08245885510712364\n",
      "4087 - Loss_train: 0.08310529610202268, Loss_test: 0.08245899371846449\n",
      "4088 - Loss_train: 0.08310525015660485, Loss_test: 0.08245913231464114\n",
      "4089 - Loss_train: 0.08310520423525511, Loss_test: 0.08245927087457176\n",
      "4090 - Loss_train: 0.08310515833774146, Loss_test: 0.08245940943118846\n",
      "4091 - Loss_train: 0.08310511246409442, Loss_test: 0.0824595479411142\n",
      "4092 - Loss_train: 0.08310506661430722, Loss_test: 0.08245968645602585\n",
      "4093 - Loss_train: 0.08310502078846378, Loss_test: 0.08245982492103611\n",
      "4094 - Loss_train: 0.08310497498654404, Loss_test: 0.08245996337025105\n",
      "4095 - Loss_train: 0.08310492920841603, Loss_test: 0.08246010180157766\n",
      "4096 - Loss_train: 0.08310488345411539, Loss_test: 0.08246024019829003\n",
      "4097 - Loss_train: 0.08310483772361095, Loss_test: 0.08246037857995278\n",
      "4098 - Loss_train: 0.08310479201691692, Loss_test: 0.08246051692424596\n",
      "4099 - Loss_train: 0.0831047463340634, Loss_test: 0.08246065526298978\n",
      "4100 - Loss_train: 0.0831047006750234, Loss_test: 0.08246079357221399\n",
      "4101 - Loss_train: 0.08310465503977268, Loss_test: 0.0824609318413526\n",
      "4102 - Loss_train: 0.08310460942836997, Loss_test: 0.08246107009997129\n",
      "4103 - Loss_train: 0.08310456384075413, Loss_test: 0.08246120833904058\n",
      "4104 - Loss_train: 0.08310451827680572, Loss_test: 0.08246134654663465\n",
      "4105 - Loss_train: 0.08310447273650688, Loss_test: 0.0824614847348983\n",
      "4106 - Loss_train: 0.08310442721998282, Loss_test: 0.08246162288913908\n",
      "4107 - Loss_train: 0.08310438172714021, Loss_test: 0.08246176103919549\n",
      "4108 - Loss_train: 0.08310433625799538, Loss_test: 0.08246189914913125\n",
      "4109 - Loss_train: 0.08310429081257457, Loss_test: 0.08246203724365199\n",
      "4110 - Loss_train: 0.08310424539080381, Loss_test: 0.08246217530103796\n",
      "4111 - Loss_train: 0.08310419999265939, Loss_test: 0.08246231333279366\n",
      "4112 - Loss_train: 0.08310415461830535, Loss_test: 0.08246245135310307\n",
      "4113 - Loss_train: 0.08310410926744662, Loss_test: 0.08246258934588857\n",
      "4114 - Loss_train: 0.0831040639402839, Loss_test: 0.08246272731593919\n",
      "4115 - Loss_train: 0.08310401863679268, Loss_test: 0.08246286525819692\n",
      "4116 - Loss_train: 0.0831039733567971, Loss_test: 0.08246300318895732\n",
      "4117 - Loss_train: 0.08310392810036797, Loss_test: 0.08246314107516012\n",
      "4118 - Loss_train: 0.08310388286753048, Loss_test: 0.0824632789536083\n",
      "4119 - Loss_train: 0.08310383765826787, Loss_test: 0.08246341680735376\n",
      "4120 - Loss_train: 0.08310379247252912, Loss_test: 0.08246355462858451\n",
      "4121 - Loss_train: 0.08310374731047447, Loss_test: 0.08246369243179659\n",
      "4122 - Loss_train: 0.08310370217182271, Loss_test: 0.08246383019486013\n",
      "4123 - Loss_train: 0.08310365705668431, Loss_test: 0.08246396795798872\n",
      "4124 - Loss_train: 0.08310361196502347, Loss_test: 0.08246410568476514\n",
      "4125 - Loss_train: 0.08310356689683153, Loss_test: 0.08246424338380721\n",
      "4126 - Loss_train: 0.08310352185227461, Loss_test: 0.0824643810686596\n",
      "4127 - Loss_train: 0.08310347683104898, Loss_test: 0.08246451872873906\n",
      "4128 - Loss_train: 0.08310343183329653, Loss_test: 0.082464656358163\n",
      "4129 - Loss_train: 0.08310338685899157, Loss_test: 0.08246479396741754\n",
      "4130 - Loss_train: 0.08310334190806115, Loss_test: 0.0824649315620117\n",
      "4131 - Loss_train: 0.08310329698053837, Loss_test: 0.08246506911397507\n",
      "4132 - Loss_train: 0.08310325207642769, Loss_test: 0.08246520664203111\n",
      "4133 - Loss_train: 0.08310320719575362, Loss_test: 0.08246534416730933\n",
      "4134 - Loss_train: 0.08310316233844663, Loss_test: 0.08246548165321657\n",
      "4135 - Loss_train: 0.0831031175044644, Loss_test: 0.08246561912372208\n",
      "4136 - Loss_train: 0.08310307269387297, Loss_test: 0.08246575655718878\n",
      "4137 - Loss_train: 0.08310302790670969, Loss_test: 0.08246589397368717\n",
      "4138 - Loss_train: 0.08310298314278952, Loss_test: 0.08246603136512624\n",
      "4139 - Loss_train: 0.0831029384021287, Loss_test: 0.08246616873747942\n",
      "4140 - Loss_train: 0.08310289368482779, Loss_test: 0.08246630607590541\n",
      "4141 - Loss_train: 0.08310284899081827, Loss_test: 0.08246644340454762\n",
      "4142 - Loss_train: 0.08310280432017192, Loss_test: 0.08246658070077008\n",
      "4143 - Loss_train: 0.08310275967270275, Loss_test: 0.08246671796294676\n",
      "4144 - Loss_train: 0.08310271504849169, Loss_test: 0.08246685522289703\n",
      "4145 - Loss_train: 0.08310267044753543, Loss_test: 0.08246699244055625\n",
      "4146 - Loss_train: 0.08310262586981369, Loss_test: 0.08246712964363655\n",
      "4147 - Loss_train: 0.08310258131537984, Loss_test: 0.08246726680971887\n",
      "4148 - Loss_train: 0.08310253678407037, Loss_test: 0.0824674039731197\n",
      "4149 - Loss_train: 0.08310249227598769, Loss_test: 0.08246754110260385\n",
      "4150 - Loss_train: 0.08310244779108275, Loss_test: 0.0824676782034922\n",
      "4151 - Loss_train: 0.08310240332935369, Loss_test: 0.08246781528519637\n",
      "4152 - Loss_train: 0.08310235889076874, Loss_test: 0.08246795233860496\n",
      "4153 - Loss_train: 0.08310231447532113, Loss_test: 0.0824680893757019\n",
      "4154 - Loss_train: 0.0831022700830079, Loss_test: 0.08246822637913383\n",
      "4155 - Loss_train: 0.08310222571384455, Loss_test: 0.08246836336531749\n",
      "4156 - Loss_train: 0.08310218136775589, Loss_test: 0.08246850032849018\n",
      "4157 - Loss_train: 0.08310213704476756, Loss_test: 0.08246863724937738\n",
      "4158 - Loss_train: 0.08310209274486073, Loss_test: 0.08246877417116169\n",
      "4159 - Loss_train: 0.08310204846812384, Loss_test: 0.08246891106841071\n",
      "4160 - Loss_train: 0.08310200421436911, Loss_test: 0.08246904793168346\n",
      "4161 - Loss_train: 0.08310195998369438, Loss_test: 0.08246918476131562\n",
      "4162 - Loss_train: 0.0831019157760699, Loss_test: 0.08246932158963849\n",
      "4163 - Loss_train: 0.08310187159140286, Loss_test: 0.08246945837238487\n",
      "4164 - Loss_train: 0.08310182742992607, Loss_test: 0.08246959513998389\n",
      "4165 - Loss_train: 0.08310178329143499, Loss_test: 0.08246973188264944\n",
      "4166 - Loss_train: 0.08310173917580387, Loss_test: 0.08246986861022684\n",
      "4167 - Loss_train: 0.08310169508324021, Loss_test: 0.08247000529642104\n",
      "4168 - Loss_train: 0.08310165101361515, Loss_test: 0.08247014198049638\n",
      "4169 - Loss_train: 0.083101606967044, Loss_test: 0.08247027862679086\n",
      "4170 - Loss_train: 0.08310156294321008, Loss_test: 0.08247041523779859\n",
      "4171 - Loss_train: 0.08310151894233762, Loss_test: 0.08247055184346674\n",
      "4172 - Loss_train: 0.08310147496443406, Loss_test: 0.08247068842009121\n",
      "4173 - Loss_train: 0.08310143100946173, Loss_test: 0.08247082496814148\n",
      "4174 - Loss_train: 0.08310138707746435, Loss_test: 0.08247096149153593\n",
      "4175 - Loss_train: 0.08310134316827511, Loss_test: 0.08247109801450948\n",
      "4176 - Loss_train: 0.08310129928189167, Loss_test: 0.08247123447115266\n",
      "4177 - Loss_train: 0.08310125541844623, Loss_test: 0.08247137093914444\n",
      "4178 - Loss_train: 0.08310121157781812, Loss_test: 0.08247150736565995\n",
      "4179 - Loss_train: 0.08310116776010157, Loss_test: 0.08247164377512675\n",
      "4180 - Loss_train: 0.08310112396517938, Loss_test: 0.08247178016084293\n",
      "4181 - Loss_train: 0.08310108019301271, Loss_test: 0.08247191652020401\n",
      "4182 - Loss_train: 0.08310103644369224, Loss_test: 0.0824720528548205\n",
      "4183 - Loss_train: 0.08310099271713156, Loss_test: 0.08247218916748862\n",
      "4184 - Loss_train: 0.08310094901334773, Loss_test: 0.08247232545516994\n",
      "4185 - Loss_train: 0.0831009053322467, Loss_test: 0.08247246170716435\n",
      "4186 - Loss_train: 0.08310086167405274, Loss_test: 0.08247259795430568\n",
      "4187 - Loss_train: 0.08310081803849251, Loss_test: 0.08247273416971242\n",
      "4188 - Loss_train: 0.08310077442576508, Loss_test: 0.08247287036310112\n",
      "4189 - Loss_train: 0.08310073083573755, Loss_test: 0.08247300653326048\n",
      "4190 - Loss_train: 0.08310068726836696, Loss_test: 0.08247314267827796\n",
      "4191 - Loss_train: 0.0831006437237798, Loss_test: 0.0824732787798742\n",
      "4192 - Loss_train: 0.08310060020182915, Loss_test: 0.08247341490649453\n",
      "4193 - Loss_train: 0.08310055670251586, Loss_test: 0.08247355095625507\n",
      "4194 - Loss_train: 0.08310051322582616, Loss_test: 0.0824736870118398\n",
      "4195 - Loss_train: 0.08310046977177694, Loss_test: 0.08247382302549106\n",
      "4196 - Loss_train: 0.08310042634046377, Loss_test: 0.08247395902630027\n",
      "4197 - Loss_train: 0.08310038293169268, Loss_test: 0.08247409499665519\n",
      "4198 - Loss_train: 0.08310033954555182, Loss_test: 0.0824742309566573\n",
      "4199 - Loss_train: 0.08310029618199112, Loss_test: 0.08247436689695486\n",
      "4200 - Loss_train: 0.08310025284100483, Loss_test: 0.08247450278804813\n",
      "4201 - Loss_train: 0.0831002095226985, Loss_test: 0.08247463865617956\n",
      "4202 - Loss_train: 0.08310016622696635, Loss_test: 0.08247477451720718\n",
      "4203 - Loss_train: 0.08310012295369155, Loss_test: 0.08247491034136722\n",
      "4204 - Loss_train: 0.08310007970299965, Loss_test: 0.08247504615140754\n",
      "4205 - Loss_train: 0.08310003647488748, Loss_test: 0.08247518194601895\n",
      "4206 - Loss_train: 0.0830999932691788, Loss_test: 0.08247531769593153\n",
      "4207 - Loss_train: 0.08309995008598556, Loss_test: 0.08247545343406669\n",
      "4208 - Loss_train: 0.08309990692532737, Loss_test: 0.08247558913840965\n",
      "4209 - Loss_train: 0.0830998637871116, Loss_test: 0.08247572484168579\n",
      "4210 - Loss_train: 0.08309982067145805, Loss_test: 0.08247586049495415\n",
      "4211 - Loss_train: 0.0830997775781878, Loss_test: 0.08247599613314195\n",
      "4212 - Loss_train: 0.08309973450736373, Loss_test: 0.08247613174515324\n",
      "4213 - Loss_train: 0.08309969145895181, Loss_test: 0.08247626734324923\n",
      "4214 - Loss_train: 0.08309964843300317, Loss_test: 0.08247640290568294\n",
      "4215 - Loss_train: 0.08309960542950748, Loss_test: 0.08247653844796521\n",
      "4216 - Loss_train: 0.0830995624483669, Loss_test: 0.08247667397691143\n",
      "4217 - Loss_train: 0.0830995194895748, Loss_test: 0.0824768094571726\n",
      "4218 - Loss_train: 0.08309947655327223, Loss_test: 0.08247694493390165\n",
      "4219 - Loss_train: 0.08309943363920436, Loss_test: 0.08247708038048433\n",
      "4220 - Loss_train: 0.08309939074765735, Loss_test: 0.08247721581228408\n",
      "4221 - Loss_train: 0.08309934787828524, Loss_test: 0.08247735120169646\n",
      "4222 - Loss_train: 0.0830993050312799, Loss_test: 0.08247748656403178\n",
      "4223 - Loss_train: 0.0830992622065875, Loss_test: 0.0824776219298122\n",
      "4224 - Loss_train: 0.08309921940433417, Loss_test: 0.08247775725786767\n",
      "4225 - Loss_train: 0.08309917662424977, Loss_test: 0.08247789256075501\n",
      "4226 - Loss_train: 0.08309913386652545, Loss_test: 0.08247802784299045\n",
      "4227 - Loss_train: 0.08309909113107317, Loss_test: 0.08247816309164352\n",
      "4228 - Loss_train: 0.08309904841782444, Loss_test: 0.08247829832359534\n",
      "4229 - Loss_train: 0.08309900572677817, Loss_test: 0.0824784335353124\n",
      "4230 - Loss_train: 0.08309896305801558, Loss_test: 0.08247856871218838\n",
      "4231 - Loss_train: 0.08309892041156572, Loss_test: 0.08247870387629266\n",
      "4232 - Loss_train: 0.08309887778735861, Loss_test: 0.08247883901194722\n",
      "4233 - Loss_train: 0.083098835185346, Loss_test: 0.08247897412635269\n",
      "4234 - Loss_train: 0.08309879260548596, Loss_test: 0.08247910921051411\n",
      "4235 - Loss_train: 0.08309875004785924, Loss_test: 0.08247924426918997\n",
      "4236 - Loss_train: 0.08309870751227968, Loss_test: 0.08247937931427424\n",
      "4237 - Loss_train: 0.0830986649989565, Loss_test: 0.08247951432206398\n",
      "4238 - Loss_train: 0.08309862250766503, Loss_test: 0.0824796493141674\n",
      "4239 - Loss_train: 0.08309858003859054, Loss_test: 0.08247978427602981\n",
      "4240 - Loss_train: 0.0830985375915466, Loss_test: 0.08247991922050231\n",
      "4241 - Loss_train: 0.08309849516679371, Loss_test: 0.08248005413847644\n",
      "4242 - Loss_train: 0.08309845276395444, Loss_test: 0.0824801890289309\n",
      "4243 - Loss_train: 0.08309841038320373, Loss_test: 0.08248032391019298\n",
      "4244 - Loss_train: 0.08309836802460563, Loss_test: 0.08248045875260615\n",
      "4245 - Loss_train: 0.08309832568798067, Loss_test: 0.08248059357337192\n",
      "4246 - Loss_train: 0.08309828337343943, Loss_test: 0.08248072838476778\n",
      "4247 - Loss_train: 0.08309824108096063, Loss_test: 0.08248086314678066\n",
      "4248 - Loss_train: 0.08309819881056836, Loss_test: 0.08248099789919464\n",
      "4249 - Loss_train: 0.08309815656212304, Loss_test: 0.0824811326373775\n",
      "4250 - Loss_train: 0.0830981143357696, Loss_test: 0.08248126732883994\n",
      "4251 - Loss_train: 0.08309807213130364, Loss_test: 0.0824814020122894\n",
      "4252 - Loss_train: 0.0830980299488721, Loss_test: 0.0824815366624876\n",
      "4253 - Loss_train: 0.08309798778833831, Loss_test: 0.08248167129173128\n",
      "4254 - Loss_train: 0.08309794564979192, Loss_test: 0.08248180591200026\n",
      "4255 - Loss_train: 0.0830979035332209, Loss_test: 0.08248194047512497\n",
      "4256 - Loss_train: 0.0830978614385653, Loss_test: 0.08248207504849146\n",
      "4257 - Loss_train: 0.08309781936594444, Loss_test: 0.08248220957605285\n",
      "4258 - Loss_train: 0.08309777731512755, Loss_test: 0.08248234408905936\n",
      "4259 - Loss_train: 0.08309773528633468, Loss_test: 0.08248247858137127\n",
      "4260 - Loss_train: 0.08309769327942058, Loss_test: 0.08248261303977479\n",
      "4261 - Loss_train: 0.08309765129433172, Loss_test: 0.08248274748868227\n",
      "4262 - Loss_train: 0.08309760933118278, Loss_test: 0.08248288189628\n",
      "4263 - Loss_train: 0.08309756738991025, Loss_test: 0.08248301628401274\n",
      "4264 - Loss_train: 0.08309752547044705, Loss_test: 0.08248315065496588\n",
      "4265 - Loss_train: 0.08309748357278214, Loss_test: 0.08248328500304954\n",
      "4266 - Loss_train: 0.08309744169690746, Loss_test: 0.0824834193154376\n",
      "4267 - Loss_train: 0.08309739984281898, Loss_test: 0.08248355361005903\n",
      "4268 - Loss_train: 0.08309735801060336, Loss_test: 0.08248368787936007\n",
      "4269 - Loss_train: 0.08309731620014496, Loss_test: 0.08248382213147265\n",
      "4270 - Loss_train: 0.08309727441145415, Loss_test: 0.08248395636071564\n",
      "4271 - Loss_train: 0.0830972326446477, Loss_test: 0.08248409054648922\n",
      "4272 - Loss_train: 0.08309719089959693, Loss_test: 0.08248422471708501\n",
      "4273 - Loss_train: 0.08309714917620363, Loss_test: 0.08248435889404364\n",
      "4274 - Loss_train: 0.08309710747451803, Loss_test: 0.0824844930105762\n",
      "4275 - Loss_train: 0.08309706579464883, Loss_test: 0.08248462710304864\n",
      "4276 - Loss_train: 0.08309702413648379, Loss_test: 0.08248476119216364\n",
      "4277 - Loss_train: 0.08309698250003174, Loss_test: 0.08248489524298579\n",
      "4278 - Loss_train: 0.0830969408852698, Loss_test: 0.08248502927460859\n",
      "4279 - Loss_train: 0.08309689929208354, Loss_test: 0.08248516328225074\n",
      "4280 - Loss_train: 0.08309685772068491, Loss_test: 0.08248529727436756\n",
      "4281 - Loss_train: 0.083096816170918, Loss_test: 0.08248543122700097\n",
      "4282 - Loss_train: 0.08309677464273899, Loss_test: 0.08248556516812171\n",
      "4283 - Loss_train: 0.08309673313625172, Loss_test: 0.08248569907958699\n",
      "4284 - Loss_train: 0.08309669165129091, Loss_test: 0.08248583296076091\n",
      "4285 - Loss_train: 0.08309665018794696, Loss_test: 0.08248596682409523\n",
      "4286 - Loss_train: 0.0830966087461844, Loss_test: 0.08248610066978822\n",
      "4287 - Loss_train: 0.0830965673259829, Loss_test: 0.08248623447116188\n",
      "4288 - Loss_train: 0.08309652592746734, Loss_test: 0.08248636827406962\n",
      "4289 - Loss_train: 0.08309648455045163, Loss_test: 0.08248650204124085\n",
      "4290 - Loss_train: 0.08309644319498223, Loss_test: 0.08248663578042664\n",
      "4291 - Loss_train: 0.08309640186114607, Loss_test: 0.08248676949118623\n",
      "4292 - Loss_train: 0.08309636054881171, Loss_test: 0.08248690319603691\n",
      "4293 - Loss_train: 0.08309631925793619, Loss_test: 0.08248703686722289\n",
      "4294 - Loss_train: 0.08309627798856752, Loss_test: 0.08248717051469699\n",
      "4295 - Loss_train: 0.0830962367406972, Loss_test: 0.0824873041383092\n",
      "4296 - Loss_train: 0.08309619551431421, Loss_test: 0.08248743773242037\n",
      "4297 - Loss_train: 0.08309615430937724, Loss_test: 0.08248757132261905\n",
      "4298 - Loss_train: 0.08309611312594416, Loss_test: 0.08248770486271945\n",
      "4299 - Loss_train: 0.0830960719639778, Loss_test: 0.08248783839312565\n",
      "4300 - Loss_train: 0.08309603082339484, Loss_test: 0.08248797189492654\n",
      "4301 - Loss_train: 0.08309598970425737, Loss_test: 0.08248810537803414\n",
      "4302 - Loss_train: 0.0830959486065731, Loss_test: 0.08248823884250266\n",
      "4303 - Loss_train: 0.08309590753022043, Loss_test: 0.08248837225483324\n",
      "4304 - Loss_train: 0.08309586647534171, Loss_test: 0.08248850568901395\n",
      "4305 - Loss_train: 0.08309582544179611, Loss_test: 0.08248863905592188\n",
      "4306 - Loss_train: 0.08309578442967634, Loss_test: 0.08248877241936427\n",
      "4307 - Loss_train: 0.08309574343899172, Loss_test: 0.0824889057652042\n",
      "4308 - Loss_train: 0.08309570246959357, Loss_test: 0.08248903906162262\n",
      "4309 - Loss_train: 0.08309566152153317, Loss_test: 0.08248917237208159\n",
      "4310 - Loss_train: 0.08309562059476573, Loss_test: 0.08248930561435358\n",
      "4311 - Loss_train: 0.08309557968932493, Loss_test: 0.08248943886972512\n",
      "4312 - Loss_train: 0.0830955388052117, Loss_test: 0.08248957208296677\n",
      "4313 - Loss_train: 0.08309549794237142, Loss_test: 0.08248970527888107\n",
      "4314 - Loss_train: 0.08309545710086627, Loss_test: 0.0824898384532531\n",
      "4315 - Loss_train: 0.08309541628072023, Loss_test: 0.08248997157991429\n",
      "4316 - Loss_train: 0.08309537548184177, Loss_test: 0.08249010471314604\n",
      "4317 - Loss_train: 0.08309533470410592, Loss_test: 0.08249023780904009\n",
      "4318 - Loss_train: 0.08309529394767202, Loss_test: 0.0824903708713334\n",
      "4319 - Loss_train: 0.08309525321239922, Loss_test: 0.08249050393573576\n",
      "4320 - Loss_train: 0.08309521249848092, Loss_test: 0.08249063694846313\n",
      "4321 - Loss_train: 0.08309517180562136, Loss_test: 0.08249076994958973\n",
      "4322 - Loss_train: 0.08309513113400417, Loss_test: 0.08249090292684126\n",
      "4323 - Loss_train: 0.08309509048358986, Loss_test: 0.08249103588521484\n",
      "4324 - Loss_train: 0.08309504985434706, Loss_test: 0.08249116881800506\n",
      "4325 - Loss_train: 0.08309500924622314, Loss_test: 0.08249130171206538\n",
      "4326 - Loss_train: 0.0830949686593102, Loss_test: 0.0824914345884741\n",
      "4327 - Loss_train: 0.08309492809355087, Loss_test: 0.08249156744813811\n",
      "4328 - Loss_train: 0.08309488754885135, Loss_test: 0.08249170028028699\n",
      "4329 - Loss_train: 0.08309484702535927, Loss_test: 0.0824918330904133\n",
      "4330 - Loss_train: 0.08309480652295606, Loss_test: 0.08249196587881218\n",
      "4331 - Loss_train: 0.0830947660416436, Loss_test: 0.08249209863972018\n",
      "4332 - Loss_train: 0.08309472558145066, Loss_test: 0.08249223137988128\n",
      "4333 - Loss_train: 0.08309468514219152, Loss_test: 0.08249236409890882\n",
      "4334 - Loss_train: 0.08309464472406566, Loss_test: 0.08249249677679202\n",
      "4335 - Loss_train: 0.08309460432697093, Loss_test: 0.08249262943465913\n",
      "4336 - Loss_train: 0.08309456395096199, Loss_test: 0.08249276209370676\n",
      "4337 - Loss_train: 0.08309452359594889, Loss_test: 0.08249289470845662\n",
      "4338 - Loss_train: 0.08309448326208287, Loss_test: 0.08249302729442055\n",
      "4339 - Loss_train: 0.08309444294911032, Loss_test: 0.08249315986168591\n",
      "4340 - Loss_train: 0.083094402657203, Loss_test: 0.0824932924069199\n",
      "4341 - Loss_train: 0.08309436238631834, Loss_test: 0.08249342493995682\n",
      "4342 - Loss_train: 0.08309432213629267, Loss_test: 0.08249355743349666\n",
      "4343 - Loss_train: 0.08309428190727633, Loss_test: 0.0824936899086964\n",
      "4344 - Loss_train: 0.08309424169916797, Loss_test: 0.08249382235075906\n",
      "4345 - Loss_train: 0.08309420151203091, Loss_test: 0.08249395477664466\n",
      "4346 - Loss_train: 0.08309416134586509, Loss_test: 0.08249408719710026\n",
      "4347 - Loss_train: 0.08309412120065976, Loss_test: 0.08249421956341765\n",
      "4348 - Loss_train: 0.08309408107631049, Loss_test: 0.08249435192853616\n",
      "4349 - Loss_train: 0.08309404097287607, Loss_test: 0.08249448424935421\n",
      "4350 - Loss_train: 0.08309400089040465, Loss_test: 0.08249461654349859\n",
      "4351 - Loss_train: 0.08309396082880044, Loss_test: 0.08249474884460564\n",
      "4352 - Loss_train: 0.08309392078796099, Loss_test: 0.08249488108501288\n",
      "4353 - Loss_train: 0.0830938807679531, Loss_test: 0.0824950133289055\n",
      "4354 - Loss_train: 0.08309384076880912, Loss_test: 0.08249514554198888\n",
      "4355 - Loss_train: 0.083093800790542, Loss_test: 0.08249527773314451\n",
      "4356 - Loss_train: 0.08309376083318863, Loss_test: 0.0824954098935284\n",
      "4357 - Loss_train: 0.08309372089663823, Loss_test: 0.08249554203885015\n",
      "4358 - Loss_train: 0.08309368098080715, Loss_test: 0.0824956741497212\n",
      "4359 - Loss_train: 0.08309364108573006, Loss_test: 0.08249580623311932\n",
      "4360 - Loss_train: 0.0830936012115663, Loss_test: 0.08249593830896491\n",
      "4361 - Loss_train: 0.08309356135808005, Loss_test: 0.08249607034101743\n",
      "4362 - Loss_train: 0.0830935215253565, Loss_test: 0.08249620238124888\n",
      "4363 - Loss_train: 0.08309348171339621, Loss_test: 0.08249633436895021\n",
      "4364 - Loss_train: 0.08309344192224924, Loss_test: 0.08249646632467968\n",
      "4365 - Loss_train: 0.08309340215174073, Loss_test: 0.08249659828770531\n",
      "4366 - Loss_train: 0.08309336240196676, Loss_test: 0.08249673020413041\n",
      "4367 - Loss_train: 0.08309332267300326, Loss_test: 0.08249686211436683\n",
      "4368 - Loss_train: 0.08309328296462201, Loss_test: 0.08249699397391506\n",
      "4369 - Loss_train: 0.08309324327688096, Loss_test: 0.0824971258310821\n",
      "4370 - Loss_train: 0.08309320360995452, Loss_test: 0.08249725766011037\n",
      "4371 - Loss_train: 0.0830931639635756, Loss_test: 0.08249738946778569\n",
      "4372 - Loss_train: 0.08309312433786915, Loss_test: 0.08249752124616602\n",
      "4373 - Loss_train: 0.08309308473279112, Loss_test: 0.08249765300605691\n",
      "4374 - Loss_train: 0.08309304514836188, Loss_test: 0.08249778473048146\n",
      "4375 - Loss_train: 0.08309300558454633, Loss_test: 0.08249791643871202\n",
      "4376 - Loss_train: 0.083092966041437, Loss_test: 0.08249804813422465\n",
      "4377 - Loss_train: 0.083092926518826, Loss_test: 0.08249817978340059\n",
      "4378 - Loss_train: 0.08309288701688183, Loss_test: 0.08249831142019043\n",
      "4379 - Loss_train: 0.0830928475355191, Loss_test: 0.08249844303745041\n",
      "4380 - Loss_train: 0.08309280807478471, Loss_test: 0.08249857461607647\n",
      "4381 - Loss_train: 0.08309276863447708, Loss_test: 0.0824987061950662\n",
      "4382 - Loss_train: 0.08309272921471136, Loss_test: 0.08249883772633128\n",
      "4383 - Loss_train: 0.08309268981557244, Loss_test: 0.0824989692513779\n",
      "4384 - Loss_train: 0.08309265043685728, Loss_test: 0.08249910073984834\n",
      "4385 - Loss_train: 0.08309261107867272, Loss_test: 0.08249923219622964\n",
      "4386 - Loss_train: 0.08309257174096311, Loss_test: 0.08249936365977688\n",
      "4387 - Loss_train: 0.0830925324236881, Loss_test: 0.0824994950737548\n",
      "4388 - Loss_train: 0.08309249312691566, Loss_test: 0.08249962646975229\n",
      "4389 - Loss_train: 0.08309245385063835, Loss_test: 0.08249975784819254\n",
      "4390 - Loss_train: 0.08309241459491314, Loss_test: 0.08249988919193874\n",
      "4391 - Loss_train: 0.08309237535955936, Loss_test: 0.08250002052855052\n",
      "4392 - Loss_train: 0.08309233614465386, Loss_test: 0.08250015183362829\n",
      "4393 - Loss_train: 0.08309229695018697, Loss_test: 0.08250028311739704\n",
      "4394 - Loss_train: 0.08309225777610307, Loss_test: 0.0825004143610349\n",
      "4395 - Loss_train: 0.08309221862243951, Loss_test: 0.08250054559151733\n",
      "4396 - Loss_train: 0.08309217948919075, Loss_test: 0.08250067678731199\n",
      "4397 - Loss_train: 0.08309214037627353, Loss_test: 0.08250080796416501\n",
      "4398 - Loss_train: 0.0830921012837186, Loss_test: 0.08250093914068463\n",
      "4399 - Loss_train: 0.08309206221155066, Loss_test: 0.08250107026640775\n",
      "4400 - Loss_train: 0.08309202315977962, Loss_test: 0.08250120137424227\n",
      "4401 - Loss_train: 0.08309198412841567, Loss_test: 0.08250133246560881\n",
      "4402 - Loss_train: 0.08309194511725813, Loss_test: 0.08250146353142161\n",
      "4403 - Loss_train: 0.08309190612651048, Loss_test: 0.08250159457778748\n",
      "4404 - Loss_train: 0.08309186715600794, Loss_test: 0.08250172558754568\n",
      "4405 - Loss_train: 0.08309182820581694, Loss_test: 0.08250185657971676\n",
      "4406 - Loss_train: 0.08309178927610288, Loss_test: 0.08250198755097678\n",
      "4407 - Loss_train: 0.08309175036645804, Loss_test: 0.08250211848703876\n",
      "4408 - Loss_train: 0.08309171147712376, Loss_test: 0.08250224941056215\n",
      "4409 - Loss_train: 0.08309167260813631, Loss_test: 0.0825023803027678\n",
      "4410 - Loss_train: 0.08309163375940606, Loss_test: 0.08250251118375664\n",
      "4411 - Loss_train: 0.08309159493088453, Loss_test: 0.08250264201907737\n",
      "4412 - Loss_train: 0.08309155612253558, Loss_test: 0.08250277284887683\n",
      "4413 - Loss_train: 0.08309151733441465, Loss_test: 0.08250290365758267\n",
      "4414 - Loss_train: 0.08309147856653522, Loss_test: 0.08250303442469502\n",
      "4415 - Loss_train: 0.08309143981881884, Loss_test: 0.08250316518113043\n",
      "4416 - Loss_train: 0.08309140109122905, Loss_test: 0.0825032959001565\n",
      "4417 - Loss_train: 0.08309136238386221, Loss_test: 0.08250342661823407\n",
      "4418 - Loss_train: 0.08309132369672227, Loss_test: 0.08250355727929906\n",
      "4419 - Loss_train: 0.08309128502962843, Loss_test: 0.08250368795469713\n",
      "4420 - Loss_train: 0.08309124638264455, Loss_test: 0.08250381858363989\n",
      "4421 - Loss_train: 0.08309120775585822, Loss_test: 0.0825039491884267\n",
      "4422 - Loss_train: 0.08309116914928769, Loss_test: 0.0825040797783145\n",
      "4423 - Loss_train: 0.0830911305627215, Loss_test: 0.08250421033109161\n",
      "4424 - Loss_train: 0.08309109199628151, Loss_test: 0.08250434087985323\n",
      "4425 - Loss_train: 0.08309105344993353, Loss_test: 0.08250447139218427\n",
      "4426 - Loss_train: 0.08309101492370143, Loss_test: 0.08250460187239607\n",
      "4427 - Loss_train: 0.08309097641755828, Loss_test: 0.08250473234636868\n",
      "4428 - Loss_train: 0.08309093793143603, Loss_test: 0.08250486278975586\n",
      "4429 - Loss_train: 0.08309089946540704, Loss_test: 0.08250499320603258\n",
      "4430 - Loss_train: 0.08309086101942642, Loss_test: 0.08250512359676106\n",
      "4431 - Loss_train: 0.08309082259338915, Loss_test: 0.0825052539666051\n",
      "4432 - Loss_train: 0.08309078418743364, Loss_test: 0.08250538432858885\n",
      "4433 - Loss_train: 0.0830907458014542, Loss_test: 0.08250551464137165\n",
      "4434 - Loss_train: 0.08309070743542807, Loss_test: 0.08250564493821697\n",
      "4435 - Loss_train: 0.08309066908936637, Loss_test: 0.0825057752154346\n",
      "4436 - Loss_train: 0.08309063076330606, Loss_test: 0.0825059054697938\n",
      "4437 - Loss_train: 0.08309059245730523, Loss_test: 0.08250603569106568\n",
      "4438 - Loss_train: 0.08309055417115649, Loss_test: 0.08250616589999817\n",
      "4439 - Loss_train: 0.08309051590494505, Loss_test: 0.08250629606431195\n",
      "4440 - Loss_train: 0.08309047765879013, Loss_test: 0.08250642624300372\n",
      "4441 - Loss_train: 0.08309043943241531, Loss_test: 0.08250655636092062\n",
      "4442 - Loss_train: 0.0830904012260765, Loss_test: 0.08250668647791075\n",
      "4443 - Loss_train: 0.08309036303963725, Loss_test: 0.08250681655885758\n",
      "4444 - Loss_train: 0.08309032487297738, Loss_test: 0.08250694661155514\n",
      "4445 - Loss_train: 0.0830902867261951, Loss_test: 0.08250707664877399\n",
      "4446 - Loss_train: 0.08309024859930444, Loss_test: 0.08250720666670157\n",
      "4447 - Loss_train: 0.08309021049229207, Loss_test: 0.08250733665829804\n",
      "4448 - Loss_train: 0.08309017240514152, Loss_test: 0.08250746662342268\n",
      "4449 - Loss_train: 0.08309013433779817, Loss_test: 0.08250759656175229\n",
      "4450 - Loss_train: 0.08309009629038515, Loss_test: 0.08250772646592354\n",
      "4451 - Loss_train: 0.08309005826269134, Loss_test: 0.08250785636787633\n",
      "4452 - Loss_train: 0.08309002025484029, Loss_test: 0.08250798623465007\n",
      "4453 - Loss_train: 0.08308998226678187, Loss_test: 0.0825081160925647\n",
      "4454 - Loss_train: 0.08308994429859086, Loss_test: 0.08250824591493638\n",
      "4455 - Loss_train: 0.08308990635010328, Loss_test: 0.0825083757000971\n",
      "4456 - Loss_train: 0.08308986842136448, Loss_test: 0.08250850548031341\n",
      "4457 - Loss_train: 0.08308983051248806, Loss_test: 0.08250863524105059\n",
      "4458 - Loss_train: 0.08308979262322054, Loss_test: 0.08250876495513165\n",
      "4459 - Loss_train: 0.08308975475372837, Loss_test: 0.08250889465641688\n",
      "4460 - Loss_train: 0.08308971690397843, Loss_test: 0.08250902434340475\n",
      "4461 - Loss_train: 0.08308967907404885, Loss_test: 0.08250915399754527\n",
      "4462 - Loss_train: 0.08308964126377935, Loss_test: 0.082509283615926\n",
      "4463 - Loss_train: 0.08308960347323416, Loss_test: 0.08250941323064262\n",
      "4464 - Loss_train: 0.08308956570227752, Loss_test: 0.08250954281283257\n",
      "4465 - Loss_train: 0.08308952795109352, Loss_test: 0.08250967237116885\n",
      "4466 - Loss_train: 0.08308949021949188, Loss_test: 0.0825098019187377\n",
      "4467 - Loss_train: 0.0830894525075097, Loss_test: 0.08250993142269863\n",
      "4468 - Loss_train: 0.08308941481518005, Loss_test: 0.08251006091695143\n",
      "4469 - Loss_train: 0.08308937714261802, Loss_test: 0.08251019038491164\n",
      "4470 - Loss_train: 0.08308933948965705, Loss_test: 0.08251031981924697\n",
      "4471 - Loss_train: 0.08308930185618633, Loss_test: 0.08251044923554206\n",
      "4472 - Loss_train: 0.08308926424234492, Loss_test: 0.08251057863780208\n",
      "4473 - Loss_train: 0.08308922664807747, Loss_test: 0.0825107080003206\n",
      "4474 - Loss_train: 0.08308918907341153, Loss_test: 0.08251083734338456\n",
      "4475 - Loss_train: 0.083089151518331, Loss_test: 0.08251096666414995\n",
      "4476 - Loss_train: 0.08308911398281116, Loss_test: 0.0825110959760382\n",
      "4477 - Loss_train: 0.08308907646693559, Loss_test: 0.08251122523113295\n",
      "4478 - Loss_train: 0.08308903897063083, Loss_test: 0.0825113544993608\n",
      "4479 - Loss_train: 0.08308900149382653, Loss_test: 0.08251148370695544\n",
      "4480 - Loss_train: 0.08308896403655615, Loss_test: 0.08251161291319326\n",
      "4481 - Loss_train: 0.08308892659869437, Loss_test: 0.08251174208838885\n",
      "4482 - Loss_train: 0.0830888891803257, Loss_test: 0.08251187124343763\n",
      "4483 - Loss_train: 0.08308885178146906, Loss_test: 0.08251200038676908\n",
      "4484 - Loss_train: 0.08308881440218699, Loss_test: 0.08251212947921154\n",
      "4485 - Loss_train: 0.08308877704234123, Loss_test: 0.08251225857206909\n",
      "4486 - Loss_train: 0.08308873970187303, Loss_test: 0.08251238762494395\n",
      "4487 - Loss_train: 0.08308870238090146, Loss_test: 0.08251251666530657\n",
      "4488 - Loss_train: 0.08308866507939927, Loss_test: 0.08251264567798428\n",
      "4489 - Loss_train: 0.08308862779727755, Loss_test: 0.08251277466392583\n",
      "4490 - Loss_train: 0.08308859053455168, Loss_test: 0.08251290362396174\n",
      "4491 - Loss_train: 0.08308855329125454, Loss_test: 0.08251303257212447\n",
      "4492 - Loss_train: 0.08308851606736327, Loss_test: 0.08251316148730511\n",
      "4493 - Loss_train: 0.08308847886289214, Loss_test: 0.08251329038218662\n",
      "4494 - Loss_train: 0.08308844167780273, Loss_test: 0.08251341924764523\n",
      "4495 - Loss_train: 0.08308840451208907, Loss_test: 0.0825135480984696\n",
      "4496 - Loss_train: 0.08308836736579879, Loss_test: 0.08251367691636702\n",
      "4497 - Loss_train: 0.0830883302387775, Loss_test: 0.08251380571239601\n",
      "4498 - Loss_train: 0.08308829313111665, Loss_test: 0.08251393449325484\n",
      "4499 - Loss_train: 0.08308825604274123, Loss_test: 0.08251406323268223\n",
      "4500 - Loss_train: 0.08308821897367036, Loss_test: 0.08251419196967558\n",
      "4501 - Loss_train: 0.08308818192396711, Loss_test: 0.08251432067139702\n",
      "4502 - Loss_train: 0.08308814489354925, Loss_test: 0.08251444934956624\n",
      "4503 - Loss_train: 0.08308810788241972, Loss_test: 0.08251457800410888\n",
      "4504 - Loss_train: 0.08308807089057986, Loss_test: 0.08251470662867297\n",
      "4505 - Loss_train: 0.08308803391806166, Loss_test: 0.08251483523939461\n",
      "4506 - Loss_train: 0.08308799696483372, Loss_test: 0.08251496382222138\n",
      "4507 - Loss_train: 0.08308796003087854, Loss_test: 0.08251509238633335\n",
      "4508 - Loss_train: 0.08308792311604568, Loss_test: 0.08251522091564308\n",
      "4509 - Loss_train: 0.08308788622040833, Loss_test: 0.08251534944434452\n",
      "4510 - Loss_train: 0.08308784934414343, Loss_test: 0.08251547791397881\n",
      "4511 - Loss_train: 0.08308781248696527, Loss_test: 0.08251560639001766\n",
      "4512 - Loss_train: 0.0830877756490333, Loss_test: 0.08251573483148368\n",
      "4513 - Loss_train: 0.08308773883032688, Loss_test: 0.08251586323427153\n",
      "4514 - Loss_train: 0.08308770203075994, Loss_test: 0.08251599164404064\n",
      "4515 - Loss_train: 0.0830876652504046, Loss_test: 0.082516120004461\n",
      "4516 - Loss_train: 0.08308762848909489, Loss_test: 0.08251624835505313\n",
      "4517 - Loss_train: 0.08308759174691398, Loss_test: 0.08251637666371162\n",
      "4518 - Loss_train: 0.08308755502388428, Loss_test: 0.08251650496907459\n",
      "4519 - Loss_train: 0.0830875183201274, Loss_test: 0.08251663324721153\n",
      "4520 - Loss_train: 0.08308748163548348, Loss_test: 0.08251676148252293\n",
      "4521 - Loss_train: 0.08308744496991222, Loss_test: 0.08251688972666742\n",
      "4522 - Loss_train: 0.08308740832338024, Loss_test: 0.08251701793388115\n",
      "4523 - Loss_train: 0.08308737169605959, Loss_test: 0.08251714609886211\n",
      "4524 - Loss_train: 0.08308733508771272, Loss_test: 0.08251727425559018\n",
      "4525 - Loss_train: 0.08308729849841207, Loss_test: 0.08251740239267838\n",
      "4526 - Loss_train: 0.08308726192821846, Loss_test: 0.08251753050166064\n",
      "4527 - Loss_train: 0.08308722537709613, Loss_test: 0.08251765858326308\n",
      "4528 - Loss_train: 0.0830871888450275, Loss_test: 0.08251778666005657\n",
      "4529 - Loss_train: 0.08308715233194676, Loss_test: 0.08251791467969526\n",
      "4530 - Loss_train: 0.08308711583784867, Loss_test: 0.08251804268023913\n",
      "4531 - Loss_train: 0.08308707936282826, Loss_test: 0.08251817068375626\n",
      "4532 - Loss_train: 0.08308704290678189, Loss_test: 0.082518298642491\n",
      "4533 - Loss_train: 0.0830870064697081, Loss_test: 0.08251842659480683\n",
      "4534 - Loss_train: 0.08308697005158813, Loss_test: 0.08251855450585266\n",
      "4535 - Loss_train: 0.08308693365247757, Loss_test: 0.0825186824048204\n",
      "4536 - Loss_train: 0.08308689727235215, Loss_test: 0.08251881027809814\n",
      "4537 - Loss_train: 0.08308686091122572, Loss_test: 0.082518938125482\n",
      "4538 - Loss_train: 0.08308682456896781, Loss_test: 0.08251906595448714\n",
      "4539 - Loss_train: 0.08308678824559815, Loss_test: 0.08251919374644184\n",
      "4540 - Loss_train: 0.08308675194121208, Loss_test: 0.08251932153374604\n",
      "4541 - Loss_train: 0.08308671565579914, Loss_test: 0.08251944926477207\n",
      "4542 - Loss_train: 0.08308667938921169, Loss_test: 0.08251957701534458\n",
      "4543 - Loss_train: 0.08308664314162031, Loss_test: 0.0825197047025576\n",
      "4544 - Loss_train: 0.08308660691290218, Loss_test: 0.08251983239179966\n",
      "4545 - Loss_train: 0.08308657070298806, Loss_test: 0.08251996003926111\n",
      "4546 - Loss_train: 0.08308653451189385, Loss_test: 0.08252008767881457\n",
      "4547 - Loss_train: 0.08308649833974906, Loss_test: 0.08252021528562002\n",
      "4548 - Loss_train: 0.08308646218641162, Loss_test: 0.08252034286923206\n",
      "4549 - Loss_train: 0.08308642605196868, Loss_test: 0.08252047042538654\n",
      "4550 - Loss_train: 0.08308638993625295, Loss_test: 0.08252059796887712\n",
      "4551 - Loss_train: 0.08308635383933997, Loss_test: 0.0825207254893585\n",
      "4552 - Loss_train: 0.0830863177613084, Loss_test: 0.08252085296898169\n",
      "4553 - Loss_train: 0.08308628170207395, Loss_test: 0.08252098043260099\n",
      "4554 - Loss_train: 0.08308624566155579, Loss_test: 0.08252110789366812\n",
      "4555 - Loss_train: 0.08308620963983919, Loss_test: 0.08252123530046422\n",
      "4556 - Loss_train: 0.0830861736368467, Loss_test: 0.08252136269400692\n",
      "4557 - Loss_train: 0.0830861376526001, Loss_test: 0.08252149007811996\n",
      "4558 - Loss_train: 0.08308610168708104, Loss_test: 0.08252161740901609\n",
      "4559 - Loss_train: 0.08308606574035127, Loss_test: 0.08252174476089764\n",
      "4560 - Loss_train: 0.08308602981241026, Loss_test: 0.08252187203529633\n",
      "4561 - Loss_train: 0.0830859939030739, Loss_test: 0.0825219993241266\n",
      "4562 - Loss_train: 0.08308595801244437, Loss_test: 0.08252212656829924\n",
      "4563 - Loss_train: 0.08308592214052872, Loss_test: 0.08252225381171316\n",
      "4564 - Loss_train: 0.08308588628736097, Loss_test: 0.08252238100057516\n",
      "4565 - Loss_train: 0.08308585045279, Loss_test: 0.08252250820257197\n",
      "4566 - Loss_train: 0.08308581463693561, Loss_test: 0.08252263536203025\n",
      "4567 - Loss_train: 0.08308577883976545, Loss_test: 0.08252276249405069\n",
      "4568 - Loss_train: 0.08308574306130627, Loss_test: 0.08252288960378955\n",
      "4569 - Loss_train: 0.08308570730144392, Loss_test: 0.08252301667487603\n",
      "4570 - Loss_train: 0.08308567156018432, Loss_test: 0.08252314375485152\n",
      "4571 - Loss_train: 0.08308563583747271, Loss_test: 0.08252327079412819\n",
      "4572 - Loss_train: 0.08308560013349506, Loss_test: 0.08252339780665272\n",
      "4573 - Loss_train: 0.08308556444803167, Loss_test: 0.08252352479844031\n",
      "4574 - Loss_train: 0.08308552878117903, Loss_test: 0.08252365177475213\n",
      "4575 - Loss_train: 0.08308549313297216, Loss_test: 0.08252377871431817\n",
      "4576 - Loss_train: 0.08308545750321422, Loss_test: 0.08252390562759553\n",
      "4577 - Loss_train: 0.08308542189204589, Loss_test: 0.0825240325351173\n",
      "4578 - Loss_train: 0.08308538629945451, Loss_test: 0.08252415939518996\n",
      "4579 - Loss_train: 0.08308535072537207, Loss_test: 0.08252428626244245\n",
      "4580 - Loss_train: 0.08308531516994888, Loss_test: 0.08252441309328175\n",
      "4581 - Loss_train: 0.08308527963293305, Loss_test: 0.08252453987899216\n",
      "4582 - Loss_train: 0.0830852441145227, Loss_test: 0.08252466666904174\n",
      "4583 - Loss_train: 0.08308520861458646, Loss_test: 0.08252479341638104\n",
      "4584 - Loss_train: 0.08308517313308021, Loss_test: 0.08252492015558001\n",
      "4585 - Loss_train: 0.08308513767010361, Loss_test: 0.08252504687084683\n",
      "4586 - Loss_train: 0.0830851022256176, Loss_test: 0.08252517353584703\n",
      "4587 - Loss_train: 0.08308506679951781, Loss_test: 0.08252530021235822\n",
      "4588 - Loss_train: 0.08308503139200106, Loss_test: 0.08252542684871177\n",
      "4589 - Loss_train: 0.08308499600284182, Loss_test: 0.08252555345561671\n",
      "4590 - Loss_train: 0.08308496063218654, Loss_test: 0.08252568004502009\n",
      "4591 - Loss_train: 0.0830849252799783, Loss_test: 0.08252580660769927\n",
      "4592 - Loss_train: 0.08308488994603935, Loss_test: 0.08252593316138317\n",
      "4593 - Loss_train: 0.08308485463054115, Loss_test: 0.08252605967857793\n",
      "4594 - Loss_train: 0.08308481933357237, Loss_test: 0.08252618616413464\n",
      "4595 - Loss_train: 0.08308478405484808, Loss_test: 0.08252631263856618\n",
      "4596 - Loss_train: 0.08308474879452953, Loss_test: 0.08252643909433047\n",
      "4597 - Loss_train: 0.08308471355259242, Loss_test: 0.08252656550993785\n",
      "4598 - Loss_train: 0.0830846783289355, Loss_test: 0.08252669190474624\n",
      "4599 - Loss_train: 0.08308464312364419, Loss_test: 0.08252681829554248\n",
      "4600 - Loss_train: 0.08308460793680655, Loss_test: 0.08252694463871395\n",
      "4601 - Loss_train: 0.08308457276823705, Loss_test: 0.08252707098039153\n",
      "4602 - Loss_train: 0.08308453761797845, Loss_test: 0.0825271972754465\n",
      "4603 - Loss_train: 0.08308450248611687, Loss_test: 0.08252732356399843\n",
      "4604 - Loss_train: 0.08308446737243917, Loss_test: 0.0825274498063936\n",
      "4605 - Loss_train: 0.08308443227705535, Loss_test: 0.08252757605394631\n",
      "4606 - Loss_train: 0.08308439720002585, Loss_test: 0.08252770225560861\n",
      "4607 - Loss_train: 0.08308436214124884, Loss_test: 0.08252782846144316\n",
      "4608 - Loss_train: 0.08308432710074423, Loss_test: 0.08252795461000914\n",
      "4609 - Loss_train: 0.08308429207848554, Loss_test: 0.08252808076512491\n",
      "4610 - Loss_train: 0.0830842570744061, Loss_test: 0.08252820686285384\n",
      "4611 - Loss_train: 0.08308422208860809, Loss_test: 0.08252833297060098\n",
      "4612 - Loss_train: 0.08308418712107435, Loss_test: 0.08252845903135145\n",
      "4613 - Loss_train: 0.08308415217168558, Loss_test: 0.08252858508423752\n",
      "4614 - Loss_train: 0.08308411724061646, Loss_test: 0.08252871110055246\n",
      "4615 - Loss_train: 0.08308408232762907, Loss_test: 0.08252883710415425\n",
      "4616 - Loss_train: 0.0830840474328346, Loss_test: 0.08252896307554647\n",
      "4617 - Loss_train: 0.08308401255624707, Loss_test: 0.08252908901931633\n",
      "4618 - Loss_train: 0.08308397769792485, Loss_test: 0.08252921496080119\n",
      "4619 - Loss_train: 0.08308394285771532, Loss_test: 0.08252934085708198\n",
      "4620 - Loss_train: 0.08308390803567223, Loss_test: 0.08252946674004419\n",
      "4621 - Loss_train: 0.08308387323176064, Loss_test: 0.08252959259220116\n",
      "4622 - Loss_train: 0.08308383844592279, Loss_test: 0.0825297184230333\n",
      "4623 - Loss_train: 0.08308380367819079, Loss_test: 0.08252984423033576\n",
      "4624 - Loss_train: 0.08308376892861552, Loss_test: 0.08252997002324441\n",
      "4625 - Loss_train: 0.08308373419709379, Loss_test: 0.08253009578042096\n",
      "4626 - Loss_train: 0.08308369948369611, Loss_test: 0.08253022152358008\n",
      "4627 - Loss_train: 0.08308366478838994, Loss_test: 0.08253034723162463\n",
      "4628 - Loss_train: 0.08308363011121539, Loss_test: 0.08253047292883398\n",
      "4629 - Loss_train: 0.08308359545198814, Loss_test: 0.08253059858974239\n",
      "4630 - Loss_train: 0.08308356081095988, Loss_test: 0.08253072423668029\n",
      "4631 - Loss_train: 0.08308352618783302, Loss_test: 0.08253084985694624\n",
      "4632 - Loss_train: 0.08308349158278194, Loss_test: 0.08253097544366061\n",
      "4633 - Loss_train: 0.08308345699576355, Loss_test: 0.08253110102500491\n",
      "4634 - Loss_train: 0.08308342242677706, Loss_test: 0.08253122657510094\n",
      "4635 - Loss_train: 0.08308338787587309, Loss_test: 0.08253135209929717\n",
      "4636 - Loss_train: 0.08308335334288343, Loss_test: 0.0825314775990781\n",
      "4637 - Loss_train: 0.08308331882798446, Loss_test: 0.08253160307222357\n",
      "4638 - Loss_train: 0.08308328433097202, Loss_test: 0.08253172853877867\n",
      "4639 - Loss_train: 0.08308324985203967, Loss_test: 0.08253185396264899\n",
      "4640 - Loss_train: 0.08308321539091755, Loss_test: 0.08253197936522053\n",
      "4641 - Loss_train: 0.08308318094773985, Loss_test: 0.0825321047478212\n",
      "4642 - Loss_train: 0.0830831465225487, Loss_test: 0.08253223012120976\n",
      "4643 - Loss_train: 0.08308311211529369, Loss_test: 0.08253235545046653\n",
      "4644 - Loss_train: 0.08308307772603794, Loss_test: 0.08253248077464136\n",
      "4645 - Loss_train: 0.08308304335464171, Loss_test: 0.0825326060394935\n",
      "4646 - Loss_train: 0.08308300900115376, Loss_test: 0.08253273132147078\n",
      "4647 - Loss_train: 0.08308297466552757, Loss_test: 0.08253285657383896\n",
      "4648 - Loss_train: 0.08308294034787858, Loss_test: 0.0825329817761285\n",
      "4649 - Loss_train: 0.08308290604803453, Loss_test: 0.08253310698255686\n",
      "4650 - Loss_train: 0.0830828717660524, Loss_test: 0.08253323214651394\n",
      "4651 - Loss_train: 0.08308283750202226, Loss_test: 0.08253335729576866\n",
      "4652 - Loss_train: 0.0830828032557517, Loss_test: 0.08253348241024855\n",
      "4653 - Loss_train: 0.08308276902734245, Loss_test: 0.08253360752260726\n",
      "4654 - Loss_train: 0.08308273481672906, Loss_test: 0.08253373259591883\n",
      "4655 - Loss_train: 0.08308270062396378, Loss_test: 0.08253385764482035\n",
      "4656 - Loss_train: 0.08308266644899436, Loss_test: 0.08253398267859835\n",
      "4657 - Loss_train: 0.08308263229181609, Loss_test: 0.08253410769683817\n",
      "4658 - Loss_train: 0.08308259815256024, Loss_test: 0.08253423267200212\n",
      "4659 - Loss_train: 0.08308256403097623, Loss_test: 0.08253435761948882\n",
      "4660 - Loss_train: 0.08308252992716199, Loss_test: 0.08253448255878942\n",
      "4661 - Loss_train: 0.08308249584109603, Loss_test: 0.08253460748062272\n",
      "4662 - Loss_train: 0.08308246177280594, Loss_test: 0.08253473235218683\n",
      "4663 - Loss_train: 0.08308242772228373, Loss_test: 0.082534857237003\n",
      "4664 - Loss_train: 0.08308239368950616, Loss_test: 0.08253498207415844\n",
      "4665 - Loss_train: 0.08308235967441667, Loss_test: 0.08253510688038071\n",
      "4666 - Loss_train: 0.08308232567707528, Loss_test: 0.0825352316888856\n",
      "4667 - Loss_train: 0.08308229169746657, Loss_test: 0.08253535644657034\n",
      "4668 - Loss_train: 0.08308225773561202, Loss_test: 0.08253548119920837\n",
      "4669 - Loss_train: 0.0830822237913793, Loss_test: 0.08253560592570514\n",
      "4670 - Loss_train: 0.08308218986487138, Loss_test: 0.08253573062315196\n",
      "4671 - Loss_train: 0.08308215595599723, Loss_test: 0.08253585529618965\n",
      "4672 - Loss_train: 0.08308212206487496, Loss_test: 0.08253597995122146\n",
      "4673 - Loss_train: 0.08308208819131692, Loss_test: 0.08253610458078631\n",
      "4674 - Loss_train: 0.08308205433537534, Loss_test: 0.08253622918315356\n",
      "4675 - Loss_train: 0.08308202049705452, Loss_test: 0.08253635375320657\n",
      "4676 - Loss_train: 0.08308198667640633, Loss_test: 0.0825364783264708\n",
      "4677 - Loss_train: 0.08308195287345396, Loss_test: 0.08253660284367761\n",
      "4678 - Loss_train: 0.08308191908803203, Loss_test: 0.08253672736848322\n",
      "4679 - Loss_train: 0.08308188532025203, Loss_test: 0.08253685185246981\n",
      "4680 - Loss_train: 0.0830818515701116, Loss_test: 0.08253697632023055\n",
      "4681 - Loss_train: 0.08308181783740835, Loss_test: 0.08253710075382395\n",
      "4682 - Loss_train: 0.08308178412228677, Loss_test: 0.08253722517710001\n",
      "4683 - Loss_train: 0.08308175042476777, Loss_test: 0.08253734956540022\n",
      "4684 - Loss_train: 0.08308171674482617, Loss_test: 0.08253747393653887\n",
      "4685 - Loss_train: 0.08308168308252803, Loss_test: 0.08253759829749419\n",
      "4686 - Loss_train: 0.08308164943766466, Loss_test: 0.08253772261340324\n",
      "4687 - Loss_train: 0.08308161581033327, Loss_test: 0.08253784690959359\n",
      "4688 - Loss_train: 0.08308158220046885, Loss_test: 0.08253797118826682\n",
      "4689 - Loss_train: 0.08308154860815689, Loss_test: 0.08253809543942946\n",
      "4690 - Loss_train: 0.0830815150334335, Loss_test: 0.08253821966005917\n",
      "4691 - Loss_train: 0.08308148147608758, Loss_test: 0.08253834385912397\n",
      "4692 - Loss_train: 0.08308144793622321, Loss_test: 0.08253846803735002\n",
      "4693 - Loss_train: 0.08308141441386595, Loss_test: 0.08253859221471956\n",
      "4694 - Loss_train: 0.08308138090893777, Loss_test: 0.0825387163356254\n",
      "4695 - Loss_train: 0.08308134742147738, Loss_test: 0.08253884045512745\n",
      "4696 - Loss_train: 0.08308131395149425, Loss_test: 0.08253896455351754\n",
      "4697 - Loss_train: 0.08308128049894696, Loss_test: 0.08253908861024564\n",
      "4698 - Loss_train: 0.0830812470637745, Loss_test: 0.08253921264649092\n",
      "4699 - Loss_train: 0.08308121364605031, Loss_test: 0.08253933665710173\n",
      "4700 - Loss_train: 0.08308118024575821, Loss_test: 0.08253946065567774\n",
      "4701 - Loss_train: 0.08308114686284279, Loss_test: 0.08253958463364444\n",
      "4702 - Loss_train: 0.08308111349731062, Loss_test: 0.08253970856404967\n",
      "4703 - Loss_train: 0.08308108014916914, Loss_test: 0.08253983249807978\n",
      "4704 - Loss_train: 0.08308104681838753, Loss_test: 0.08253995639817056\n",
      "4705 - Loss_train: 0.0830810135049861, Loss_test: 0.08254008026817687\n",
      "4706 - Loss_train: 0.08308098020894976, Loss_test: 0.08254020412213794\n",
      "4707 - Loss_train: 0.08308094693023636, Loss_test: 0.08254032794433248\n",
      "4708 - Loss_train: 0.08308091366880521, Loss_test: 0.0825404517559949\n",
      "4709 - Loss_train: 0.083080880424758, Loss_test: 0.082540575525168\n",
      "4710 - Loss_train: 0.08308084719805309, Loss_test: 0.08254069929047493\n",
      "4711 - Loss_train: 0.08308081398863797, Loss_test: 0.08254082302802282\n",
      "4712 - Loss_train: 0.08308078079650881, Loss_test: 0.08254094673563703\n",
      "4713 - Loss_train: 0.08308074762177987, Loss_test: 0.08254107043156397\n",
      "4714 - Loss_train: 0.0830807144642301, Loss_test: 0.08254119408624015\n",
      "4715 - Loss_train: 0.08308068132405896, Loss_test: 0.08254131773727594\n",
      "4716 - Loss_train: 0.08308064820113369, Loss_test: 0.08254144134715759\n",
      "4717 - Loss_train: 0.08308061509537845, Loss_test: 0.08254156493834149\n",
      "4718 - Loss_train: 0.08308058200684033, Loss_test: 0.08254168851689396\n",
      "4719 - Loss_train: 0.08308054893567142, Loss_test: 0.08254181205425994\n",
      "4720 - Loss_train: 0.0830805158816468, Loss_test: 0.0825419355834092\n",
      "4721 - Loss_train: 0.08308048284484408, Loss_test: 0.08254205908374912\n",
      "4722 - Loss_train: 0.0830804498252699, Loss_test: 0.0825421825546737\n",
      "4723 - Loss_train: 0.08308041682296426, Loss_test: 0.08254230601203394\n",
      "4724 - Loss_train: 0.0830803838377409, Loss_test: 0.08254242943082261\n",
      "4725 - Loss_train: 0.08308035086981536, Loss_test: 0.08254255285314097\n",
      "4726 - Loss_train: 0.08308031791898422, Loss_test: 0.08254267621212757\n",
      "4727 - Loss_train: 0.08308028498525444, Loss_test: 0.08254279958251258\n",
      "4728 - Loss_train: 0.08308025206883833, Loss_test: 0.0825429229218045\n",
      "4729 - Loss_train: 0.08308021916960504, Loss_test: 0.08254304621919731\n",
      "4730 - Loss_train: 0.08308018628734103, Loss_test: 0.08254316952814132\n",
      "4731 - Loss_train: 0.08308015342222605, Loss_test: 0.08254329278115517\n",
      "4732 - Loss_train: 0.08308012057428446, Loss_test: 0.08254341602228915\n",
      "4733 - Loss_train: 0.08308008774341807, Loss_test: 0.08254353925208409\n",
      "4734 - Loss_train: 0.08308005492965072, Loss_test: 0.08254366243521889\n",
      "4735 - Loss_train: 0.08308002213295324, Loss_test: 0.08254378560968176\n",
      "4736 - Loss_train: 0.08307998935345576, Loss_test: 0.08254390876314266\n",
      "4737 - Loss_train: 0.08307995659094868, Loss_test: 0.08254403186871921\n",
      "4738 - Loss_train: 0.08307992384551224, Loss_test: 0.08254415497728307\n",
      "4739 - Loss_train: 0.08307989111709688, Loss_test: 0.08254427805420395\n",
      "4740 - Loss_train: 0.08307985840586457, Loss_test: 0.08254440111854576\n",
      "4741 - Loss_train: 0.08307982571165366, Loss_test: 0.08254452414088778\n",
      "4742 - Loss_train: 0.08307979303450057, Loss_test: 0.0825446471451987\n",
      "4743 - Loss_train: 0.08307976037423127, Loss_test: 0.08254477012736698\n",
      "4744 - Loss_train: 0.0830797277311241, Loss_test: 0.08254489309139759\n",
      "4745 - Loss_train: 0.08307969510497673, Loss_test: 0.08254501601738408\n",
      "4746 - Loss_train: 0.08307966249577174, Loss_test: 0.08254513893301647\n",
      "4747 - Loss_train: 0.08307962990367684, Loss_test: 0.08254526183021144\n",
      "4748 - Loss_train: 0.08307959732844013, Loss_test: 0.08254538469018481\n",
      "4749 - Loss_train: 0.0830795647701388, Loss_test: 0.08254550752789042\n",
      "4750 - Loss_train: 0.0830795322288183, Loss_test: 0.0825456303536026\n",
      "4751 - Loss_train: 0.08307949970453055, Loss_test: 0.08254575315702463\n",
      "4752 - Loss_train: 0.08307946719712853, Loss_test: 0.08254587592212463\n",
      "4753 - Loss_train: 0.08307943470673154, Loss_test: 0.08254599866787156\n",
      "4754 - Loss_train: 0.08307940223312506, Loss_test: 0.0825461213955772\n",
      "4755 - Loss_train: 0.08307936977649823, Loss_test: 0.08254624409875788\n",
      "4756 - Loss_train: 0.0830793373367199, Loss_test: 0.08254636676921358\n",
      "4757 - Loss_train: 0.08307930491395168, Loss_test: 0.08254648943011084\n",
      "4758 - Loss_train: 0.08307927250801087, Loss_test: 0.08254661205450357\n",
      "4759 - Loss_train: 0.08307924011892981, Loss_test: 0.08254673467438474\n",
      "4760 - Loss_train: 0.08307920774674733, Loss_test: 0.08254685725038183\n",
      "4761 - Loss_train: 0.08307917539143674, Loss_test: 0.08254697981731367\n",
      "4762 - Loss_train: 0.08307914305297816, Loss_test: 0.08254710236238758\n",
      "4763 - Loss_train: 0.08307911073143195, Loss_test: 0.08254722485837424\n",
      "4764 - Loss_train: 0.08307907842671079, Loss_test: 0.08254734735659815\n",
      "4765 - Loss_train: 0.08307904613885098, Loss_test: 0.08254746982431986\n",
      "4766 - Loss_train: 0.08307901386768977, Loss_test: 0.08254759226697886\n",
      "4767 - Loss_train: 0.08307898161340548, Loss_test: 0.08254771469192423\n",
      "4768 - Loss_train: 0.08307894937592289, Loss_test: 0.08254783709020273\n",
      "4769 - Loss_train: 0.0830789171552226, Loss_test: 0.08254795945814879\n",
      "4770 - Loss_train: 0.08307888495128893, Loss_test: 0.08254808180008238\n",
      "4771 - Loss_train: 0.08307885276413583, Loss_test: 0.08254820414405747\n",
      "4772 - Loss_train: 0.08307882059381522, Loss_test: 0.08254832643568796\n",
      "4773 - Loss_train: 0.08307878844025575, Loss_test: 0.08254844871948096\n",
      "4774 - Loss_train: 0.0830787563033969, Loss_test: 0.08254857096767575\n",
      "4775 - Loss_train: 0.08307872418325873, Loss_test: 0.08254869321026899\n",
      "4776 - Loss_train: 0.0830786920799924, Loss_test: 0.08254881542284054\n",
      "4777 - Loss_train: 0.08307865999330723, Loss_test: 0.082548937606645\n",
      "4778 - Loss_train: 0.08307862792335949, Loss_test: 0.08254905975741156\n",
      "4779 - Loss_train: 0.08307859587015975, Loss_test: 0.08254918192124111\n",
      "4780 - Loss_train: 0.08307856383354023, Loss_test: 0.08254930400690241\n",
      "4781 - Loss_train: 0.08307853181373498, Loss_test: 0.08254942613018139\n",
      "4782 - Loss_train: 0.08307849981053297, Loss_test: 0.08254954818930382\n",
      "4783 - Loss_train: 0.0830784678241142, Loss_test: 0.08254967023480869\n",
      "4784 - Loss_train: 0.0830784358542671, Loss_test: 0.08254979224786642\n",
      "4785 - Loss_train: 0.0830784039010624, Loss_test: 0.0825499142554618\n",
      "4786 - Loss_train: 0.08307837196455789, Loss_test: 0.08255003622758622\n",
      "4787 - Loss_train: 0.08307834004467005, Loss_test: 0.08255015819665314\n",
      "4788 - Loss_train: 0.08307830814139788, Loss_test: 0.08255028011201862\n",
      "4789 - Loss_train: 0.08307827625479783, Loss_test: 0.082550402021946\n",
      "4790 - Loss_train: 0.08307824438474552, Loss_test: 0.08255052390062191\n",
      "4791 - Loss_train: 0.08307821253134562, Loss_test: 0.0825506457696537\n",
      "4792 - Loss_train: 0.08307818069448851, Loss_test: 0.08255076758439793\n",
      "4793 - Loss_train: 0.08307814887423651, Loss_test: 0.08255088942371963\n",
      "4794 - Loss_train: 0.08307811707053607, Loss_test: 0.08255101119140926\n",
      "4795 - Loss_train: 0.08307808528345206, Loss_test: 0.08255113296842531\n",
      "4796 - Loss_train: 0.08307805351287559, Loss_test: 0.08255125470376236\n",
      "4797 - Loss_train: 0.08307802175889137, Loss_test: 0.08255137642562921\n",
      "4798 - Loss_train: 0.08307799002142888, Loss_test: 0.08255149812057488\n",
      "4799 - Loss_train: 0.08307795830052594, Loss_test: 0.08255161980280215\n",
      "4800 - Loss_train: 0.08307792659610262, Loss_test: 0.08255174143339479\n",
      "4801 - Loss_train: 0.0830778949082566, Loss_test: 0.08255186308103754\n",
      "4802 - Loss_train: 0.08307786323698983, Loss_test: 0.08255198466878083\n",
      "4803 - Loss_train: 0.08307783158221438, Loss_test: 0.08255210625091505\n",
      "4804 - Loss_train: 0.08307779994385418, Loss_test: 0.08255222779399451\n",
      "4805 - Loss_train: 0.0830777683219674, Loss_test: 0.08255234934384395\n",
      "4806 - Loss_train: 0.08307773671653083, Loss_test: 0.0825524708390549\n",
      "4807 - Loss_train: 0.08307770512759392, Loss_test: 0.08255259232404631\n",
      "4808 - Loss_train: 0.08307767355517194, Loss_test: 0.08255271379560983\n",
      "4809 - Loss_train: 0.08307764199911177, Loss_test: 0.08255283523025422\n",
      "4810 - Loss_train: 0.08307761045950536, Loss_test: 0.08255295664531279\n",
      "4811 - Loss_train: 0.08307757893632857, Loss_test: 0.08255307803499697\n",
      "4812 - Loss_train: 0.08307754742956071, Loss_test: 0.0825531994079056\n",
      "4813 - Loss_train: 0.08307751593921527, Loss_test: 0.0825533207467838\n",
      "4814 - Loss_train: 0.08307748446537298, Loss_test: 0.0825534420748968\n",
      "4815 - Loss_train: 0.08307745300783176, Loss_test: 0.08255356337176054\n",
      "4816 - Loss_train: 0.08307742156666846, Loss_test: 0.08255368464414253\n",
      "4817 - Loss_train: 0.08307739014188921, Loss_test: 0.08255380589097787\n",
      "4818 - Loss_train: 0.0830773587334921, Loss_test: 0.08255392711724864\n",
      "4819 - Loss_train: 0.08307732734152043, Loss_test: 0.08255404834348772\n",
      "4820 - Loss_train: 0.0830772959658372, Loss_test: 0.08255416950599793\n",
      "4821 - Loss_train: 0.0830772646064561, Loss_test: 0.08255429067665324\n",
      "4822 - Loss_train: 0.08307723326343583, Loss_test: 0.0825544118051092\n",
      "4823 - Loss_train: 0.0830772019367447, Loss_test: 0.08255453291905\n",
      "4824 - Loss_train: 0.08307717062640455, Loss_test: 0.08255465401021236\n",
      "4825 - Loss_train: 0.08307713933235646, Loss_test: 0.08255477506463256\n",
      "4826 - Loss_train: 0.08307710805458177, Loss_test: 0.08255489611983696\n",
      "4827 - Loss_train: 0.08307707679313443, Loss_test: 0.08255501712893157\n",
      "4828 - Loss_train: 0.0830770455479725, Loss_test: 0.0825551381315065\n",
      "4829 - Loss_train: 0.08307701431909008, Loss_test: 0.0825552591088503\n",
      "4830 - Loss_train: 0.08307698310658634, Loss_test: 0.08255538004846881\n",
      "4831 - Loss_train: 0.08307695191023785, Loss_test: 0.08255550097062478\n",
      "4832 - Loss_train: 0.08307692073014852, Loss_test: 0.08255562188226062\n",
      "4833 - Loss_train: 0.08307688956631164, Loss_test: 0.08255574275423362\n",
      "4834 - Loss_train: 0.08307685841868011, Loss_test: 0.08255586362276741\n",
      "4835 - Loss_train: 0.08307682728725684, Loss_test: 0.08255598444809564\n",
      "4836 - Loss_train: 0.08307679617209385, Loss_test: 0.08255610525272913\n",
      "4837 - Loss_train: 0.08307676507313115, Loss_test: 0.0825562260337586\n",
      "4838 - Loss_train: 0.08307673399034174, Loss_test: 0.08255634681133613\n",
      "4839 - Loss_train: 0.08307670292375556, Loss_test: 0.08255646753498505\n",
      "4840 - Loss_train: 0.0830766718733919, Loss_test: 0.08255658826206105\n",
      "4841 - Loss_train: 0.08307664083921339, Loss_test: 0.08255670895946271\n",
      "4842 - Loss_train: 0.08307660982128061, Loss_test: 0.08255682961832543\n",
      "4843 - Loss_train: 0.08307657881938675, Loss_test: 0.08255695026253689\n",
      "4844 - Loss_train: 0.08307654783359883, Loss_test: 0.08255707087523678\n",
      "4845 - Loss_train: 0.08307651686403335, Loss_test: 0.08255719149744652\n",
      "4846 - Loss_train: 0.08307648591058883, Loss_test: 0.0825573120545745\n",
      "4847 - Loss_train: 0.08307645497328656, Loss_test: 0.08255743262305959\n",
      "4848 - Loss_train: 0.08307642405214899, Loss_test: 0.08255755313831273\n",
      "4849 - Loss_train: 0.08307639314705709, Loss_test: 0.08255767365765737\n",
      "4850 - Loss_train: 0.08307636225806093, Loss_test: 0.08255779412971308\n",
      "4851 - Loss_train: 0.08307633138525944, Loss_test: 0.08255791459738178\n",
      "4852 - Loss_train: 0.08307630052851228, Loss_test: 0.08255803503072119\n",
      "4853 - Loss_train: 0.08307626968776509, Loss_test: 0.08255815544500393\n",
      "4854 - Loss_train: 0.08307623886316398, Loss_test: 0.0825582758235028\n",
      "4855 - Loss_train: 0.08307620805462491, Loss_test: 0.08255839620291133\n",
      "4856 - Loss_train: 0.08307617726212077, Loss_test: 0.08255851654431517\n",
      "4857 - Loss_train: 0.08307614648558193, Loss_test: 0.08255863686081351\n",
      "4858 - Loss_train: 0.0830761157250798, Loss_test: 0.08255875716232287\n",
      "4859 - Loss_train: 0.08307608498057313, Loss_test: 0.08255887743487333\n",
      "4860 - Loss_train: 0.08307605425219626, Loss_test: 0.08255899767618943\n",
      "4861 - Loss_train: 0.08307602353976715, Loss_test: 0.08255911791562895\n",
      "4862 - Loss_train: 0.08307599284333121, Loss_test: 0.0825592381153857\n",
      "4863 - Loss_train: 0.08307596216297734, Loss_test: 0.08255935829080287\n",
      "4864 - Loss_train: 0.08307593149858587, Loss_test: 0.08255947845347811\n",
      "4865 - Loss_train: 0.083075900850155, Loss_test: 0.08255959858138497\n",
      "4866 - Loss_train: 0.08307587021762787, Loss_test: 0.08255971869929367\n",
      "4867 - Loss_train: 0.08307583960106506, Loss_test: 0.08255983877667791\n",
      "4868 - Loss_train: 0.08307580900054409, Loss_test: 0.08255995885185938\n",
      "4869 - Loss_train: 0.08307577841589465, Loss_test: 0.08256007889031267\n",
      "4870 - Loss_train: 0.08307574784726081, Loss_test: 0.08256019891432115\n",
      "4871 - Loss_train: 0.08307571729443641, Loss_test: 0.0825603188956526\n",
      "4872 - Loss_train: 0.08307568675750863, Loss_test: 0.08256043887092111\n",
      "4873 - Loss_train: 0.08307565623658936, Loss_test: 0.08256055881881684\n",
      "4874 - Loss_train: 0.08307562573151944, Loss_test: 0.08256067874349966\n",
      "4875 - Loss_train: 0.08307559524237988, Loss_test: 0.08256079865236128\n",
      "4876 - Loss_train: 0.08307556476905936, Loss_test: 0.08256091851867463\n",
      "4877 - Loss_train: 0.08307553431162391, Loss_test: 0.08256103837728598\n",
      "4878 - Loss_train: 0.08307550387003756, Loss_test: 0.08256115820819321\n",
      "4879 - Loss_train: 0.08307547344438466, Loss_test: 0.08256127803793178\n",
      "4880 - Loss_train: 0.08307544303456127, Loss_test: 0.08256139780295774\n",
      "4881 - Loss_train: 0.08307541264054925, Loss_test: 0.08256151757404001\n",
      "4882 - Loss_train: 0.08307538226236909, Loss_test: 0.08256163730863555\n",
      "4883 - Loss_train: 0.08307535190011679, Loss_test: 0.08256175702771637\n",
      "4884 - Loss_train: 0.08307532155357593, Loss_test: 0.082561876715561\n",
      "4885 - Loss_train: 0.08307529122288149, Loss_test: 0.0825619963956187\n",
      "4886 - Loss_train: 0.08307526090799966, Loss_test: 0.08256211602821234\n",
      "4887 - Loss_train: 0.0830752306088627, Loss_test: 0.08256223565596157\n",
      "4888 - Loss_train: 0.0830752003255369, Loss_test: 0.0825623552551791\n",
      "4889 - Loss_train: 0.08307517005806332, Loss_test: 0.08256247483847114\n",
      "4890 - Loss_train: 0.08307513980636415, Loss_test: 0.08256259439468573\n",
      "4891 - Loss_train: 0.08307510957032499, Loss_test: 0.08256271391979297\n",
      "4892 - Loss_train: 0.0830750793500441, Loss_test: 0.0825628334300144\n",
      "4893 - Loss_train: 0.08307504914550866, Loss_test: 0.08256295290738556\n",
      "4894 - Loss_train: 0.08307501895669957, Loss_test: 0.0825630723698545\n",
      "4895 - Loss_train: 0.08307498878371322, Loss_test: 0.0825631918131217\n",
      "4896 - Loss_train: 0.08307495862634992, Loss_test: 0.0825633112288129\n",
      "4897 - Loss_train: 0.08307492848468569, Loss_test: 0.0825634306124628\n",
      "4898 - Loss_train: 0.08307489835873515, Loss_test: 0.08256354998061644\n",
      "4899 - Loss_train: 0.08307486824849017, Loss_test: 0.08256366932159045\n",
      "4900 - Loss_train: 0.08307483815400468, Loss_test: 0.08256378865950369\n",
      "4901 - Loss_train: 0.08307480807509428, Loss_test: 0.08256390794422529\n",
      "4902 - Loss_train: 0.0830747780118852, Loss_test: 0.08256402723112646\n",
      "4903 - Loss_train: 0.08307474796435865, Loss_test: 0.08256414646982448\n",
      "4904 - Loss_train: 0.08307471793254244, Loss_test: 0.08256426571150191\n",
      "4905 - Loss_train: 0.0830746879163015, Loss_test: 0.08256438489898646\n",
      "4906 - Loss_train: 0.08307465791567181, Loss_test: 0.08256450408983756\n",
      "4907 - Loss_train: 0.08307462793070046, Loss_test: 0.08256462325655273\n",
      "4908 - Loss_train: 0.08307459796142067, Loss_test: 0.08256474238911583\n",
      "4909 - Loss_train: 0.08307456800771562, Loss_test: 0.0825648615173921\n",
      "4910 - Loss_train: 0.0830745380695792, Loss_test: 0.08256498057842399\n",
      "4911 - Loss_train: 0.08307450814701849, Loss_test: 0.0825650996610992\n",
      "4912 - Loss_train: 0.08307447824015014, Loss_test: 0.08256521870644126\n",
      "4913 - Loss_train: 0.08307444834886686, Loss_test: 0.08256533772399599\n",
      "4914 - Loss_train: 0.08307441847315647, Loss_test: 0.08256545672225517\n",
      "4915 - Loss_train: 0.08307438861294937, Loss_test: 0.08256557570651556\n",
      "4916 - Loss_train: 0.08307435876827453, Loss_test: 0.08256569464109223\n",
      "4917 - Loss_train: 0.08307432893912794, Loss_test: 0.08256581357168485\n",
      "4918 - Loss_train: 0.08307429912553453, Loss_test: 0.08256593247798827\n",
      "4919 - Loss_train: 0.08307426932748213, Loss_test: 0.08256605135610748\n",
      "4920 - Loss_train: 0.08307423954496498, Loss_test: 0.08256617023005408\n",
      "4921 - Loss_train: 0.08307420977793988, Loss_test: 0.08256628904863153\n",
      "4922 - Loss_train: 0.0830741800265035, Loss_test: 0.08256640787366447\n",
      "4923 - Loss_train: 0.08307415029057877, Loss_test: 0.0825665266477503\n",
      "4924 - Loss_train: 0.08307412057006126, Loss_test: 0.08256664542211889\n",
      "4925 - Loss_train: 0.08307409086501132, Loss_test: 0.08256676415571608\n",
      "4926 - Loss_train: 0.08307406117545116, Loss_test: 0.08256688288748355\n",
      "4927 - Loss_train: 0.08307403150135262, Loss_test: 0.0825670015725626\n",
      "4928 - Loss_train: 0.08307400184272863, Loss_test: 0.0825671202559933\n",
      "4929 - Loss_train: 0.0830739721995472, Loss_test: 0.08256723889799679\n",
      "4930 - Loss_train: 0.0830739425719047, Loss_test: 0.08256735753384019\n",
      "4931 - Loss_train: 0.08307391295971142, Loss_test: 0.08256747614296327\n",
      "4932 - Loss_train: 0.08307388336284935, Loss_test: 0.08256759471630414\n",
      "4933 - Loss_train: 0.08307385378146774, Loss_test: 0.08256771327378795\n",
      "4934 - Loss_train: 0.08307382421542532, Loss_test: 0.08256783179881239\n",
      "4935 - Loss_train: 0.08307379466476317, Loss_test: 0.08256795032560171\n",
      "4936 - Loss_train: 0.0830737651296045, Loss_test: 0.08256806880342964\n",
      "4937 - Loss_train: 0.08307373560972853, Loss_test: 0.08256818728005313\n",
      "4938 - Loss_train: 0.08307370610533739, Loss_test: 0.08256830572126256\n",
      "4939 - Loss_train: 0.08307367661620566, Loss_test: 0.08256842412407804\n",
      "4940 - Loss_train: 0.0830736471424761, Loss_test: 0.08256854254083879\n",
      "4941 - Loss_train: 0.08307361768409403, Loss_test: 0.08256866090594947\n",
      "4942 - Loss_train: 0.08307358824117347, Loss_test: 0.0825687792650542\n",
      "4943 - Loss_train: 0.08307355881343356, Loss_test: 0.08256889757710281\n",
      "4944 - Loss_train: 0.08307352940101326, Loss_test: 0.08256901588574567\n",
      "4945 - Loss_train: 0.08307350000404741, Loss_test: 0.08256913416397832\n",
      "4946 - Loss_train: 0.08307347062234381, Loss_test: 0.08256925242365662\n",
      "4947 - Loss_train: 0.08307344125598916, Loss_test: 0.08256937065693125\n",
      "4948 - Loss_train: 0.0830734119049455, Loss_test: 0.08256948887598087\n",
      "4949 - Loss_train: 0.08307338256915489, Loss_test: 0.0825696070635141\n",
      "4950 - Loss_train: 0.08307335324870668, Loss_test: 0.08256972522408079\n",
      "4951 - Loss_train: 0.08307332394350095, Loss_test: 0.08256984335813938\n",
      "4952 - Loss_train: 0.08307329465348703, Loss_test: 0.08256996147932957\n",
      "4953 - Loss_train: 0.08307326537874167, Loss_test: 0.08257007957691258\n",
      "4954 - Loss_train: 0.08307323611926834, Loss_test: 0.08257019765418282\n",
      "4955 - Loss_train: 0.0830732068750335, Loss_test: 0.08257031570614272\n",
      "4956 - Loss_train: 0.08307317764603928, Loss_test: 0.08257043373343537\n",
      "4957 - Loss_train: 0.08307314843222527, Loss_test: 0.0825705517317819\n",
      "4958 - Loss_train: 0.08307311923368176, Loss_test: 0.0825706697242713\n",
      "4959 - Loss_train: 0.08307309005040736, Loss_test: 0.08257078767330789\n",
      "4960 - Loss_train: 0.08307306088222588, Loss_test: 0.08257090561550945\n",
      "4961 - Loss_train: 0.08307303172937437, Loss_test: 0.08257102352091315\n",
      "4962 - Loss_train: 0.08307300259161407, Loss_test: 0.08257114140233142\n",
      "4963 - Loss_train: 0.08307297346904309, Loss_test: 0.08257125928067309\n",
      "4964 - Loss_train: 0.08307294436162421, Loss_test: 0.08257137710989335\n",
      "4965 - Loss_train: 0.0830729152693673, Loss_test: 0.08257149493900462\n",
      "4966 - Loss_train: 0.08307288619227177, Loss_test: 0.08257161272753084\n",
      "4967 - Loss_train: 0.08307285713041272, Loss_test: 0.08257173050256578\n",
      "4968 - Loss_train: 0.08307282808364118, Loss_test: 0.08257184826305972\n",
      "4969 - Loss_train: 0.08307279905194812, Loss_test: 0.08257196597624794\n",
      "4970 - Loss_train: 0.08307277003540518, Loss_test: 0.08257208368161253\n",
      "4971 - Loss_train: 0.0830727410339552, Loss_test: 0.08257220136251547\n",
      "4972 - Loss_train: 0.08307271204764717, Loss_test: 0.0825723190313562\n",
      "4973 - Loss_train: 0.08307268307641437, Loss_test: 0.08257243666057387\n",
      "4974 - Loss_train: 0.08307265412031141, Loss_test: 0.08257255428064253\n",
      "4975 - Loss_train: 0.08307262517936495, Loss_test: 0.08257267185154149\n",
      "4976 - Loss_train: 0.08307259625341015, Loss_test: 0.08257278942754648\n",
      "4977 - Loss_train: 0.08307256734255093, Loss_test: 0.08257290696663049\n",
      "4978 - Loss_train: 0.083072538446713, Loss_test: 0.0825730244859163\n",
      "4979 - Loss_train: 0.0830725095659433, Loss_test: 0.08257314197683108\n",
      "4980 - Loss_train: 0.08307248070024223, Loss_test: 0.0825732594618992\n",
      "4981 - Loss_train: 0.08307245184954543, Loss_test: 0.0825733769071845\n",
      "4982 - Loss_train: 0.08307242301397073, Loss_test: 0.08257349433980178\n",
      "4983 - Loss_train: 0.08307239419333481, Loss_test: 0.08257361173787507\n",
      "4984 - Loss_train: 0.08307236538772013, Loss_test: 0.0825737291281758\n",
      "4985 - Loss_train: 0.08307233659711168, Loss_test: 0.08257384647804239\n",
      "4986 - Loss_train: 0.08307230782153106, Loss_test: 0.08257396382302112\n",
      "4987 - Loss_train: 0.08307227906095985, Loss_test: 0.08257408113986373\n",
      "4988 - Loss_train: 0.08307225031536049, Loss_test: 0.08257419841542245\n",
      "4989 - Loss_train: 0.08307222158473829, Loss_test: 0.08257431569787871\n",
      "4990 - Loss_train: 0.08307219286912162, Loss_test: 0.0825744329285402\n",
      "4991 - Loss_train: 0.0830721641684178, Loss_test: 0.08257455015750548\n",
      "4992 - Loss_train: 0.08307213548264228, Loss_test: 0.08257466735660873\n",
      "4993 - Loss_train: 0.08307210681179976, Loss_test: 0.0825747845212505\n",
      "4994 - Loss_train: 0.08307207815596801, Loss_test: 0.08257490167913853\n",
      "4995 - Loss_train: 0.08307204951510351, Loss_test: 0.08257501881358204\n",
      "4996 - Loss_train: 0.08307202088908155, Loss_test: 0.08257513591467586\n",
      "4997 - Loss_train: 0.0830719922780232, Loss_test: 0.08257525298960224\n",
      "4998 - Loss_train: 0.083071963681856, Loss_test: 0.08257537005651669\n",
      "4999 - Loss_train: 0.08307193510055583, Loss_test: 0.08257548708487845\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data(5000, seed=10)\n",
    "X_test, y_test = generate_data(50, seed=11)\n",
    "W = fit(X, y,X_test, y_test, 5000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3594c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Loss_train: 0.09298705633056822, Loss_test: 0.09112561736495997\n",
      "0 - Loss_train: 0.09298189047956687, Loss_test: 0.09112087560815821\n",
      "1 - Loss_train: 0.09297672729002042, Loss_test: 0.09111613640464179\n",
      "2 - Loss_train: 0.09297156677047734, Loss_test: 0.09111139976610998\n",
      "3 - Loss_train: 0.09296640891764642, Loss_test: 0.09110666568595273\n",
      "4 - Loss_train: 0.0929612537256161, Loss_test: 0.09110193416001709\n",
      "5 - Loss_train: 0.09295610120199058, Loss_test: 0.0910972051952098\n",
      "6 - Loss_train: 0.09295095133946223, Loss_test: 0.09109247878520302\n",
      "7 - Loss_train: 0.09294580413252156, Loss_test: 0.0910877549234454\n",
      "8 - Loss_train: 0.09294065958889225, Loss_test: 0.09108303361921843\n",
      "9 - Loss_train: 0.09293551769847933, Loss_test: 0.09107831486184031\n",
      "10 - Loss_train: 0.09293037847195969, Loss_test: 0.09107359865816272\n",
      "11 - Loss_train: 0.09292524189067229, Loss_test: 0.09106888499843308\n",
      "12 - Loss_train: 0.09292010796818524, Loss_test: 0.09106417388616546\n",
      "13 - Loss_train: 0.09291497669289864, Loss_test: 0.09105946531691834\n",
      "14 - Loss_train: 0.09290984807572338, Loss_test: 0.09105475929814774\n",
      "15 - Loss_train: 0.09290472210237835, Loss_test: 0.09105005581625134\n",
      "16 - Loss_train: 0.09289959877675819, Loss_test: 0.09104535487720787\n",
      "17 - Loss_train: 0.09289447809701797, Loss_test: 0.0910406564759221\n",
      "18 - Loss_train: 0.09288936006707091, Loss_test: 0.09103596061817759\n",
      "19 - Loss_train: 0.09288424467556156, Loss_test: 0.09103126729293717\n",
      "20 - Loss_train: 0.09287913192659006, Loss_test: 0.09102657650354856\n",
      "21 - Loss_train: 0.09287402182534901, Loss_test: 0.0910218882569458\n",
      "22 - Loss_train: 0.09286891436129066, Loss_test: 0.09101720253696101\n",
      "23 - Loss_train: 0.09286380953216827, Loss_test: 0.091012519348665\n",
      "24 - Loss_train: 0.09285870734321357, Loss_test: 0.09100783869342456\n",
      "25 - Loss_train: 0.09285360778766524, Loss_test: 0.09100316056580081\n",
      "26 - Loss_train: 0.09284851086955212, Loss_test: 0.0909984849677548\n",
      "27 - Loss_train: 0.09284341658512388, Loss_test: 0.09099381189845018\n",
      "28 - Loss_train: 0.09283832493408375, Loss_test: 0.09098914135550623\n",
      "29 - Loss_train: 0.09283323591449931, Loss_test: 0.09098447333819382\n",
      "30 - Loss_train: 0.09282814952261483, Loss_test: 0.09097980784168341\n",
      "31 - Loss_train: 0.09282306575404439, Loss_test: 0.09097514486307084\n",
      "32 - Loss_train: 0.09281798461181591, Loss_test: 0.09097048440528249\n",
      "33 - Loss_train: 0.09281290609438908, Loss_test: 0.09096582646458519\n",
      "34 - Loss_train: 0.09280783020054473, Loss_test: 0.09096117104274347\n",
      "35 - Loss_train: 0.09280275693219175, Loss_test: 0.09095651813791969\n",
      "36 - Loss_train: 0.09279768628359344, Loss_test: 0.09095186775026616\n",
      "37 - Loss_train: 0.09279261825244603, Loss_test: 0.09094721987158982\n",
      "38 - Loss_train: 0.09278755284020454, Loss_test: 0.09094257450683195\n",
      "39 - Loss_train: 0.09278249004906967, Loss_test: 0.0909379316568349\n",
      "40 - Loss_train: 0.09277742986896888, Loss_test: 0.09093329131080385\n",
      "41 - Loss_train: 0.09277237230705514, Loss_test: 0.0909286534789848\n",
      "42 - Loss_train: 0.09276731735326403, Loss_test: 0.09092401814812155\n",
      "43 - Loss_train: 0.09276226501590887, Loss_test: 0.09091938532904184\n",
      "44 - Loss_train: 0.09275721528463038, Loss_test: 0.09091475500845654\n",
      "45 - Loss_train: 0.09275216816113561, Loss_test: 0.09091012719070038\n",
      "46 - Loss_train: 0.09274712364546314, Loss_test: 0.09090550187528187\n",
      "47 - Loss_train: 0.09274208173606201, Loss_test: 0.09090087905987482\n",
      "48 - Loss_train: 0.09273704243125155, Loss_test: 0.09089625874404231\n",
      "49 - Loss_train: 0.09273200572950865, Loss_test: 0.09089164092534745\n",
      "50 - Loss_train: 0.09272697163470017, Loss_test: 0.0908870256063055\n",
      "51 - Loss_train: 0.09272194013659599, Loss_test: 0.0908824127800814\n",
      "52 - Loss_train: 0.09271691124258258, Loss_test: 0.09087780245140491\n",
      "53 - Loss_train: 0.09271188494154446, Loss_test: 0.09087319460903703\n",
      "54 - Loss_train: 0.0927068612377128, Loss_test: 0.09086858926203392\n",
      "55 - Loss_train: 0.09270184012860395, Loss_test: 0.09086398640049484\n",
      "56 - Loss_train: 0.09269682161659316, Loss_test: 0.09085938603134029\n",
      "57 - Loss_train: 0.09269180569957514, Loss_test: 0.09085478815248743\n",
      "58 - Loss_train: 0.09268679237417196, Loss_test: 0.0908501927577291\n",
      "59 - Loss_train: 0.09268178163402864, Loss_test: 0.09084559984615906\n",
      "60 - Loss_train: 0.09267677348242574, Loss_test: 0.09084100941489871\n",
      "61 - Loss_train: 0.09267176791815429, Loss_test: 0.0908364214670165\n",
      "62 - Loss_train: 0.09266676494089211, Loss_test: 0.09083183600125379\n",
      "63 - Loss_train: 0.09266176454671522, Loss_test: 0.09082725301153642\n",
      "64 - Loss_train: 0.09265676673708903, Loss_test: 0.09082267250065824\n",
      "65 - Loss_train: 0.09265177151175588, Loss_test: 0.09081809447100846\n",
      "66 - Loss_train: 0.09264677887088806, Loss_test: 0.09081351891808236\n",
      "67 - Loss_train: 0.09264178880355531, Loss_test: 0.09080894583317159\n",
      "68 - Loss_train: 0.09263680131413399, Loss_test: 0.09080437522342365\n",
      "69 - Loss_train: 0.09263181640127327, Loss_test: 0.09079980708490747\n",
      "70 - Loss_train: 0.0926268340637848, Loss_test: 0.09079524141543689\n",
      "71 - Loss_train: 0.09262185430396276, Loss_test: 0.09079067821919048\n",
      "72 - Loss_train: 0.0926168771126494, Loss_test: 0.09078611748493483\n",
      "73 - Loss_train: 0.092611902494013, Loss_test: 0.09078155921987106\n",
      "74 - Loss_train: 0.09260693045098367, Loss_test: 0.09077700342289911\n",
      "75 - Loss_train: 0.09260196096982834, Loss_test: 0.09077245008607365\n",
      "76 - Loss_train: 0.09259699405909096, Loss_test: 0.09076789921255697\n",
      "77 - Loss_train: 0.0925920297168015, Loss_test: 0.0907633508026282\n",
      "78 - Loss_train: 0.09258706793692648, Loss_test: 0.09075880485030594\n",
      "79 - Loss_train: 0.09258210871882017, Loss_test: 0.09075426135466284\n",
      "80 - Loss_train: 0.0925771520681332, Loss_test: 0.09074972032098612\n",
      "81 - Loss_train: 0.09257219797524828, Loss_test: 0.09074518174159026\n",
      "82 - Loss_train: 0.09256724644007465, Loss_test: 0.09074064561404178\n",
      "83 - Loss_train: 0.09256229746754646, Loss_test: 0.09073611194413313\n",
      "84 - Loss_train: 0.09255735104761528, Loss_test: 0.09073158072280153\n",
      "85 - Loss_train: 0.0925524071834902, Loss_test: 0.09072705195208827\n",
      "86 - Loss_train: 0.09254746587407842, Loss_test: 0.09072252563079533\n",
      "87 - Loss_train: 0.09254252712204698, Loss_test: 0.09071800176255494\n",
      "88 - Loss_train: 0.09253759091734766, Loss_test: 0.09071348033709455\n",
      "89 - Loss_train: 0.09253265726400599, Loss_test: 0.09070896135859248\n",
      "90 - Loss_train: 0.0925277261629932, Loss_test: 0.09070444482701043\n",
      "91 - Loss_train: 0.09252279760512756, Loss_test: 0.09069993073371523\n",
      "92 - Loss_train: 0.09251787159444588, Loss_test: 0.09069541908331417\n",
      "93 - Loss_train: 0.09251294812844581, Loss_test: 0.09069090987372486\n",
      "94 - Loss_train: 0.09250802720594535, Loss_test: 0.09068640310228711\n",
      "95 - Loss_train: 0.0925031088316091, Loss_test: 0.09068189877507123\n",
      "96 - Loss_train: 0.09249819299297149, Loss_test: 0.09067739687812382\n",
      "97 - Loss_train: 0.09249327969530152, Loss_test: 0.09067289741774652\n",
      "98 - Loss_train: 0.09248836893590638, Loss_test: 0.09066840039214095\n",
      "99 - Loss_train: 0.09248346071419318, Loss_test: 0.0906639057981994\n",
      "100 - Loss_train: 0.09247855503243985, Loss_test: 0.0906594136408917\n",
      "101 - Loss_train: 0.0924736518800547, Loss_test: 0.09065492390966949\n",
      "102 - Loss_train: 0.09246875126363281, Loss_test: 0.09065043660832739\n",
      "103 - Loss_train: 0.09246385318134596, Loss_test: 0.09064595173630449\n",
      "104 - Loss_train: 0.09245895762618618, Loss_test: 0.09064146928812136\n",
      "105 - Loss_train: 0.09245406460423362, Loss_test: 0.09063698926880766\n",
      "106 - Loss_train: 0.09244917410703447, Loss_test: 0.09063251166941269\n",
      "107 - Loss_train: 0.09244428613679853, Loss_test: 0.0906280364933898\n",
      "108 - Loss_train: 0.09243940069379879, Loss_test: 0.09062356374031287\n",
      "109 - Loss_train: 0.0924345177713637, Loss_test: 0.09061909340363933\n",
      "110 - Loss_train: 0.09242963737585942, Loss_test: 0.09061462548882764\n",
      "111 - Loss_train: 0.09242475949951351, Loss_test: 0.09061015999102547\n",
      "112 - Loss_train: 0.0924198841417055, Loss_test: 0.09060569690531435\n",
      "113 - Loss_train: 0.09241501130544, Loss_test: 0.09060123623985769\n",
      "114 - Loss_train: 0.09241014098770266, Loss_test: 0.09059677798604023\n",
      "115 - Loss_train: 0.09240527318527735, Loss_test: 0.09059232214442481\n",
      "116 - Loss_train: 0.09240040790092381, Loss_test: 0.09058786871718352\n",
      "117 - Loss_train: 0.09239554513193235, Loss_test: 0.09058341770034488\n",
      "118 - Loss_train: 0.0923906848718642, Loss_test: 0.09057896908970671\n",
      "119 - Loss_train: 0.09238582712776217, Loss_test: 0.09057452289053214\n",
      "120 - Loss_train: 0.09238097188827599, Loss_test: 0.09057007909205919\n",
      "121 - Loss_train: 0.09237611915627504, Loss_test: 0.09056563769742808\n",
      "122 - Loss_train: 0.09237126893336679, Loss_test: 0.09056119870744356\n",
      "123 - Loss_train: 0.09236642122060489, Loss_test: 0.09055676212464359\n",
      "124 - Loss_train: 0.09236157601416191, Loss_test: 0.0905523279435503\n",
      "125 - Loss_train: 0.09235673330753526, Loss_test: 0.09054789615844824\n",
      "126 - Loss_train: 0.09235189310445367, Loss_test: 0.09054346677344481\n",
      "127 - Loss_train: 0.09234705540148125, Loss_test: 0.09053903978432705\n",
      "128 - Loss_train: 0.09234222019653374, Loss_test: 0.09053461519046561\n",
      "129 - Loss_train: 0.09233738748969926, Loss_test: 0.0905301929909916\n",
      "130 - Loss_train: 0.09233255727858806, Loss_test: 0.09052577318367473\n",
      "131 - Loss_train: 0.09232772956940857, Loss_test: 0.09052135577314434\n",
      "132 - Loss_train: 0.09232290435006674, Loss_test: 0.09051694075140913\n",
      "133 - Loss_train: 0.09231808162595613, Loss_test: 0.09051252811844934\n",
      "134 - Loss_train: 0.09231326139507914, Loss_test: 0.09050811787642642\n",
      "135 - Loss_train: 0.09230844365151061, Loss_test: 0.09050371001955945\n",
      "136 - Loss_train: 0.09230362839740117, Loss_test: 0.0904993045460695\n",
      "137 - Loss_train: 0.09229881563052916, Loss_test: 0.09049490145883958\n",
      "138 - Loss_train: 0.09229400535020296, Loss_test: 0.09049050075265078\n",
      "139 - Loss_train: 0.0922891975601844, Loss_test: 0.09048610243355931\n",
      "140 - Loss_train: 0.09228439225169908, Loss_test: 0.09048170649358396\n",
      "141 - Loss_train: 0.09227958943043438, Loss_test: 0.0904773129376538\n",
      "142 - Loss_train: 0.09227478909204076, Loss_test: 0.09047292175921787\n",
      "143 - Loss_train: 0.09226999122855903, Loss_test: 0.09046853295424415\n",
      "144 - Loss_train: 0.09226519584434209, Loss_test: 0.09046414652481988\n",
      "145 - Loss_train: 0.09226040293859576, Loss_test: 0.09045976247031695\n",
      "146 - Loss_train: 0.09225561251349117, Loss_test: 0.09045538079357977\n",
      "147 - Loss_train: 0.09225082455997528, Loss_test: 0.09045100148481368\n",
      "148 - Loss_train: 0.09224603907913638, Loss_test: 0.09044662454517813\n",
      "149 - Loss_train: 0.09224125607676738, Loss_test: 0.09044224998176903\n",
      "150 - Loss_train: 0.0922364755441946, Loss_test: 0.0904378777831464\n",
      "151 - Loss_train: 0.0922316974842767, Loss_test: 0.09043350795629908\n",
      "152 - Loss_train: 0.09222692189207848, Loss_test: 0.09042914049245293\n",
      "153 - Loss_train: 0.09222214876426571, Loss_test: 0.09042477539095209\n",
      "154 - Loss_train: 0.09221737810482193, Loss_test: 0.0904204126537979\n",
      "155 - Loss_train: 0.0922126099087726, Loss_test: 0.09041605227877318\n",
      "156 - Loss_train: 0.09220784417603957, Loss_test: 0.09041169426349387\n",
      "157 - Loss_train: 0.09220308090803157, Loss_test: 0.09040733860719286\n",
      "158 - Loss_train: 0.09219832009875886, Loss_test: 0.0904029853111097\n",
      "159 - Loss_train: 0.09219356175388926, Loss_test: 0.09039863437279944\n",
      "160 - Loss_train: 0.09218880586536023, Loss_test: 0.09039428578923601\n",
      "161 - Loss_train: 0.0921840524334649, Loss_test: 0.09038993956089644\n",
      "162 - Loss_train: 0.0921793014570034, Loss_test: 0.09038559568354688\n",
      "163 - Loss_train: 0.09217455294098288, Loss_test: 0.09038125416327307\n",
      "164 - Loss_train: 0.09216980688012247, Loss_test: 0.09037691499636501\n",
      "165 - Loss_train: 0.0921650632662736, Loss_test: 0.09037257817342484\n",
      "166 - Loss_train: 0.09216032210908072, Loss_test: 0.09036824370434146\n",
      "167 - Loss_train: 0.09215558339695401, Loss_test: 0.0903639115775029\n",
      "168 - Loss_train: 0.09215084713287372, Loss_test: 0.09035958179585332\n",
      "169 - Loss_train: 0.09214611331709326, Loss_test: 0.09035525436095364\n",
      "170 - Loss_train: 0.09214138194967593, Loss_test: 0.09035092927045608\n",
      "171 - Loss_train: 0.09213665302567126, Loss_test: 0.09034660652113394\n",
      "172 - Loss_train: 0.09213192654707535, Loss_test: 0.0903422861146663\n",
      "173 - Loss_train: 0.09212720250946807, Loss_test: 0.09033796804658745\n",
      "174 - Loss_train: 0.09212248091447894, Loss_test: 0.09033365231860085\n",
      "175 - Loss_train: 0.09211776175749545, Loss_test: 0.09032933892656857\n",
      "176 - Loss_train: 0.0921130450415497, Loss_test: 0.0903250278729899\n",
      "177 - Loss_train: 0.0921083307610472, Loss_test: 0.09032071915117575\n",
      "178 - Loss_train: 0.09210361891863021, Loss_test: 0.09031641276633953\n",
      "179 - Loss_train: 0.09209890950882159, Loss_test: 0.09031210871081824\n",
      "180 - Loss_train: 0.09209420253724325, Loss_test: 0.09030780699100169\n",
      "181 - Loss_train: 0.0920894979991549, Loss_test: 0.09030350760308663\n",
      "182 - Loss_train: 0.09208479588563571, Loss_test: 0.0902992105366807\n",
      "183 - Loss_train: 0.09208009620587033, Loss_test: 0.09029491580285601\n",
      "184 - Loss_train: 0.09207539895814333, Loss_test: 0.09029062339703671\n",
      "185 - Loss_train: 0.09207070413143378, Loss_test: 0.090286333311021\n",
      "186 - Loss_train: 0.09206601173382079, Loss_test: 0.09028204555201653\n",
      "187 - Loss_train: 0.09206132176108006, Loss_test: 0.09027776011497013\n",
      "188 - Loss_train: 0.09205663421447313, Loss_test: 0.09027347700270563\n",
      "189 - Loss_train: 0.09205194908597426, Loss_test: 0.09026919620504502\n",
      "190 - Loss_train: 0.0920472663775205, Loss_test: 0.09026491772667969\n",
      "191 - Loss_train: 0.09204258609313017, Loss_test: 0.09026064156894095\n",
      "192 - Loss_train: 0.0920379082226722, Loss_test: 0.0902563677260123\n",
      "193 - Loss_train: 0.09203323277575223, Loss_test: 0.09025209620069127\n",
      "194 - Loss_train: 0.0920285597449495, Loss_test: 0.09024782699038214\n",
      "195 - Loss_train: 0.09202388912525303, Loss_test: 0.0902435600893889\n",
      "196 - Loss_train: 0.09201922091809939, Loss_test: 0.09023929550035671\n",
      "197 - Loss_train: 0.09201455512799767, Loss_test: 0.09023503322537678\n",
      "198 - Loss_train: 0.09200989174845757, Loss_test: 0.09023077325791387\n",
      "199 - Loss_train: 0.09200523077688144, Loss_test: 0.09022651559866672\n",
      "200 - Loss_train: 0.09200057221683389, Loss_test: 0.09022226024692084\n",
      "201 - Loss_train: 0.09199591606283007, Loss_test: 0.09021800720082719\n",
      "202 - Loss_train: 0.09199126231901958, Loss_test: 0.09021375646205426\n",
      "203 - Loss_train: 0.09198661097932848, Loss_test: 0.09020950802591583\n",
      "204 - Loss_train: 0.09198196204306605, Loss_test: 0.09020526189129521\n",
      "205 - Loss_train: 0.09197731550555399, Loss_test: 0.09020101805429115\n",
      "206 - Loss_train: 0.09197267136914923, Loss_test: 0.09019677651638551\n",
      "207 - Loss_train: 0.09196802963337426, Loss_test: 0.0901925372775915\n",
      "208 - Loss_train: 0.09196339029808387, Loss_test: 0.09018830033621011\n",
      "209 - Loss_train: 0.09195875335927951, Loss_test: 0.09018406569111866\n",
      "210 - Loss_train: 0.09195411882161257, Loss_test: 0.0901798333446426\n",
      "211 - Loss_train: 0.09194948667692965, Loss_test: 0.0901756032897965\n",
      "212 - Loss_train: 0.09194485692175637, Loss_test: 0.09017137552323354\n",
      "213 - Loss_train: 0.0919402295636451, Loss_test: 0.09016715005186518\n",
      "214 - Loss_train: 0.09193560459481338, Loss_test: 0.09016292686824678\n",
      "215 - Loss_train: 0.09193098201781308, Loss_test: 0.09015870597509032\n",
      "216 - Loss_train: 0.09192636182732242, Loss_test: 0.09015448736676968\n",
      "217 - Loss_train: 0.0919217440251054, Loss_test: 0.09015027104428618\n",
      "218 - Loss_train: 0.09191712861525288, Loss_test: 0.09014605701506384\n",
      "219 - Loss_train: 0.091912515584665, Loss_test: 0.09014184526114459\n",
      "220 - Loss_train: 0.09190790493852762, Loss_test: 0.0901376357920758\n",
      "221 - Loss_train: 0.09190329667483625, Loss_test: 0.09013342860388764\n",
      "222 - Loss_train: 0.09189869079857842, Loss_test: 0.0901292236995479\n",
      "223 - Loss_train: 0.091894087299431, Loss_test: 0.09012502107333246\n",
      "224 - Loss_train: 0.09188948617951498, Loss_test: 0.09012082072372786\n",
      "225 - Loss_train: 0.09188488743618999, Loss_test: 0.09011662265125193\n",
      "226 - Loss_train: 0.09188029106868568, Loss_test: 0.09011242685195055\n",
      "227 - Loss_train: 0.0918756970770653, Loss_test: 0.09010823332742168\n",
      "228 - Loss_train: 0.09187110546029353, Loss_test: 0.0901040420764631\n",
      "229 - Loss_train: 0.09186651621616077, Loss_test: 0.09009985309817975\n",
      "230 - Loss_train: 0.09186192934366083, Loss_test: 0.09009566638966188\n",
      "231 - Loss_train: 0.09185734484578593, Loss_test: 0.09009148195369683\n",
      "232 - Loss_train: 0.09185276271812147, Loss_test: 0.09008729978738514\n",
      "233 - Loss_train: 0.0918481829541882, Loss_test: 0.0900831198824166\n",
      "234 - Loss_train: 0.09184360555644588, Loss_test: 0.09007894224544744\n",
      "235 - Loss_train: 0.09183903052979833, Loss_test: 0.09007476687584139\n",
      "236 - Loss_train: 0.09183445786845684, Loss_test: 0.0900705937714251\n",
      "237 - Loss_train: 0.09182988756655955, Loss_test: 0.09006642292553595\n",
      "238 - Loss_train: 0.09182531962553821, Loss_test: 0.09006225434053676\n",
      "239 - Loss_train: 0.09182075404954096, Loss_test: 0.09005808801802878\n",
      "240 - Loss_train: 0.09181619083524704, Loss_test: 0.09005392395789022\n",
      "241 - Loss_train: 0.09181162997386463, Loss_test: 0.09004976214720238\n",
      "242 - Loss_train: 0.09180707147213642, Loss_test: 0.0900456025980202\n",
      "243 - Loss_train: 0.09180251532352497, Loss_test: 0.09004144530015891\n",
      "244 - Loss_train: 0.09179796153462301, Loss_test: 0.09003729026145305\n",
      "245 - Loss_train: 0.09179341009993731, Loss_test: 0.09003313747495024\n",
      "246 - Loss_train: 0.09178886101750236, Loss_test: 0.09002898694079788\n",
      "247 - Loss_train: 0.09178431428353952, Loss_test: 0.0900248386539613\n",
      "248 - Loss_train: 0.09177976990109514, Loss_test: 0.09002069261875328\n",
      "249 - Loss_train: 0.09177522786175202, Loss_test: 0.09001654882572019\n",
      "250 - Loss_train: 0.09177068817604903, Loss_test: 0.09001240728552402\n",
      "251 - Loss_train: 0.09176615083410336, Loss_test: 0.09000826798796542\n",
      "252 - Loss_train: 0.09176161583711388, Loss_test: 0.09000413093549404\n",
      "253 - Loss_train: 0.09175708318618982, Loss_test: 0.08999999612845426\n",
      "254 - Loss_train: 0.09175255287610169, Loss_test: 0.08999586356114074\n",
      "255 - Loss_train: 0.09174802490990595, Loss_test: 0.08999173323718576\n",
      "256 - Loss_train: 0.09174349928436033, Loss_test: 0.08998760515351094\n",
      "257 - Loss_train: 0.09173897599265562, Loss_test: 0.08998347930370833\n",
      "258 - Loss_train: 0.09173445504204218, Loss_test: 0.08997935569418633\n",
      "259 - Loss_train: 0.09172993643059804, Loss_test: 0.08997523432326018\n",
      "260 - Loss_train: 0.09172542015548368, Loss_test: 0.08997111518762707\n",
      "261 - Loss_train: 0.09172090621341142, Loss_test: 0.0899669982860275\n",
      "262 - Loss_train: 0.09171639460493067, Loss_test: 0.08996288361583704\n",
      "263 - Loss_train: 0.09171188532924246, Loss_test: 0.08995877117970143\n",
      "264 - Loss_train: 0.09170737838333143, Loss_test: 0.0899546609728084\n",
      "265 - Loss_train: 0.09170287376312274, Loss_test: 0.08995055298936228\n",
      "266 - Loss_train: 0.09169837147051615, Loss_test: 0.08994644723739179\n",
      "267 - Loss_train: 0.09169387150686925, Loss_test: 0.08994234370981384\n",
      "268 - Loss_train: 0.0916893738671854, Loss_test: 0.08993824240888734\n",
      "269 - Loss_train: 0.09168487855721345, Loss_test: 0.08993414333639975\n",
      "270 - Loss_train: 0.09168038556631108, Loss_test: 0.08993004648244421\n",
      "271 - Loss_train: 0.09167589490188151, Loss_test: 0.08992595185454011\n",
      "272 - Loss_train: 0.09167140655824246, Loss_test: 0.08992185944896933\n",
      "273 - Loss_train: 0.09166692053097994, Loss_test: 0.08991776925779812\n",
      "274 - Loss_train: 0.09166243682146678, Loss_test: 0.08991368128528184\n",
      "275 - Loss_train: 0.09165795542957851, Loss_test: 0.08990959553048768\n",
      "276 - Loss_train: 0.09165347635875672, Loss_test: 0.08990551199647964\n",
      "277 - Loss_train: 0.09164899960088249, Loss_test: 0.08990143067518998\n",
      "278 - Loss_train: 0.09164452516154264, Loss_test: 0.08989735157199581\n",
      "279 - Loss_train: 0.0916400530304791, Loss_test: 0.08989327467903424\n",
      "280 - Loss_train: 0.09163558321124074, Loss_test: 0.0898891999956744\n",
      "281 - Loss_train: 0.09163111570057741, Loss_test: 0.08988512752283868\n",
      "282 - Loss_train: 0.09162665050520924, Loss_test: 0.08988105726347512\n",
      "283 - Loss_train: 0.09162218761478444, Loss_test: 0.08987698920953495\n",
      "284 - Loss_train: 0.09161772702925226, Loss_test: 0.08987292336245302\n",
      "285 - Loss_train: 0.09161326875254458, Loss_test: 0.08986885972301815\n",
      "286 - Loss_train: 0.0916088127781319, Loss_test: 0.08986479828636738\n",
      "287 - Loss_train: 0.09160435910833145, Loss_test: 0.08986073905457896\n",
      "288 - Loss_train: 0.09159990773764735, Loss_test: 0.08985668202260388\n",
      "289 - Loss_train: 0.0915954586679248, Loss_test: 0.08985262719114565\n",
      "290 - Loss_train: 0.09159101189816597, Loss_test: 0.08984857456050978\n",
      "291 - Loss_train: 0.09158656742649404, Loss_test: 0.08984452412939374\n",
      "292 - Loss_train: 0.09158212525402348, Loss_test: 0.08984047589627597\n",
      "293 - Loss_train: 0.09157768538024759, Loss_test: 0.08983642986224759\n",
      "294 - Loss_train: 0.09157324779973867, Loss_test: 0.08983238602122727\n",
      "295 - Loss_train: 0.09156881251031548, Loss_test: 0.08982834437346375\n",
      "296 - Loss_train: 0.09156437951785543, Loss_test: 0.0898243049217399\n",
      "297 - Loss_train: 0.09155994881805361, Loss_test: 0.08982026766351599\n",
      "298 - Loss_train: 0.09155552040688344, Loss_test: 0.08981623259495582\n",
      "299 - Loss_train: 0.09155109428317448, Loss_test: 0.08981219971237171\n",
      "300 - Loss_train: 0.09154667044529104, Loss_test: 0.08980816901992919\n",
      "301 - Loss_train: 0.09154224889917895, Loss_test: 0.0898041405160729\n",
      "302 - Loss_train: 0.0915378296357709, Loss_test: 0.08980011419855374\n",
      "303 - Loss_train: 0.09153341265689362, Loss_test: 0.08979609006422283\n",
      "304 - Loss_train: 0.09152899796045062, Loss_test: 0.08979206811472001\n",
      "305 - Loss_train: 0.09152458554432233, Loss_test: 0.08978804834360526\n",
      "306 - Loss_train: 0.09152017541492177, Loss_test: 0.0897840307622878\n",
      "307 - Loss_train: 0.0915157675618552, Loss_test: 0.08978001535727033\n",
      "308 - Loss_train: 0.09151136198606914, Loss_test: 0.08977600212933513\n",
      "309 - Loss_train: 0.09150695868691042, Loss_test: 0.08977199108001038\n",
      "310 - Loss_train: 0.09150255766449138, Loss_test: 0.08976798220783226\n",
      "311 - Loss_train: 0.09149815891686924, Loss_test: 0.0897639755120304\n",
      "312 - Loss_train: 0.09149376244271994, Loss_test: 0.08975997099056088\n",
      "313 - Loss_train: 0.09148936824320471, Loss_test: 0.08975596864370292\n",
      "314 - Loss_train: 0.09148497631502353, Loss_test: 0.08975196846941953\n",
      "315 - Loss_train: 0.09148058665575191, Loss_test: 0.08974797046395791\n",
      "316 - Loss_train: 0.0914761992699275, Loss_test: 0.08974397463372952\n",
      "317 - Loss_train: 0.09147181414786867, Loss_test: 0.08973998096837676\n",
      "318 - Loss_train: 0.09146743129426338, Loss_test: 0.08973598947281096\n",
      "319 - Loss_train: 0.09146305070498097, Loss_test: 0.08973200014156514\n",
      "320 - Loss_train: 0.09145867238059727, Loss_test: 0.08972801297677199\n",
      "321 - Loss_train: 0.09145429631885969, Loss_test: 0.08972402797499274\n",
      "322 - Loss_train: 0.09144992251887565, Loss_test: 0.08972004513832507\n",
      "323 - Loss_train: 0.09144555098511273, Loss_test: 0.0897160644663482\n",
      "324 - Loss_train: 0.09144118170726236, Loss_test: 0.08971208595227452\n",
      "325 - Loss_train: 0.09143681468715313, Loss_test: 0.08970810959811547\n",
      "326 - Loss_train: 0.09143244992840158, Loss_test: 0.08970413540562122\n",
      "327 - Loss_train: 0.09142808742699811, Loss_test: 0.08970016337118071\n",
      "328 - Loss_train: 0.09142372717927806, Loss_test: 0.08969619349266132\n",
      "329 - Loss_train: 0.09141936918189499, Loss_test: 0.0896922257653345\n",
      "330 - Loss_train: 0.09141501344240187, Loss_test: 0.08968826019633036\n",
      "331 - Loss_train: 0.0914106599513829, Loss_test: 0.08968429677939628\n",
      "332 - Loss_train: 0.09140630870924908, Loss_test: 0.08968033551036875\n",
      "333 - Loss_train: 0.09140195971717475, Loss_test: 0.08967637639421305\n",
      "334 - Loss_train: 0.09139761297610766, Loss_test: 0.08967241942916591\n",
      "335 - Loss_train: 0.09139326847941193, Loss_test: 0.08966846461083523\n",
      "336 - Loss_train: 0.09138892622817055, Loss_test: 0.08966451193968669\n",
      "337 - Loss_train: 0.091384586221071, Loss_test: 0.08966056141391322\n",
      "338 - Loss_train: 0.09138024846058503, Loss_test: 0.08965661303652353\n",
      "339 - Loss_train: 0.09137591294192329, Loss_test: 0.0896526668002509\n",
      "340 - Loss_train: 0.09137157966292493, Loss_test: 0.08964872270710757\n",
      "341 - Loss_train: 0.09136724862633575, Loss_test: 0.08964478075858447\n",
      "342 - Loss_train: 0.09136291982991136, Loss_test: 0.08964084095006226\n",
      "343 - Loss_train: 0.09135859326810994, Loss_test: 0.08963690327864338\n",
      "344 - Loss_train: 0.09135426894309091, Loss_test: 0.08963296774578909\n",
      "345 - Loss_train: 0.0913499468520072, Loss_test: 0.0896290343482401\n",
      "346 - Loss_train: 0.09134562699560488, Loss_test: 0.08962510308754854\n",
      "347 - Loss_train: 0.0913413093748326, Loss_test: 0.08962117396350597\n",
      "348 - Loss_train: 0.09133699398584649, Loss_test: 0.08961724697270429\n",
      "349 - Loss_train: 0.09133268083402192, Loss_test: 0.08961332212012531\n",
      "350 - Loss_train: 0.09132836990521764, Loss_test: 0.08960939939436229\n",
      "351 - Loss_train: 0.09132406120787381, Loss_test: 0.08960547880069444\n",
      "352 - Loss_train: 0.09131975473762362, Loss_test: 0.08960156033516205\n",
      "353 - Loss_train: 0.09131545049088863, Loss_test: 0.0895976439973365\n",
      "354 - Loss_train: 0.09131114846941055, Loss_test: 0.0895937297845353\n",
      "355 - Loss_train: 0.09130684867203588, Loss_test: 0.08958981769914058\n",
      "356 - Loss_train: 0.09130255109830766, Loss_test: 0.08958590773923732\n",
      "357 - Loss_train: 0.09129825574552121, Loss_test: 0.08958199990136176\n",
      "358 - Loss_train: 0.09129396261312821, Loss_test: 0.08957809418552783\n",
      "359 - Loss_train: 0.09128967169812122, Loss_test: 0.08957419059127145\n",
      "360 - Loss_train: 0.09128538300397553, Loss_test: 0.08957028911646045\n",
      "361 - Loss_train: 0.0912810965282364, Loss_test: 0.08956638976495476\n",
      "362 - Loss_train: 0.09127681226759099, Loss_test: 0.08956249252963691\n",
      "363 - Loss_train: 0.0912725302230365, Loss_test: 0.0895585974111319\n",
      "364 - Loss_train: 0.0912682503914339, Loss_test: 0.08955470441057066\n",
      "365 - Loss_train: 0.0912639727730303, Loss_test: 0.08955081352302632\n",
      "366 - Loss_train: 0.09125969736863968, Loss_test: 0.0895469247517819\n",
      "367 - Loss_train: 0.09125542417267234, Loss_test: 0.08954303809039424\n",
      "368 - Loss_train: 0.09125115319000601, Loss_test: 0.08953915354599427\n",
      "369 - Loss_train: 0.09124688441394471, Loss_test: 0.08953527110978039\n",
      "370 - Loss_train: 0.09124261784031314, Loss_test: 0.0895313907790783\n",
      "371 - Loss_train: 0.09123835347250565, Loss_test: 0.08952751255662347\n",
      "372 - Loss_train: 0.09123409130946888, Loss_test: 0.08952363644043349\n",
      "373 - Loss_train: 0.09122983135422945, Loss_test: 0.08951976243484763\n",
      "374 - Loss_train: 0.09122557359861082, Loss_test: 0.08951589053088663\n",
      "375 - Loss_train: 0.09122131804229969, Loss_test: 0.08951202072851898\n",
      "376 - Loss_train: 0.0912170646902152, Loss_test: 0.08950815303134624\n",
      "377 - Loss_train: 0.09121281353718769, Loss_test: 0.08950428743865484\n",
      "378 - Loss_train: 0.09120856457831779, Loss_test: 0.08950042393985852\n",
      "379 - Loss_train: 0.09120431782003031, Loss_test: 0.08949656254561437\n",
      "380 - Loss_train: 0.09120007325291922, Loss_test: 0.08949270324530768\n",
      "381 - Loss_train: 0.09119583088551603, Loss_test: 0.08948884604529916\n",
      "382 - Loss_train: 0.09119159070808189, Loss_test: 0.08948499093986195\n",
      "383 - Loss_train: 0.0911873527234481, Loss_test: 0.08948113792837278\n",
      "384 - Loss_train: 0.09118311692986748, Loss_test: 0.08947728701253936\n",
      "385 - Loss_train: 0.09117888332426397, Loss_test: 0.08947343818548087\n",
      "386 - Loss_train: 0.09117465191150449, Loss_test: 0.0894695914543281\n",
      "387 - Loss_train: 0.0911704226822645, Loss_test: 0.08946574680949777\n",
      "388 - Loss_train: 0.09116619564402915, Loss_test: 0.08946190425752924\n",
      "389 - Loss_train: 0.0911619707862369, Loss_test: 0.08945806379036599\n",
      "390 - Loss_train: 0.0911577481176796, Loss_test: 0.0894542254140105\n",
      "391 - Loss_train: 0.0911535276289171, Loss_test: 0.08945038911933365\n",
      "392 - Loss_train: 0.09114930932165098, Loss_test: 0.08944655491253056\n",
      "393 - Loss_train: 0.09114509319638837, Loss_test: 0.08944272278632305\n",
      "394 - Loss_train: 0.09114087924970801, Loss_test: 0.08943889274370181\n",
      "395 - Loss_train: 0.09113666748365269, Loss_test: 0.08943506478320956\n",
      "396 - Loss_train: 0.09113245789508995, Loss_test: 0.08943123890413882\n",
      "397 - Loss_train: 0.09112825048148264, Loss_test: 0.08942741510304052\n",
      "398 - Loss_train: 0.09112404524641855, Loss_test: 0.0894235933829847\n",
      "399 - Loss_train: 0.09111984218249754, Loss_test: 0.08941977373699003\n",
      "400 - Loss_train: 0.09111564129094409, Loss_test: 0.08941595616609936\n",
      "401 - Loss_train: 0.09111144257238699, Loss_test: 0.08941214067174928\n",
      "402 - Loss_train: 0.09110724602821807, Loss_test: 0.08940832725456326\n",
      "403 - Loss_train: 0.09110305164947341, Loss_test: 0.08940451590593652\n",
      "404 - Loss_train: 0.09109885944120306, Loss_test: 0.08940070663035977\n",
      "405 - Loss_train: 0.0910946694010634, Loss_test: 0.08939689942669271\n",
      "406 - Loss_train: 0.09109048152526164, Loss_test: 0.08939309429015446\n",
      "407 - Loss_train: 0.09108629581422378, Loss_test: 0.08938929122228954\n",
      "408 - Loss_train: 0.09108211226732219, Loss_test: 0.08938549022083792\n",
      "409 - Loss_train: 0.09107793088271232, Loss_test: 0.08938169128549146\n",
      "410 - Loss_train: 0.09107375166114574, Loss_test: 0.0893778944150628\n",
      "411 - Loss_train: 0.09106957460233865, Loss_test: 0.08937409961151091\n",
      "412 - Loss_train: 0.09106539970366774, Loss_test: 0.08937030687112629\n",
      "413 - Loss_train: 0.09106122696197827, Loss_test: 0.08936651619096758\n",
      "414 - Loss_train: 0.09105705637793274, Loss_test: 0.08936272757264757\n",
      "415 - Loss_train: 0.09105288795220615, Loss_test: 0.08935894101359702\n",
      "416 - Loss_train: 0.09104872167793113, Loss_test: 0.08935515651224674\n",
      "417 - Loss_train: 0.09104455756287255, Loss_test: 0.08935137407149127\n",
      "418 - Loss_train: 0.09104039559697284, Loss_test: 0.08934759368471669\n",
      "419 - Loss_train: 0.0910362357853861, Loss_test: 0.08934381535438177\n",
      "420 - Loss_train: 0.09103207812674921, Loss_test: 0.08934003908131183\n",
      "421 - Loss_train: 0.09102792261340988, Loss_test: 0.08933626485687815\n",
      "422 - Loss_train: 0.09102376924825999, Loss_test: 0.08933249268347575\n",
      "423 - Loss_train: 0.09101961803537466, Loss_test: 0.0893287225664635\n",
      "424 - Loss_train: 0.09101546896876805, Loss_test: 0.08932495449778276\n",
      "425 - Loss_train: 0.09101132204371952, Loss_test: 0.08932118847597469\n",
      "426 - Loss_train: 0.0910071772621021, Loss_test: 0.08931742450156498\n",
      "427 - Loss_train: 0.09100303462751708, Loss_test: 0.08931366257438714\n",
      "428 - Loss_train: 0.09099889413125144, Loss_test: 0.08930990269465013\n",
      "429 - Loss_train: 0.09099475577925227, Loss_test: 0.08930614485944519\n",
      "430 - Loss_train: 0.09099061956264233, Loss_test: 0.08930238906220521\n",
      "431 - Loss_train: 0.09098648548480626, Loss_test: 0.08929863531020728\n",
      "432 - Loss_train: 0.09098235354779607, Loss_test: 0.08929488360141531\n",
      "433 - Loss_train: 0.09097822374491864, Loss_test: 0.08929113393026426\n",
      "434 - Loss_train: 0.09097409607776735, Loss_test: 0.08928738629927381\n",
      "435 - Loss_train: 0.09096997054541635, Loss_test: 0.08928364070577997\n",
      "436 - Loss_train: 0.09096584714554223, Loss_test: 0.08927989715096167\n",
      "437 - Loss_train: 0.09096172587679535, Loss_test: 0.08927615562928701\n",
      "438 - Loss_train: 0.09095760673966283, Loss_test: 0.08927241614394023\n",
      "439 - Loss_train: 0.09095348973494956, Loss_test: 0.08926867869343519\n",
      "440 - Loss_train: 0.09094937485964444, Loss_test: 0.08926494327698875\n",
      "441 - Loss_train: 0.09094526211053812, Loss_test: 0.08926120989056947\n",
      "442 - Loss_train: 0.09094115149160666, Loss_test: 0.08925747853837\n",
      "443 - Loss_train: 0.09093704299715137, Loss_test: 0.08925374921469588\n",
      "444 - Loss_train: 0.09093293662334453, Loss_test: 0.08925002191551033\n",
      "445 - Loss_train: 0.09092883237152417, Loss_test: 0.08924629664314945\n",
      "446 - Loss_train: 0.09092473024289925, Loss_test: 0.08924257339719815\n",
      "447 - Loss_train: 0.09092063023709121, Loss_test: 0.08923885217850756\n",
      "448 - Loss_train: 0.09091653235325696, Loss_test: 0.0892351329855264\n",
      "449 - Loss_train: 0.09091243658674289, Loss_test: 0.08923141581548591\n",
      "450 - Loss_train: 0.09090834294031419, Loss_test: 0.08922770066700511\n",
      "451 - Loss_train: 0.09090425141055991, Loss_test: 0.08922398754055987\n",
      "452 - Loss_train: 0.09090016199156045, Loss_test: 0.08922027642877965\n",
      "453 - Loss_train: 0.09089607469180937, Loss_test: 0.08921656734137487\n",
      "454 - Loss_train: 0.09089198950168559, Loss_test: 0.08921286026781942\n",
      "455 - Loss_train: 0.09088790642424606, Loss_test: 0.08920915521037369\n",
      "456 - Loss_train: 0.0908838254615944, Loss_test: 0.08920545217135166\n",
      "457 - Loss_train: 0.09087974660536566, Loss_test: 0.0892017511456366\n",
      "458 - Loss_train: 0.09087566985834875, Loss_test: 0.08919805213157478\n",
      "459 - Loss_train: 0.09087159522265072, Loss_test: 0.0891943551343301\n",
      "460 - Loss_train: 0.09086752269068399, Loss_test: 0.08919066014543094\n",
      "461 - Loss_train: 0.0908634522650252, Loss_test: 0.08918696716435405\n",
      "462 - Loss_train: 0.09085938394820423, Loss_test: 0.08918327619885599\n",
      "463 - Loss_train: 0.09085531773564422, Loss_test: 0.08917958724140938\n",
      "464 - Loss_train: 0.09085125361961337, Loss_test: 0.08917590028392762\n",
      "465 - Loss_train: 0.09084719160664353, Loss_test: 0.08917221533389019\n",
      "466 - Loss_train: 0.09084313169459617, Loss_test: 0.08916853239478448\n",
      "467 - Loss_train: 0.09083907388480898, Loss_test: 0.08916485145851986\n",
      "468 - Loss_train: 0.09083501816935248, Loss_test: 0.08916117251899011\n",
      "469 - Loss_train: 0.09083096455144929, Loss_test: 0.08915749558460812\n",
      "470 - Loss_train: 0.09082691303028981, Loss_test: 0.08915382065021227\n",
      "471 - Loss_train: 0.09082286360741597, Loss_test: 0.08915014771843724\n",
      "472 - Loss_train: 0.09081881627487978, Loss_test: 0.08914647678384323\n",
      "473 - Loss_train: 0.09081477103904528, Loss_test: 0.0891428078494803\n",
      "474 - Loss_train: 0.09081072789095936, Loss_test: 0.08913914090694539\n",
      "475 - Loss_train: 0.09080668683356863, Loss_test: 0.08913547596046299\n",
      "476 - Loss_train: 0.09080264786487693, Loss_test: 0.08913181300730111\n",
      "477 - Loss_train: 0.09079861098763616, Loss_test: 0.08912815204993481\n",
      "478 - Loss_train: 0.0907945762018918, Loss_test: 0.08912449309031129\n",
      "479 - Loss_train: 0.09079054349687954, Loss_test: 0.08912083611244452\n",
      "480 - Loss_train: 0.09078651287842229, Loss_test: 0.0891171811296229\n",
      "481 - Loss_train: 0.09078248434475293, Loss_test: 0.08911352813619192\n",
      "482 - Loss_train: 0.09077845789479215, Loss_test: 0.08910987713129669\n",
      "483 - Loss_train: 0.09077443352682009, Loss_test: 0.08910622811073331\n",
      "484 - Loss_train: 0.09077041124327227, Loss_test: 0.08910258108244563\n",
      "485 - Loss_train: 0.09076639103891788, Loss_test: 0.08909893603590538\n",
      "486 - Loss_train: 0.09076237290946551, Loss_test: 0.08909529297185893\n",
      "487 - Loss_train: 0.09075835686028406, Loss_test: 0.08909165189076562\n",
      "488 - Loss_train: 0.09075434288893389, Loss_test: 0.08908801279525273\n",
      "489 - Loss_train: 0.0907503309895174, Loss_test: 0.0890843756734901\n",
      "490 - Loss_train: 0.09074632116910555, Loss_test: 0.0890807405384217\n",
      "491 - Loss_train: 0.09074231341713904, Loss_test: 0.08907710737600714\n",
      "492 - Loss_train: 0.09073830773865595, Loss_test: 0.08907347619258321\n",
      "493 - Loss_train: 0.09073430413189354, Loss_test: 0.08906984698473798\n",
      "494 - Loss_train: 0.09073030260016011, Loss_test: 0.0890662197587025\n",
      "495 - Loss_train: 0.09072630313604363, Loss_test: 0.08906259450337414\n",
      "496 - Loss_train: 0.09072230574087378, Loss_test: 0.08905897122398665\n",
      "497 - Loss_train: 0.09071831040881079, Loss_test: 0.08905534991336406\n",
      "498 - Loss_train: 0.09071431714683852, Loss_test: 0.08905173057569675\n",
      "499 - Loss_train: 0.09071032594656754, Loss_test: 0.08904811320824099\n",
      "500 - Loss_train: 0.0907063368105741, Loss_test: 0.08904449780833854\n",
      "501 - Loss_train: 0.0907023497370058, Loss_test: 0.08904088437658066\n",
      "502 - Loss_train: 0.0906983647245792, Loss_test: 0.08903727291196867\n",
      "503 - Loss_train: 0.09069438177672068, Loss_test: 0.08903366341686342\n",
      "504 - Loss_train: 0.0906904008888652, Loss_test: 0.08903005588659094\n",
      "505 - Loss_train: 0.09068642205667143, Loss_test: 0.08902645031719225\n",
      "506 - Loss_train: 0.09068244528536545, Loss_test: 0.089022846715331\n",
      "507 - Loss_train: 0.09067847057162212, Loss_test: 0.08901924507518315\n",
      "508 - Loss_train: 0.09067449791035256, Loss_test: 0.08901564539416451\n",
      "509 - Loss_train: 0.09067052730190425, Loss_test: 0.08901204767084087\n",
      "510 - Loss_train: 0.0906665587498776, Loss_test: 0.08900845190981997\n",
      "511 - Loss_train: 0.09066259225269817, Loss_test: 0.08900485810936531\n",
      "512 - Loss_train: 0.09065862780103824, Loss_test: 0.08900126625999796\n",
      "513 - Loss_train: 0.0906546654015658, Loss_test: 0.08899767636722741\n",
      "514 - Loss_train: 0.09065070504994009, Loss_test: 0.088994088428728\n",
      "515 - Loss_train: 0.09064674674585096, Loss_test: 0.08899050244441266\n",
      "516 - Loss_train: 0.09064279048992661, Loss_test: 0.08898691841309829\n",
      "517 - Loss_train: 0.0906388362783975, Loss_test: 0.08898333633268465\n",
      "518 - Loss_train: 0.09063488411401462, Loss_test: 0.08897975620640175\n",
      "519 - Loss_train: 0.09063093399140641, Loss_test: 0.08897617802428999\n",
      "520 - Loss_train: 0.09062698591190066, Loss_test: 0.08897260179295392\n",
      "521 - Loss_train: 0.09062303987585293, Loss_test: 0.08896902750974327\n",
      "522 - Loss_train: 0.09061909587931646, Loss_test: 0.0889654551723494\n",
      "523 - Loss_train: 0.09061515392254207, Loss_test: 0.08896188478193794\n",
      "524 - Loss_train: 0.09061121400438779, Loss_test: 0.08895831633389445\n",
      "525 - Loss_train: 0.0906072761241624, Loss_test: 0.08895474983107833\n",
      "526 - Loss_train: 0.09060334027997068, Loss_test: 0.08895118526993344\n",
      "527 - Loss_train: 0.0905994064719273, Loss_test: 0.08894762265089709\n",
      "528 - Loss_train: 0.09059547470126438, Loss_test: 0.08894406197421742\n",
      "529 - Loss_train: 0.09059154496058221, Loss_test: 0.08894050323472232\n",
      "530 - Loss_train: 0.09058761725102282, Loss_test: 0.08893694643284188\n",
      "531 - Loss_train: 0.09058369157353156, Loss_test: 0.08893339156837665\n",
      "532 - Loss_train: 0.09057976793007579, Loss_test: 0.08892983864352845\n",
      "533 - Loss_train: 0.09057584631738771, Loss_test: 0.08892628765535607\n",
      "534 - Loss_train: 0.09057192673316575, Loss_test: 0.0889227386016784\n",
      "535 - Loss_train: 0.09056800917069054, Loss_test: 0.08891919147636568\n",
      "536 - Loss_train: 0.09056409363825944, Loss_test: 0.08891564628694655\n",
      "537 - Loss_train: 0.0905601801283338, Loss_test: 0.08891210302601701\n",
      "538 - Loss_train: 0.09055626864053867, Loss_test: 0.08890856169860699\n",
      "539 - Loss_train: 0.0905523591765513, Loss_test: 0.08890502229162114\n",
      "540 - Loss_train: 0.09054845173692898, Loss_test: 0.08890148482169959\n",
      "541 - Loss_train: 0.09054454631483054, Loss_test: 0.08889794927443379\n",
      "542 - Loss_train: 0.09054064291714749, Loss_test: 0.08889441565643531\n",
      "543 - Loss_train: 0.09053674153475477, Loss_test: 0.08889088396164073\n",
      "544 - Loss_train: 0.09053284216940508, Loss_test: 0.08888735418899289\n",
      "545 - Loss_train: 0.09052894482095777, Loss_test: 0.08888382634195803\n",
      "546 - Loss_train: 0.09052504949155016, Loss_test: 0.08888030041758627\n",
      "547 - Loss_train: 0.09052115617471539, Loss_test: 0.08887677641444938\n",
      "548 - Loss_train: 0.0905172648693374, Loss_test: 0.08887325432593711\n",
      "549 - Loss_train: 0.09051337557715089, Loss_test: 0.08886973416080793\n",
      "550 - Loss_train: 0.09050948829631315, Loss_test: 0.0888662159118608\n",
      "551 - Loss_train: 0.09050560302494505, Loss_test: 0.08886269957828422\n",
      "552 - Loss_train: 0.09050171976414449, Loss_test: 0.08885918516242566\n",
      "553 - Loss_train: 0.09049783851597437, Loss_test: 0.08885567266607622\n",
      "554 - Loss_train: 0.09049395927431132, Loss_test: 0.0888521620828812\n",
      "555 - Loss_train: 0.09049008203908646, Loss_test: 0.08884865341086977\n",
      "556 - Loss_train: 0.09048620680688396, Loss_test: 0.08884514665097916\n",
      "557 - Loss_train: 0.09048233357749329, Loss_test: 0.08884164179753354\n",
      "558 - Loss_train: 0.09047846235675715, Loss_test: 0.08883813886191372\n",
      "559 - Loss_train: 0.0904745931335879, Loss_test: 0.08883463783006833\n",
      "560 - Loss_train: 0.0904707259101132, Loss_test: 0.08883113870487114\n",
      "561 - Loss_train: 0.0904668606920495, Loss_test: 0.08882764149055201\n",
      "562 - Loss_train: 0.09046299747316675, Loss_test: 0.08882414618287374\n",
      "563 - Loss_train: 0.09045913625117097, Loss_test: 0.08882065277981217\n",
      "564 - Loss_train: 0.09045527702663338, Loss_test: 0.08881716127903246\n",
      "565 - Loss_train: 0.09045141979790776, Loss_test: 0.08881367168310395\n",
      "566 - Loss_train: 0.09044756456050047, Loss_test: 0.08881018398267136\n",
      "567 - Loss_train: 0.09044371131721611, Loss_test: 0.08880669818587035\n",
      "568 - Loss_train: 0.09043986006705157, Loss_test: 0.08880321428880465\n",
      "569 - Loss_train: 0.0904360108084443, Loss_test: 0.08879973228816179\n",
      "570 - Loss_train: 0.09043216353974373, Loss_test: 0.08879625218647015\n",
      "571 - Loss_train: 0.09042831826446712, Loss_test: 0.08879277398371038\n",
      "572 - Loss_train: 0.09042447497591252, Loss_test: 0.08878929767528303\n",
      "573 - Loss_train: 0.09042063367450161, Loss_test: 0.08878582326169099\n",
      "574 - Loss_train: 0.09041679436143167, Loss_test: 0.08878235074303009\n",
      "575 - Loss_train: 0.09041295703735083, Loss_test: 0.08877888011939135\n",
      "576 - Loss_train: 0.09040912169468954, Loss_test: 0.08877541138423141\n",
      "577 - Loss_train: 0.09040528833889921, Loss_test: 0.08877194454448512\n",
      "578 - Loss_train: 0.09040145696335262, Loss_test: 0.08876847959073599\n",
      "579 - Loss_train: 0.09039762757226043, Loss_test: 0.08876501653021446\n",
      "580 - Loss_train: 0.09039380016093519, Loss_test: 0.08876155535447691\n",
      "581 - Loss_train: 0.09038997472905327, Loss_test: 0.0887580960656953\n",
      "582 - Loss_train: 0.09038615127436588, Loss_test: 0.08875463866282862\n",
      "583 - Loss_train: 0.09038232979492185, Loss_test: 0.08875118314221135\n",
      "584 - Loss_train: 0.09037851029319482, Loss_test: 0.08874772950512415\n",
      "585 - Loss_train: 0.09037469276991104, Loss_test: 0.08874427775484672\n",
      "586 - Loss_train: 0.09037087721773221, Loss_test: 0.08874082788332474\n",
      "587 - Loss_train: 0.09036706364302202, Loss_test: 0.08873737989525417\n",
      "588 - Loss_train: 0.0903632520372869, Loss_test: 0.08873393378481388\n",
      "589 - Loss_train: 0.09035944240386994, Loss_test: 0.08873048955355635\n",
      "590 - Loss_train: 0.09035563474317777, Loss_test: 0.0887270472020475\n",
      "591 - Loss_train: 0.09035182905156701, Loss_test: 0.08872360672714177\n",
      "592 - Loss_train: 0.09034802532558485, Loss_test: 0.08872016812533905\n",
      "593 - Loss_train: 0.09034422356660764, Loss_test: 0.08871673139803839\n",
      "594 - Loss_train: 0.090340423774, Loss_test: 0.08871329654533025\n",
      "595 - Loss_train: 0.09033662594709822, Loss_test: 0.08870986356530164\n",
      "596 - Loss_train: 0.09033283009084599, Loss_test: 0.08870643246108638\n",
      "597 - Loss_train: 0.0903290361953945, Loss_test: 0.08870300323010867\n",
      "598 - Loss_train: 0.09032524426227805, Loss_test: 0.08869957586508885\n",
      "599 - Loss_train: 0.09032145428979328, Loss_test: 0.0886961503699334\n",
      "600 - Loss_train: 0.09031766627567542, Loss_test: 0.08869272674066285\n",
      "601 - Loss_train: 0.09031388022305269, Loss_test: 0.08868930497992748\n",
      "602 - Loss_train: 0.09031009612504502, Loss_test: 0.0886858850819458\n",
      "603 - Loss_train: 0.09030631398501152, Loss_test: 0.08868246704915402\n",
      "604 - Loss_train: 0.09030253380054644, Loss_test: 0.08867905088080233\n",
      "605 - Loss_train: 0.09029875557209642, Loss_test: 0.08867563657617908\n",
      "606 - Loss_train: 0.0902949793021382, Loss_test: 0.08867222413615734\n",
      "607 - Loss_train: 0.09029120498013674, Loss_test: 0.08866881355295829\n",
      "608 - Loss_train: 0.09028743261132495, Loss_test: 0.08866540482984239\n",
      "609 - Loss_train: 0.09028366219755136, Loss_test: 0.08866199797075167\n",
      "610 - Loss_train: 0.09027989373012724, Loss_test: 0.08865859296651178\n",
      "611 - Loss_train: 0.09027612721360781, Loss_test: 0.08865518981857583\n",
      "612 - Loss_train: 0.09027236264332388, Loss_test: 0.08865178852704099\n",
      "613 - Loss_train: 0.09026860002525901, Loss_test: 0.08864838909301825\n",
      "614 - Loss_train: 0.09026483934885303, Loss_test: 0.08864499151265409\n",
      "615 - Loss_train: 0.0902610806171958, Loss_test: 0.08864159578170418\n",
      "616 - Loss_train: 0.09025732383417423, Loss_test: 0.08863820190805638\n",
      "617 - Loss_train: 0.09025356899414015, Loss_test: 0.08863480988539721\n",
      "618 - Loss_train: 0.090249816092966, Loss_test: 0.08863141971015713\n",
      "619 - Loss_train: 0.09024606513277683, Loss_test: 0.08862803138412183\n",
      "620 - Loss_train: 0.0902423161157502, Loss_test: 0.08862464490865088\n",
      "621 - Loss_train: 0.09023856903471282, Loss_test: 0.08862126027771601\n",
      "622 - Loss_train: 0.0902348238919073, Loss_test: 0.08861787749292166\n",
      "623 - Loss_train: 0.09023108069106059, Loss_test: 0.08861449655756103\n",
      "624 - Loss_train: 0.09022733942144848, Loss_test: 0.08861111746321201\n",
      "625 - Loss_train: 0.09022360008786065, Loss_test: 0.08860774021083907\n",
      "626 - Loss_train: 0.09021986269029904, Loss_test: 0.0886043648042255\n",
      "627 - Loss_train: 0.090216127224144, Loss_test: 0.08860099123827492\n",
      "628 - Loss_train: 0.09021239369180208, Loss_test: 0.08859761951173152\n",
      "629 - Loss_train: 0.09020866208966993, Loss_test: 0.08859424962497643\n",
      "630 - Loss_train: 0.09020493241701331, Loss_test: 0.08859088157682685\n",
      "631 - Loss_train: 0.09020120467401287, Loss_test: 0.08858751536497453\n",
      "632 - Loss_train: 0.09019747886250186, Loss_test: 0.08858415099388085\n",
      "633 - Loss_train: 0.09019375497824945, Loss_test: 0.08858078845863059\n",
      "634 - Loss_train: 0.09019003301985928, Loss_test: 0.08857742775758136\n",
      "635 - Loss_train: 0.09018631298542022, Loss_test: 0.08857406888763066\n",
      "636 - Loss_train: 0.09018259487367901, Loss_test: 0.08857071185096102\n",
      "637 - Loss_train: 0.09017887868511014, Loss_test: 0.08856735664362703\n",
      "638 - Loss_train: 0.0901751644208653, Loss_test: 0.08856400327245721\n",
      "639 - Loss_train: 0.09017145207513012, Loss_test: 0.08856065172457743\n",
      "640 - Loss_train: 0.09016774165068045, Loss_test: 0.08855730200871753\n",
      "641 - Loss_train: 0.09016403314570823, Loss_test: 0.08855395411940005\n",
      "642 - Loss_train: 0.09016032655934447, Loss_test: 0.08855060805848557\n",
      "643 - Loss_train: 0.09015662189028631, Loss_test: 0.08854726382323386\n",
      "644 - Loss_train: 0.09015291913709807, Loss_test: 0.08854392141222515\n",
      "645 - Loss_train: 0.09014921830330083, Loss_test: 0.08854058082952326\n",
      "646 - Loss_train: 0.09014551938032435, Loss_test: 0.08853724206594825\n",
      "647 - Loss_train: 0.0901418223703745, Loss_test: 0.08853390512410106\n",
      "648 - Loss_train: 0.09013812727479333, Loss_test: 0.08853057000526907\n",
      "649 - Loss_train: 0.09013443409057392, Loss_test: 0.08852723670697787\n",
      "650 - Loss_train: 0.09013074281847652, Loss_test: 0.08852390522965964\n",
      "651 - Loss_train: 0.09012705345252903, Loss_test: 0.08852057556649885\n",
      "652 - Loss_train: 0.09012336599721175, Loss_test: 0.08851724772246992\n",
      "653 - Loss_train: 0.0901196804475017, Loss_test: 0.08851392169441424\n",
      "654 - Loss_train: 0.09011599680458612, Loss_test: 0.0885105974805836\n",
      "655 - Loss_train: 0.09011231506683584, Loss_test: 0.08850727508196758\n",
      "656 - Loss_train: 0.090108635235589, Loss_test: 0.08850395449819812\n",
      "657 - Loss_train: 0.0901049573095356, Loss_test: 0.08850063572669761\n",
      "658 - Loss_train: 0.09010128128289643, Loss_test: 0.08849731876626751\n",
      "659 - Loss_train: 0.09009760715895744, Loss_test: 0.08849400361624116\n",
      "660 - Loss_train: 0.09009393493438844, Loss_test: 0.08849069027546323\n",
      "661 - Loss_train: 0.0900902646106909, Loss_test: 0.08848737874376002\n",
      "662 - Loss_train: 0.09008659618764454, Loss_test: 0.08848406902064936\n",
      "663 - Loss_train: 0.09008292966322148, Loss_test: 0.08848076110574282\n",
      "664 - Loss_train: 0.09007926503304511, Loss_test: 0.0884774549953188\n",
      "665 - Loss_train: 0.09007560229822789, Loss_test: 0.0884741506874032\n",
      "666 - Loss_train: 0.09007194145685087, Loss_test: 0.08847084818491349\n",
      "667 - Loss_train: 0.09006828250896241, Loss_test: 0.08846754748333763\n",
      "668 - Loss_train: 0.09006462545433824, Loss_test: 0.08846424858425568\n",
      "669 - Loss_train: 0.09006097029131632, Loss_test: 0.08846095148560479\n",
      "670 - Loss_train: 0.09005731702277653, Loss_test: 0.08845765619228323\n",
      "671 - Loss_train: 0.09005366564188534, Loss_test: 0.08845436269452724\n",
      "672 - Loss_train: 0.09005001615222943, Loss_test: 0.08845107099651354\n",
      "673 - Loss_train: 0.09004636855071514, Loss_test: 0.08844778109621869\n",
      "674 - Loss_train: 0.09004272283132789, Loss_test: 0.08844449298753586\n",
      "675 - Loss_train: 0.09003907899920358, Loss_test: 0.08844120667638082\n",
      "676 - Loss_train: 0.09003543705531132, Loss_test: 0.08843792216180202\n",
      "677 - Loss_train: 0.09003179699223354, Loss_test: 0.08843463943838789\n",
      "678 - Loss_train: 0.09002815881143057, Loss_test: 0.08843135850551843\n",
      "679 - Loss_train: 0.09002452251369235, Loss_test: 0.08842807936569232\n",
      "680 - Loss_train: 0.09002088810062482, Loss_test: 0.08842480201844764\n",
      "681 - Loss_train: 0.09001725556383496, Loss_test: 0.08842152645926352\n",
      "682 - Loss_train: 0.09001362490639427, Loss_test: 0.08841825268995648\n",
      "683 - Loss_train: 0.09000999612833223, Loss_test: 0.08841498070471897\n",
      "684 - Loss_train: 0.09000636922549536, Loss_test: 0.08841171050753643\n",
      "685 - Loss_train: 0.09000274420004994, Loss_test: 0.08840844209767819\n",
      "686 - Loss_train: 0.08999912105346299, Loss_test: 0.08840517547451508\n",
      "687 - Loss_train: 0.0899954997791941, Loss_test: 0.08840191063301364\n",
      "688 - Loss_train: 0.08999188038029864, Loss_test: 0.08839864757797962\n",
      "689 - Loss_train: 0.08998826285587792, Loss_test: 0.08839538630506259\n",
      "690 - Loss_train: 0.08998464719736489, Loss_test: 0.08839212681099085\n",
      "691 - Loss_train: 0.08998103341397232, Loss_test: 0.08838886909836087\n",
      "692 - Loss_train: 0.08997742149556509, Loss_test: 0.08838561316199689\n",
      "693 - Loss_train: 0.08997381144691295, Loss_test: 0.08838235900545367\n",
      "694 - Loss_train: 0.08997020326831252, Loss_test: 0.08837910662851566\n",
      "695 - Loss_train: 0.08996659695279573, Loss_test: 0.08837585602378381\n",
      "696 - Loss_train: 0.08996299250612881, Loss_test: 0.08837260719814087\n",
      "697 - Loss_train: 0.08995938992428043, Loss_test: 0.0883693601464981\n",
      "698 - Loss_train: 0.08995578920326551, Loss_test: 0.08836611486523928\n",
      "699 - Loss_train: 0.08995219034505943, Loss_test: 0.08836287135737438\n",
      "700 - Loss_train: 0.08994859334922269, Loss_test: 0.08835962962202724\n",
      "701 - Loss_train: 0.08994499821369316, Loss_test: 0.08835638965789497\n",
      "702 - Loss_train: 0.08994140493954283, Loss_test: 0.08835315146289445\n",
      "703 - Loss_train: 0.08993781352648521, Loss_test: 0.08834991503932442\n",
      "704 - Loss_train: 0.08993422396873536, Loss_test: 0.08834668038282811\n",
      "705 - Loss_train: 0.08993063627052647, Loss_test: 0.08834344749178992\n",
      "706 - Loss_train: 0.08992705042485882, Loss_test: 0.08834021636697152\n",
      "707 - Loss_train: 0.0899234664360327, Loss_test: 0.08833698700864542\n",
      "708 - Loss_train: 0.08991988429987356, Loss_test: 0.08833375941265069\n",
      "709 - Loss_train: 0.08991630401790911, Loss_test: 0.08833053358023124\n",
      "710 - Loss_train: 0.08991272558991971, Loss_test: 0.08832730951288474\n",
      "711 - Loss_train: 0.08990914901039097, Loss_test: 0.08832408720178324\n",
      "712 - Loss_train: 0.08990557428122604, Loss_test: 0.08832086665325144\n",
      "713 - Loss_train: 0.08990200140634837, Loss_test: 0.08831764786778429\n",
      "714 - Loss_train: 0.0898984303780971, Loss_test: 0.08831443084253641\n",
      "715 - Loss_train: 0.08989486119719922, Loss_test: 0.08831121557165122\n",
      "716 - Loss_train: 0.08989129386308733, Loss_test: 0.08830800205763052\n",
      "717 - Loss_train: 0.08988772837208137, Loss_test: 0.08830479029760348\n",
      "718 - Loss_train: 0.08988416472538423, Loss_test: 0.08830158029337117\n",
      "719 - Loss_train: 0.08988060292187293, Loss_test: 0.08829837204242767\n",
      "720 - Loss_train: 0.0898770429646631, Loss_test: 0.08829516554573086\n",
      "721 - Loss_train: 0.08987348484498772, Loss_test: 0.0882919607984541\n",
      "722 - Loss_train: 0.08986992857059153, Loss_test: 0.08828875780726177\n",
      "723 - Loss_train: 0.08986637413276713, Loss_test: 0.08828555656263971\n",
      "724 - Loss_train: 0.08986282153358997, Loss_test: 0.08828235706608996\n",
      "725 - Loss_train: 0.08985927077644708, Loss_test: 0.0882791593227994\n",
      "726 - Loss_train: 0.0898557218528958, Loss_test: 0.08827596332422616\n",
      "727 - Loss_train: 0.08985217476573114, Loss_test: 0.08827276907313307\n",
      "728 - Loss_train: 0.08984862951301341, Loss_test: 0.08826957656511544\n",
      "729 - Loss_train: 0.08984508609493312, Loss_test: 0.08826638580317306\n",
      "730 - Loss_train: 0.08984154451525835, Loss_test: 0.0882631967887302\n",
      "731 - Loss_train: 0.08983800476674841, Loss_test: 0.08826000951691002\n",
      "732 - Loss_train: 0.08983446684594772, Loss_test: 0.08825682398452209\n",
      "733 - Loss_train: 0.08983093075536822, Loss_test: 0.08825364019167935\n",
      "734 - Loss_train: 0.08982739649464258, Loss_test: 0.08825045813837448\n",
      "735 - Loss_train: 0.08982386406365729, Loss_test: 0.08824727782874926\n",
      "736 - Loss_train: 0.08982033345850898, Loss_test: 0.08824409925420651\n",
      "737 - Loss_train: 0.08981680468334025, Loss_test: 0.08824092241837632\n",
      "738 - Loss_train: 0.08981327773040496, Loss_test: 0.0882377473175277\n",
      "739 - Loss_train: 0.08980975260611676, Loss_test: 0.08823457395407165\n",
      "740 - Loss_train: 0.08980622930275842, Loss_test: 0.08823140232516341\n",
      "741 - Loss_train: 0.08980270782352699, Loss_test: 0.08822823242839373\n",
      "742 - Loss_train: 0.08979918816887104, Loss_test: 0.08822506426725435\n",
      "743 - Loss_train: 0.08979567033455123, Loss_test: 0.08822189783922317\n",
      "744 - Loss_train: 0.08979215431967941, Loss_test: 0.08821873313992705\n",
      "745 - Loss_train: 0.08978864012164872, Loss_test: 0.08821557016931406\n",
      "746 - Loss_train: 0.08978512774151014, Loss_test: 0.08821240892653082\n",
      "747 - Loss_train: 0.089781617178211, Loss_test: 0.08820924941272591\n",
      "748 - Loss_train: 0.08977810843485153, Loss_test: 0.08820609162675887\n",
      "749 - Loss_train: 0.08977460150268453, Loss_test: 0.08820293556605431\n",
      "750 - Loss_train: 0.08977109638580773, Loss_test: 0.08819978123113954\n",
      "751 - Loss_train: 0.08976759308280856, Loss_test: 0.08819662862078383\n",
      "752 - Loss_train: 0.089764091591939, Loss_test: 0.08819347773380601\n",
      "753 - Loss_train: 0.08976059191296774, Loss_test: 0.08819032856894014\n",
      "754 - Loss_train: 0.0897570940483754, Loss_test: 0.0881871811315176\n",
      "755 - Loss_train: 0.08975359798989432, Loss_test: 0.08818403540848908\n",
      "756 - Loss_train: 0.08975010374041525, Loss_test: 0.08818089140693627\n",
      "757 - Loss_train: 0.08974661130229936, Loss_test: 0.08817774912758414\n",
      "758 - Loss_train: 0.08974312067131812, Loss_test: 0.08817460856572776\n",
      "759 - Loss_train: 0.08973963184489388, Loss_test: 0.08817146972129787\n",
      "760 - Loss_train: 0.08973614482332669, Loss_test: 0.08816833259106209\n",
      "761 - Loss_train: 0.08973265960812772, Loss_test: 0.08816519717972666\n",
      "762 - Loss_train: 0.089729176195465, Loss_test: 0.08816206348269794\n",
      "763 - Loss_train: 0.08972569458549583, Loss_test: 0.0881589314961097\n",
      "764 - Loss_train: 0.089722214776587, Loss_test: 0.08815580122683132\n",
      "765 - Loss_train: 0.08971873676619643, Loss_test: 0.08815267266494181\n",
      "766 - Loss_train: 0.08971526055762989, Loss_test: 0.08814954581573468\n",
      "767 - Loss_train: 0.08971178614564654, Loss_test: 0.08814642067562531\n",
      "768 - Loss_train: 0.08970831352937468, Loss_test: 0.08814329724167844\n",
      "769 - Loss_train: 0.08970484271231885, Loss_test: 0.08814017551799944\n",
      "770 - Loss_train: 0.0897013736889325, Loss_test: 0.0881370555009098\n",
      "771 - Loss_train: 0.0896979064635047, Loss_test: 0.08813393719276214\n",
      "772 - Loss_train: 0.08969444102971662, Loss_test: 0.08813082058626527\n",
      "773 - Loss_train: 0.0896909773882982, Loss_test: 0.08812770568525415\n",
      "774 - Loss_train: 0.08968751554230278, Loss_test: 0.0881245924903041\n",
      "775 - Loss_train: 0.08968405548473611, Loss_test: 0.08812148099547473\n",
      "776 - Loss_train: 0.08968059721762518, Loss_test: 0.08811837120239015\n",
      "777 - Loss_train: 0.08967714073993857, Loss_test: 0.08811526311226582\n",
      "778 - Loss_train: 0.08967368605150017, Loss_test: 0.08811215672081839\n",
      "779 - Loss_train: 0.08967023315112439, Loss_test: 0.08810905202826971\n",
      "780 - Loss_train: 0.08966678203635524, Loss_test: 0.08810594903649735\n",
      "781 - Loss_train: 0.08966333270595363, Loss_test: 0.08810284773828281\n",
      "782 - Loss_train: 0.089659885161643, Loss_test: 0.08809974813673574\n",
      "783 - Loss_train: 0.08965643940099263, Loss_test: 0.08809665023342925\n",
      "784 - Loss_train: 0.08965299542318418, Loss_test: 0.08809355402197959\n",
      "785 - Loss_train: 0.08964955322781286, Loss_test: 0.08809045950588125\n",
      "786 - Loss_train: 0.08964611281520317, Loss_test: 0.08808736668425947\n",
      "787 - Loss_train: 0.08964267418077611, Loss_test: 0.08808427555271924\n",
      "788 - Loss_train: 0.08963923732865742, Loss_test: 0.08808118611532526\n",
      "789 - Loss_train: 0.08963580225267938, Loss_test: 0.08807809836384804\n",
      "790 - Loss_train: 0.08963236895205036, Loss_test: 0.0880750123030277\n",
      "791 - Loss_train: 0.08962893743298195, Loss_test: 0.08807192793298604\n",
      "792 - Loss_train: 0.08962550768894088, Loss_test: 0.08806884525164252\n",
      "793 - Loss_train: 0.08962207971598375, Loss_test: 0.08806576425286487\n",
      "794 - Loss_train: 0.08961865351741301, Loss_test: 0.08806268493926317\n",
      "795 - Loss_train: 0.08961522909169607, Loss_test: 0.08805960731254119\n",
      "796 - Loss_train: 0.08961180643875846, Loss_test: 0.0880565313676029\n",
      "797 - Loss_train: 0.08960838555566523, Loss_test: 0.08805345710713397\n",
      "798 - Loss_train: 0.08960496644288152, Loss_test: 0.08805038452813016\n",
      "799 - Loss_train: 0.08960154910013489, Loss_test: 0.08804731363160406\n",
      "800 - Loss_train: 0.08959813352749363, Loss_test: 0.0880442444163016\n",
      "801 - Loss_train: 0.0895947197230122, Loss_test: 0.08804117688110313\n",
      "802 - Loss_train: 0.08959130768154859, Loss_test: 0.0880381110216934\n",
      "803 - Loss_train: 0.08958789740561028, Loss_test: 0.08803504683976761\n",
      "804 - Loss_train: 0.089584488894439, Loss_test: 0.08803198433500124\n",
      "805 - Loss_train: 0.08958108214704351, Loss_test: 0.08802892350713448\n",
      "806 - Loss_train: 0.08957767716350966, Loss_test: 0.08802586435272991\n",
      "807 - Loss_train: 0.0895742739439099, Loss_test: 0.08802280687965368\n",
      "808 - Loss_train: 0.08957087248357574, Loss_test: 0.0880197510687169\n",
      "809 - Loss_train: 0.08956747278342883, Loss_test: 0.0880166969392154\n",
      "810 - Loss_train: 0.08956407484098124, Loss_test: 0.08801364447481913\n",
      "811 - Loss_train: 0.08956067866149597, Loss_test: 0.08801059368724859\n",
      "812 - Loss_train: 0.08955728423827927, Loss_test: 0.08800754456808986\n",
      "813 - Loss_train: 0.08955389157018682, Loss_test: 0.08800449711634085\n",
      "814 - Loss_train: 0.08955050065510554, Loss_test: 0.08800145133018474\n",
      "815 - Loss_train: 0.08954711149646372, Loss_test: 0.08799840721284781\n",
      "816 - Loss_train: 0.08954372409075222, Loss_test: 0.08799536476080154\n",
      "817 - Loss_train: 0.08954033843737157, Loss_test: 0.0879923239745216\n",
      "818 - Loss_train: 0.08953695453622536, Loss_test: 0.08798928484940623\n",
      "819 - Loss_train: 0.08953357238979233, Loss_test: 0.08798624739662818\n",
      "820 - Loss_train: 0.08953019198982841, Loss_test: 0.08798321159818863\n",
      "821 - Loss_train: 0.08952681333912897, Loss_test: 0.08798017746387571\n",
      "822 - Loss_train: 0.08952343643987015, Loss_test: 0.08797714499376788\n",
      "823 - Loss_train: 0.08952006128605093, Loss_test: 0.08797411418051054\n",
      "824 - Loss_train: 0.08951668787842744, Loss_test: 0.08797108502645493\n",
      "825 - Loss_train: 0.08951331621943852, Loss_test: 0.08796805753523071\n",
      "826 - Loss_train: 0.08950994630279702, Loss_test: 0.08796503169579897\n",
      "827 - Loss_train: 0.08950657812820417, Loss_test: 0.08796200751404978\n",
      "828 - Loss_train: 0.08950321169710705, Loss_test: 0.08795898498559504\n",
      "829 - Loss_train: 0.08949984700776438, Loss_test: 0.0879559641151756\n",
      "830 - Loss_train: 0.08949648406064936, Loss_test: 0.08795294489606933\n",
      "831 - Loss_train: 0.08949312285641846, Loss_test: 0.08794992733463293\n",
      "832 - Loss_train: 0.08948976338807674, Loss_test: 0.08794691142086901\n",
      "833 - Loss_train: 0.08948640566097303, Loss_test: 0.08794389716221027\n",
      "834 - Loss_train: 0.08948304966814222, Loss_test: 0.08794088454996658\n",
      "835 - Loss_train: 0.08947969541569033, Loss_test: 0.08793787358941334\n",
      "836 - Loss_train: 0.08947634289524305, Loss_test: 0.08793486427768009\n",
      "837 - Loss_train: 0.08947299210892219, Loss_test: 0.08793185660864342\n",
      "838 - Loss_train: 0.08946964306069204, Loss_test: 0.08792885059429152\n",
      "839 - Loss_train: 0.08946629574735963, Loss_test: 0.08792584622461357\n",
      "840 - Loss_train: 0.08946295016532585, Loss_test: 0.08792284350112378\n",
      "841 - Loss_train: 0.08945960631150797, Loss_test: 0.08791984241689964\n",
      "842 - Loss_train: 0.08945626419066925, Loss_test: 0.08791684297955826\n",
      "843 - Loss_train: 0.08945292379887843, Loss_test: 0.08791384518428098\n",
      "844 - Loss_train: 0.08944958513282558, Loss_test: 0.08791084902880689\n",
      "845 - Loss_train: 0.08944624819470552, Loss_test: 0.08790785451311763\n",
      "846 - Loss_train: 0.08944291298463786, Loss_test: 0.08790486163939218\n",
      "847 - Loss_train: 0.08943957949945938, Loss_test: 0.08790187040246261\n",
      "848 - Loss_train: 0.08943624773957319, Loss_test: 0.08789888080409616\n",
      "849 - Loss_train: 0.08943291770490018, Loss_test: 0.08789589284600384\n",
      "850 - Loss_train: 0.08942958939623707, Loss_test: 0.0878929065243841\n",
      "851 - Loss_train: 0.08942626280722903, Loss_test: 0.08788992183568012\n",
      "852 - Loss_train: 0.08942293793921441, Loss_test: 0.087886938782889\n",
      "853 - Loss_train: 0.08941961479184456, Loss_test: 0.08788395736424681\n",
      "854 - Loss_train: 0.08941629336269946, Loss_test: 0.08788097757554253\n",
      "855 - Loss_train: 0.08941297365384149, Loss_test: 0.0878779994221237\n",
      "856 - Loss_train: 0.08940965566466852, Loss_test: 0.08787502289968091\n",
      "857 - Loss_train: 0.08940633938999298, Loss_test: 0.08787204800553641\n",
      "858 - Loss_train: 0.08940302483455873, Loss_test: 0.08786907474515479\n",
      "859 - Loss_train: 0.08939971199220464, Loss_test: 0.0878661031084265\n",
      "860 - Loss_train: 0.0893964008642261, Loss_test: 0.08786313310301855\n",
      "861 - Loss_train: 0.08939309144989034, Loss_test: 0.0878601647231206\n",
      "862 - Loss_train: 0.08938978374666401, Loss_test: 0.08785719796649896\n",
      "863 - Loss_train: 0.08938647775710984, Loss_test: 0.08785423283856024\n",
      "864 - Loss_train: 0.08938317347951674, Loss_test: 0.0878512693352143\n",
      "865 - Loss_train: 0.08937987090945093, Loss_test: 0.08784830745248688\n",
      "866 - Loss_train: 0.08937657005185638, Loss_test: 0.08784534719654014\n",
      "867 - Loss_train: 0.08937327089885587, Loss_test: 0.08784238855870521\n",
      "868 - Loss_train: 0.0893699734569755, Loss_test: 0.08783943154506225\n",
      "869 - Loss_train: 0.08936667771780583, Loss_test: 0.0878364761483688\n",
      "870 - Loss_train: 0.08936338368367543, Loss_test: 0.08783352236855135\n",
      "871 - Loss_train: 0.0893600913548118, Loss_test: 0.08783057020999846\n",
      "872 - Loss_train: 0.08935680073133381, Loss_test: 0.08782761966817652\n",
      "873 - Loss_train: 0.08935351181113278, Loss_test: 0.08782467074466674\n",
      "874 - Loss_train: 0.08935022459168183, Loss_test: 0.08782172343600785\n",
      "875 - Loss_train: 0.08934693907147684, Loss_test: 0.08781877774011776\n",
      "876 - Loss_train: 0.08934365525407141, Loss_test: 0.08781583365877599\n",
      "877 - Loss_train: 0.08934037313810002, Loss_test: 0.08781289119503491\n",
      "878 - Loss_train: 0.08933709271844417, Loss_test: 0.08780995033957076\n",
      "879 - Loss_train: 0.08933381399539977, Loss_test: 0.08780701109577731\n",
      "880 - Loss_train: 0.08933053697120702, Loss_test: 0.08780407346381766\n",
      "881 - Loss_train: 0.08932726164145434, Loss_test: 0.08780113744059993\n",
      "882 - Loss_train: 0.08932398801059906, Loss_test: 0.08779820302788476\n",
      "883 - Loss_train: 0.089320716069562, Loss_test: 0.08779527022254661\n",
      "884 - Loss_train: 0.08931744582287642, Loss_test: 0.08779233902265342\n",
      "885 - Loss_train: 0.08931417726833704, Loss_test: 0.08778940943128823\n",
      "886 - Loss_train: 0.08931091040801249, Loss_test: 0.08778648144446136\n",
      "887 - Loss_train: 0.08930764523888572, Loss_test: 0.0877835550655091\n",
      "888 - Loss_train: 0.08930438175664833, Loss_test: 0.08778063028643136\n",
      "889 - Loss_train: 0.0893011199641577, Loss_test: 0.08777770711016371\n",
      "890 - Loss_train: 0.08929785985880154, Loss_test: 0.08777478553767905\n",
      "891 - Loss_train: 0.08929460143944953, Loss_test: 0.08777186556474789\n",
      "892 - Loss_train: 0.08929134470969351, Loss_test: 0.08776894719304754\n",
      "893 - Loss_train: 0.08928808966359907, Loss_test: 0.08776603042029801\n",
      "894 - Loss_train: 0.08928483630113151, Loss_test: 0.0877631152458008\n",
      "895 - Loss_train: 0.08928158462292386, Loss_test: 0.08776020166827152\n",
      "896 - Loss_train: 0.08927833463065617, Loss_test: 0.08775728969345337\n",
      "897 - Loss_train: 0.08927508632069509, Loss_test: 0.08775437931179944\n",
      "898 - Loss_train: 0.089271839688636, Loss_test: 0.08775147052477432\n",
      "899 - Loss_train: 0.08926859473746973, Loss_test: 0.08774856333198001\n",
      "900 - Loss_train: 0.0892653514670442, Loss_test: 0.08774565773352527\n",
      "901 - Loss_train: 0.08926210987461572, Loss_test: 0.08774275372730808\n",
      "902 - Loss_train: 0.08925886995877662, Loss_test: 0.08773985131298734\n",
      "903 - Loss_train: 0.0892556317213181, Loss_test: 0.08773695048923952\n",
      "904 - Loss_train: 0.08925239515949081, Loss_test: 0.08773405125900191\n",
      "905 - Loss_train: 0.08924916027073271, Loss_test: 0.08773115361334514\n",
      "906 - Loss_train: 0.08924592705706035, Loss_test: 0.08772825755798211\n",
      "907 - Loss_train: 0.08924269551791816, Loss_test: 0.08772536309168216\n",
      "908 - Loss_train: 0.08923946565256112, Loss_test: 0.08772247021154275\n",
      "909 - Loss_train: 0.08923623745537568, Loss_test: 0.08771957891620877\n",
      "910 - Loss_train: 0.08923301092935872, Loss_test: 0.08771668920632178\n",
      "911 - Loss_train: 0.08922978607361706, Loss_test: 0.0877138010805824\n",
      "912 - Loss_train: 0.08922656288708751, Loss_test: 0.08771091453878885\n",
      "913 - Loss_train: 0.0892233413692753, Loss_test: 0.08770802957992414\n",
      "914 - Loss_train: 0.08922012152128204, Loss_test: 0.0877051462054895\n",
      "915 - Loss_train: 0.08921690333659905, Loss_test: 0.08770226440819029\n",
      "916 - Loss_train: 0.08921368681844796, Loss_test: 0.0876993841934438\n",
      "917 - Loss_train: 0.08921047196378262, Loss_test: 0.08769650555481426\n",
      "918 - Loss_train: 0.08920725877324921, Loss_test: 0.08769362849686517\n",
      "919 - Loss_train: 0.08920404724892608, Loss_test: 0.08769075301853695\n",
      "920 - Loss_train: 0.08920083738676958, Loss_test: 0.08768787911742118\n",
      "921 - Loss_train: 0.08919762918590715, Loss_test: 0.08768500679206713\n",
      "922 - Loss_train: 0.08919442264711364, Loss_test: 0.08768213604200278\n",
      "923 - Loss_train: 0.08919121776240042, Loss_test: 0.08767926686350627\n",
      "924 - Loss_train: 0.0891880145378533, Loss_test: 0.08767639925808737\n",
      "925 - Loss_train: 0.0891848129745388, Loss_test: 0.08767353322982081\n",
      "926 - Loss_train: 0.08918161306756295, Loss_test: 0.0876706687696729\n",
      "927 - Loss_train: 0.08917841481507308, Loss_test: 0.08766780588346867\n",
      "928 - Loss_train: 0.08917521821808608, Loss_test: 0.08766494456522915\n",
      "929 - Loss_train: 0.08917202327522461, Loss_test: 0.08766208481497886\n",
      "930 - Loss_train: 0.08916882998372257, Loss_test: 0.08765922663344461\n",
      "931 - Loss_train: 0.0891656383459773, Loss_test: 0.0876563700165689\n",
      "932 - Loss_train: 0.08916244836179847, Loss_test: 0.08765351497280637\n",
      "933 - Loss_train: 0.08915926003055613, Loss_test: 0.08765066149223404\n",
      "934 - Loss_train: 0.08915607334644293, Loss_test: 0.0876478095777285\n",
      "935 - Loss_train: 0.08915288831195994, Loss_test: 0.08764495922592912\n",
      "936 - Loss_train: 0.08914970492573696, Loss_test: 0.08764211043716122\n",
      "937 - Loss_train: 0.08914652318684388, Loss_test: 0.08763926321220732\n",
      "938 - Loss_train: 0.08914334309915123, Loss_test: 0.08763641755151788\n",
      "939 - Loss_train: 0.08914016465341996, Loss_test: 0.08763357344809933\n",
      "940 - Loss_train: 0.08913698785245552, Loss_test: 0.08763073090665044\n",
      "941 - Loss_train: 0.08913381269657975, Loss_test: 0.08762788992206663\n",
      "942 - Loss_train: 0.08913063918415477, Loss_test: 0.08762505049890429\n",
      "943 - Loss_train: 0.08912746731628907, Loss_test: 0.08762221263244147\n",
      "944 - Loss_train: 0.08912429708733523, Loss_test: 0.08761937632289557\n",
      "945 - Loss_train: 0.08912112849932412, Loss_test: 0.08761654156834438\n",
      "946 - Loss_train: 0.08911796155096541, Loss_test: 0.08761370836833104\n",
      "947 - Loss_train: 0.08911479624330568, Loss_test: 0.0876108767260243\n",
      "948 - Loss_train: 0.089111632575471, Loss_test: 0.08760804663562022\n",
      "949 - Loss_train: 0.08910847054297148, Loss_test: 0.08760521809682173\n",
      "950 - Loss_train: 0.08910531014924426, Loss_test: 0.08760239111419116\n",
      "951 - Loss_train: 0.08910215139136338, Loss_test: 0.08759956568027198\n",
      "952 - Loss_train: 0.08909899426642523, Loss_test: 0.08759674179425102\n",
      "953 - Loss_train: 0.08909583877417604, Loss_test: 0.08759391945902073\n",
      "954 - Loss_train: 0.0890926849154988, Loss_test: 0.08759109867149066\n",
      "955 - Loss_train: 0.08908953269004533, Loss_test: 0.08758827943180551\n",
      "956 - Loss_train: 0.08908638209631341, Loss_test: 0.08758546173865404\n",
      "957 - Loss_train: 0.08908323313584329, Loss_test: 0.0875826455960946\n",
      "958 - Loss_train: 0.08908008580310231, Loss_test: 0.08757983099629196\n",
      "959 - Loss_train: 0.08907694009818257, Loss_test: 0.08757701793858946\n",
      "960 - Loss_train: 0.08907379602125125, Loss_test: 0.08757420642569509\n",
      "961 - Loss_train: 0.08907065357255767, Loss_test: 0.08757139645508621\n",
      "962 - Loss_train: 0.08906751275068736, Loss_test: 0.0875685880289092\n",
      "963 - Loss_train: 0.08906437355426806, Loss_test: 0.08756578114149224\n",
      "964 - Loss_train: 0.08906123598244282, Loss_test: 0.08756297579558105\n",
      "965 - Loss_train: 0.08905810003480677, Loss_test: 0.08756017198827215\n",
      "966 - Loss_train: 0.0890549657103167, Loss_test: 0.087557369720646\n",
      "967 - Loss_train: 0.08905183300780595, Loss_test: 0.0875545689901472\n",
      "968 - Loss_train: 0.08904870192632833, Loss_test: 0.08755176979707936\n",
      "969 - Loss_train: 0.08904557246559933, Loss_test: 0.08754897213908448\n",
      "970 - Loss_train: 0.08904244462531903, Loss_test: 0.08754617601861289\n",
      "971 - Loss_train: 0.0890393184074874, Loss_test: 0.08754338143537696\n",
      "972 - Loss_train: 0.08903619380447521, Loss_test: 0.08754058838369097\n",
      "973 - Loss_train: 0.08903307082211591, Loss_test: 0.08753779686744205\n",
      "974 - Loss_train: 0.08902994945386326, Loss_test: 0.08753500687984755\n",
      "975 - Loss_train: 0.08902682970355823, Loss_test: 0.08753221842844991\n",
      "976 - Loss_train: 0.0890237115647971, Loss_test: 0.08752943150273228\n",
      "977 - Loss_train: 0.08902059504315504, Loss_test: 0.08752664611187135\n",
      "978 - Loss_train: 0.08901748013244547, Loss_test: 0.08752386224600187\n",
      "979 - Loss_train: 0.08901436683331275, Loss_test: 0.08752107990848926\n",
      "980 - Loss_train: 0.08901125514582832, Loss_test: 0.08751829909699672\n",
      "981 - Loss_train: 0.08900814506906661, Loss_test: 0.08751551981247785\n",
      "982 - Loss_train: 0.08900503660324122, Loss_test: 0.08751274205631324\n",
      "983 - Loss_train: 0.08900192974480509, Loss_test: 0.08750996582403829\n",
      "984 - Loss_train: 0.0889988244959181, Loss_test: 0.08750719111463179\n",
      "985 - Loss_train: 0.08899572085339605, Loss_test: 0.08750441792948058\n",
      "986 - Loss_train: 0.08899261882056823, Loss_test: 0.08750164626873336\n",
      "987 - Loss_train: 0.08898951839045413, Loss_test: 0.08749887612852694\n",
      "988 - Loss_train: 0.08898641956809318, Loss_test: 0.08749610750926072\n",
      "989 - Loss_train: 0.08898332234639539, Loss_test: 0.0874933404100483\n",
      "990 - Loss_train: 0.08898022672848717, Loss_test: 0.08749057482834761\n",
      "991 - Loss_train: 0.08897713271562625, Loss_test: 0.08748781076978211\n",
      "992 - Loss_train: 0.0889740403014795, Loss_test: 0.08748504822463829\n",
      "993 - Loss_train: 0.08897094949158331, Loss_test: 0.08748228719897183\n",
      "994 - Loss_train: 0.08896786028105139, Loss_test: 0.087479527689481\n",
      "995 - Loss_train: 0.08896477266983747, Loss_test: 0.08747676969740008\n",
      "996 - Loss_train: 0.08896168665484003, Loss_test: 0.08747401321614479\n",
      "997 - Loss_train: 0.08895860223735517, Loss_test: 0.08747125824878538\n",
      "998 - Loss_train: 0.08895551941527029, Loss_test: 0.08746850479369042\n",
      "999 - Loss_train: 0.08895243818927541, Loss_test: 0.08746575284977758\n",
      "1000 - Loss_train: 0.08894935855822309, Loss_test: 0.08746300241820337\n",
      "1001 - Loss_train: 0.08894628052134985, Loss_test: 0.08746025349742859\n",
      "1002 - Loss_train: 0.08894320407899932, Loss_test: 0.08745750608569165\n",
      "1003 - Loss_train: 0.0889401292294137, Loss_test: 0.08745476018518912\n",
      "1004 - Loss_train: 0.08893705596987632, Loss_test: 0.08745201579004205\n",
      "1005 - Loss_train: 0.08893398430520956, Loss_test: 0.08744927290587473\n",
      "1006 - Loss_train: 0.08893091422697122, Loss_test: 0.08744653152462109\n",
      "1007 - Loss_train: 0.08892784574033814, Loss_test: 0.08744379165177354\n",
      "1008 - Loss_train: 0.08892477883841422, Loss_test: 0.08744105328020853\n",
      "1009 - Loss_train: 0.08892171352805246, Loss_test: 0.08743831641708834\n",
      "1010 - Loss_train: 0.08891864980459488, Loss_test: 0.08743558105637987\n",
      "1011 - Loss_train: 0.08891558766630321, Loss_test: 0.0874328471977914\n",
      "1012 - Loss_train: 0.08891252711364073, Loss_test: 0.08743011484107682\n",
      "1013 - Loss_train: 0.08890946814225908, Loss_test: 0.08742738398346515\n",
      "1014 - Loss_train: 0.08890641075268502, Loss_test: 0.0874246546245372\n",
      "1015 - Loss_train: 0.08890335494683847, Loss_test: 0.08742192676347384\n",
      "1016 - Loss_train: 0.08890030072266122, Loss_test: 0.08741920040277493\n",
      "1017 - Loss_train: 0.08889724807943733, Loss_test: 0.08741647554012504\n",
      "1018 - Loss_train: 0.08889419701762018, Loss_test: 0.08741375217314341\n",
      "1019 - Loss_train: 0.0888911475380392, Loss_test: 0.08741103030628972\n",
      "1020 - Loss_train: 0.08888809963427262, Loss_test: 0.08740830993012537\n",
      "1021 - Loss_train: 0.08888505330916231, Loss_test: 0.08740559105442226\n",
      "1022 - Loss_train: 0.0888820085612494, Loss_test: 0.08740287366668455\n",
      "1023 - Loss_train: 0.08887896538837581, Loss_test: 0.08740015777525653\n",
      "1024 - Loss_train: 0.0888759237939096, Loss_test: 0.08739744337687887\n",
      "1025 - Loss_train: 0.0888728837697151, Loss_test: 0.08739473046621844\n",
      "1026 - Loss_train: 0.08886984532300056, Loss_test: 0.08739201904889621\n",
      "1027 - Loss_train: 0.0888668084467015, Loss_test: 0.08738930912049923\n",
      "1028 - Loss_train: 0.08886377314205568, Loss_test: 0.08738660067925548\n",
      "1029 - Loss_train: 0.08886073940983415, Loss_test: 0.0873838937288582\n",
      "1030 - Loss_train: 0.08885770725021708, Loss_test: 0.08738118826620422\n",
      "1031 - Loss_train: 0.08885467665608375, Loss_test: 0.08737848428726418\n",
      "1032 - Loss_train: 0.0888516476346434, Loss_test: 0.08737578179719187\n",
      "1033 - Loss_train: 0.08884862017792271, Loss_test: 0.0873730807885819\n",
      "1034 - Loss_train: 0.08884559428918164, Loss_test: 0.08737038126599209\n",
      "1035 - Loss_train: 0.08884256996950841, Loss_test: 0.08736768322844071\n",
      "1036 - Loss_train: 0.08883954721157077, Loss_test: 0.08736498667143508\n",
      "1037 - Loss_train: 0.08883652601989496, Loss_test: 0.08736229159561168\n",
      "1038 - Loss_train: 0.0888335063948033, Loss_test: 0.08735959800589292\n",
      "1039 - Loss_train: 0.08883048833130458, Loss_test: 0.08735690588977138\n",
      "1040 - Loss_train: 0.08882747183268254, Loss_test: 0.08735421526240116\n",
      "1041 - Loss_train: 0.08882445689071465, Loss_test: 0.08735152610440389\n",
      "1042 - Loss_train: 0.08882144351270886, Loss_test: 0.08734883842884715\n",
      "1043 - Loss_train: 0.08881843169521264, Loss_test: 0.08734615223056176\n",
      "1044 - Loss_train: 0.08881542143724004, Loss_test: 0.08734346750918277\n",
      "1045 - Loss_train: 0.08881241273735903, Loss_test: 0.08734078426327731\n",
      "1046 - Loss_train: 0.08880940559578997, Loss_test: 0.08733810249210491\n",
      "1047 - Loss_train: 0.0888064000077621, Loss_test: 0.08733542219380756\n",
      "1048 - Loss_train: 0.08880339597545528, Loss_test: 0.0873327433672486\n",
      "1049 - Loss_train: 0.08880039349819427, Loss_test: 0.08733006601256137\n",
      "1050 - Loss_train: 0.08879739257546983, Loss_test: 0.08732739013134679\n",
      "1051 - Loss_train: 0.0887943932077834, Loss_test: 0.08732471572073486\n",
      "1052 - Loss_train: 0.08879139539126607, Loss_test: 0.08732204277957162\n",
      "1053 - Loss_train: 0.08878839912950215, Loss_test: 0.08731937131001305\n",
      "1054 - Loss_train: 0.08878540441853014, Loss_test: 0.08731670131101894\n",
      "1055 - Loss_train: 0.08878241125721391, Loss_test: 0.08731403277391162\n",
      "1056 - Loss_train: 0.08877941964643425, Loss_test: 0.08731136570897058\n",
      "1057 - Loss_train: 0.08877642958155664, Loss_test: 0.08730870010629486\n",
      "1058 - Loss_train: 0.0887734410639297, Loss_test: 0.08730603596874857\n",
      "1059 - Loss_train: 0.08877045409436972, Loss_test: 0.08730337329678792\n",
      "1060 - Loss_train: 0.08876746867432532, Loss_test: 0.08730071209124265\n",
      "1061 - Loss_train: 0.0887644847995634, Loss_test: 0.08729805234922401\n",
      "1062 - Loss_train: 0.08876150246700294, Loss_test: 0.08729539406523333\n",
      "1063 - Loss_train: 0.08875852168184893, Loss_test: 0.08729273724818384\n",
      "1064 - Loss_train: 0.08875554243570992, Loss_test: 0.08729008188869117\n",
      "1065 - Loss_train: 0.08875256473591124, Loss_test: 0.08728742798968983\n",
      "1066 - Loss_train: 0.08874958857692478, Loss_test: 0.08728477555255065\n",
      "1067 - Loss_train: 0.08874661395871712, Loss_test: 0.08728212457117224\n",
      "1068 - Loss_train: 0.08874364088077746, Loss_test: 0.08727947504825613\n",
      "1069 - Loss_train: 0.0887406693387762, Loss_test: 0.08727682698122737\n",
      "1070 - Loss_train: 0.08873769933822011, Loss_test: 0.08727418037119503\n",
      "1071 - Loss_train: 0.08873473087586951, Loss_test: 0.08727153521838991\n",
      "1072 - Loss_train: 0.08873176394596077, Loss_test: 0.08726889151514813\n",
      "1073 - Loss_train: 0.08872879855623228, Loss_test: 0.08726624926977847\n",
      "1074 - Loss_train: 0.088725834698309, Loss_test: 0.0872636084741941\n",
      "1075 - Loss_train: 0.08872287237711236, Loss_test: 0.08726096913425453\n",
      "1076 - Loss_train: 0.08871991158686786, Loss_test: 0.08725833124096251\n",
      "1077 - Loss_train: 0.08871695232900745, Loss_test: 0.08725569480055662\n",
      "1078 - Loss_train: 0.08871399460386575, Loss_test: 0.08725305980943236\n",
      "1079 - Loss_train: 0.08871103840940056, Loss_test: 0.08725042626668565\n",
      "1080 - Loss_train: 0.08870808374631717, Loss_test: 0.08724779417382929\n",
      "1081 - Loss_train: 0.08870513061290604, Loss_test: 0.08724516352736723\n",
      "1082 - Loss_train: 0.08870217900815804, Loss_test: 0.08724253433003759\n",
      "1083 - Loss_train: 0.08869922893202993, Loss_test: 0.08723990657647085\n",
      "1084 - Loss_train: 0.08869628038407794, Loss_test: 0.08723728027105751\n",
      "1085 - Loss_train: 0.08869333336123382, Loss_test: 0.0872346554081451\n",
      "1086 - Loss_train: 0.08869038786378376, Loss_test: 0.0872320319885616\n",
      "1087 - Loss_train: 0.08868744389177305, Loss_test: 0.08722941001370421\n",
      "1088 - Loss_train: 0.08868450144381075, Loss_test: 0.08722678947932025\n",
      "1089 - Loss_train: 0.08868156052206605, Loss_test: 0.08722417038889288\n",
      "1090 - Loss_train: 0.08867862112019317, Loss_test: 0.08722155273855804\n",
      "1091 - Loss_train: 0.08867568324098925, Loss_test: 0.08721893652816363\n",
      "1092 - Loss_train: 0.0886727468843291, Loss_test: 0.08721632175812033\n",
      "1093 - Loss_train: 0.08866981204566787, Loss_test: 0.08721370842324426\n",
      "1094 - Loss_train: 0.08866687872898787, Loss_test: 0.0872110965312451\n",
      "1095 - Loss_train: 0.08866394692847371, Loss_test: 0.0872084860701266\n",
      "1096 - Loss_train: 0.08866101664666601, Loss_test: 0.08720587704979896\n",
      "1097 - Loss_train: 0.08865808788086339, Loss_test: 0.08720326946096445\n",
      "1098 - Loss_train: 0.08865516063125026, Loss_test: 0.0872006633089299\n",
      "1099 - Loss_train: 0.08865223490023763, Loss_test: 0.08719805859244519\n",
      "1100 - Loss_train: 0.08864931068383504, Loss_test: 0.08719545530904957\n",
      "1101 - Loss_train: 0.08864638797868965, Loss_test: 0.08719285345476753\n",
      "1102 - Loss_train: 0.08864346678672691, Loss_test: 0.08719025303340756\n",
      "1103 - Loss_train: 0.0886405471094762, Loss_test: 0.08718765404306009\n",
      "1104 - Loss_train: 0.08863762894233447, Loss_test: 0.0871850564836766\n",
      "1105 - Loss_train: 0.08863471228555185, Loss_test: 0.0871824603517151\n",
      "1106 - Loss_train: 0.08863179713858887, Loss_test: 0.08717986564749729\n",
      "1107 - Loss_train: 0.08862888350446284, Loss_test: 0.08717727237532104\n",
      "1108 - Loss_train: 0.08862597137539734, Loss_test: 0.08717468052658778\n",
      "1109 - Loss_train: 0.08862306075577979, Loss_test: 0.08717209010456439\n",
      "1110 - Loss_train: 0.08862015164115748, Loss_test: 0.08716950111053916\n",
      "1111 - Loss_train: 0.08861724403540652, Loss_test: 0.0871669135403868\n",
      "1112 - Loss_train: 0.08861433793237622, Loss_test: 0.08716432739044885\n",
      "1113 - Loss_train: 0.08861143333303662, Loss_test: 0.08716174266644267\n",
      "1114 - Loss_train: 0.08860853023899494, Loss_test: 0.08715915936330236\n",
      "1115 - Loss_train: 0.08860562864747049, Loss_test: 0.08715657748312362\n",
      "1116 - Loss_train: 0.08860272855875265, Loss_test: 0.08715399702292388\n",
      "1117 - Loss_train: 0.08859982997193408, Loss_test: 0.08715141798495886\n",
      "1118 - Loss_train: 0.08859693288786939, Loss_test: 0.08714884036734398\n",
      "1119 - Loss_train: 0.0885940373039977, Loss_test: 0.08714626416950418\n",
      "1120 - Loss_train: 0.08859114321652246, Loss_test: 0.08714368938570576\n",
      "1121 - Loss_train: 0.08858825062750988, Loss_test: 0.08714111601858392\n",
      "1122 - Loss_train: 0.0885853595371631, Loss_test: 0.08713854407140714\n",
      "1123 - Loss_train: 0.08858246994439399, Loss_test: 0.08713597353816743\n",
      "1124 - Loss_train: 0.08857958184860379, Loss_test: 0.08713340442210502\n",
      "1125 - Loss_train: 0.08857669524758069, Loss_test: 0.08713083671886668\n",
      "1126 - Loss_train: 0.08857381013928285, Loss_test: 0.08712827042756134\n",
      "1127 - Loss_train: 0.08857092652413687, Loss_test: 0.08712570554930095\n",
      "1128 - Loss_train: 0.08856804440353146, Loss_test: 0.08712314208260127\n",
      "1129 - Loss_train: 0.08856516377783646, Loss_test: 0.08712058003204122\n",
      "1130 - Loss_train: 0.08856228463911923, Loss_test: 0.08711801938499202\n",
      "1131 - Loss_train: 0.08855940699223391, Loss_test: 0.08711546015182307\n",
      "1132 - Loss_train: 0.08855653083409087, Loss_test: 0.08711290232196427\n",
      "1133 - Loss_train: 0.08855365616691893, Loss_test: 0.08711034590680981\n",
      "1134 - Loss_train: 0.08855078298999862, Loss_test: 0.08710779089775522\n",
      "1135 - Loss_train: 0.08854791129757238, Loss_test: 0.0871052372934548\n",
      "1136 - Loss_train: 0.08854504109235369, Loss_test: 0.087102685095426\n",
      "1137 - Loss_train: 0.08854217237624039, Loss_test: 0.08710013430522885\n",
      "1138 - Loss_train: 0.08853930514189502, Loss_test: 0.08709758491567988\n",
      "1139 - Loss_train: 0.08853643939263581, Loss_test: 0.08709503693135986\n",
      "1140 - Loss_train: 0.08853357512748648, Loss_test: 0.08709249034980249\n",
      "1141 - Loss_train: 0.08853071234789701, Loss_test: 0.08708994517223313\n",
      "1142 - Loss_train: 0.08852785104859652, Loss_test: 0.0870874013947582\n",
      "1143 - Loss_train: 0.08852499123151832, Loss_test: 0.087084859019629\n",
      "1144 - Loss_train: 0.0885221328939133, Loss_test: 0.08708231804082234\n",
      "1145 - Loss_train: 0.08851927603644494, Loss_test: 0.08707977846319555\n",
      "1146 - Loss_train: 0.0885164206578865, Loss_test: 0.08707724028415555\n",
      "1147 - Loss_train: 0.08851356676109542, Loss_test: 0.08707470350471905\n",
      "1148 - Loss_train: 0.08851071434006201, Loss_test: 0.0870721681211642\n",
      "1149 - Loss_train: 0.08850786339437491, Loss_test: 0.0870696341330942\n",
      "1150 - Loss_train: 0.08850501392451915, Loss_test: 0.08706710153853897\n",
      "1151 - Loss_train: 0.08850216593144457, Loss_test: 0.08706457034165538\n",
      "1152 - Loss_train: 0.08849931941223574, Loss_test: 0.0870620405368606\n",
      "1153 - Loss_train: 0.08849647436647315, Loss_test: 0.08705951212574652\n",
      "1154 - Loss_train: 0.08849363079567402, Loss_test: 0.08705698510825352\n",
      "1155 - Loss_train: 0.08849078869491528, Loss_test: 0.08705445948153054\n",
      "1156 - Loss_train: 0.08848794806771945, Loss_test: 0.08705193524685831\n",
      "1157 - Loss_train: 0.08848510891086549, Loss_test: 0.08704941240252408\n",
      "1158 - Loss_train: 0.08848227122444587, Loss_test: 0.08704689094683113\n",
      "1159 - Loss_train: 0.08847943500535897, Loss_test: 0.08704437088111647\n",
      "1160 - Loss_train: 0.0884766002557615, Loss_test: 0.08704185220048896\n",
      "1161 - Loss_train: 0.08847376697403139, Loss_test: 0.08703933491001671\n",
      "1162 - Loss_train: 0.08847093515920192, Loss_test: 0.08703681900489826\n",
      "1163 - Loss_train: 0.08846810481230617, Loss_test: 0.08703430448704799\n",
      "1164 - Loss_train: 0.08846527593216075, Loss_test: 0.08703179135640907\n",
      "1165 - Loss_train: 0.08846244851532936, Loss_test: 0.0870292796075961\n",
      "1166 - Loss_train: 0.0884596225644064, Loss_test: 0.08702676924549697\n",
      "1167 - Loss_train: 0.08845679807366053, Loss_test: 0.0870242602628719\n",
      "1168 - Loss_train: 0.08845397504549778, Loss_test: 0.0870217526620127\n",
      "1169 - Loss_train: 0.08845115347911373, Loss_test: 0.08701924644498583\n",
      "1170 - Loss_train: 0.0884483333745008, Loss_test: 0.08701674160745318\n",
      "1171 - Loss_train: 0.08844551472866512, Loss_test: 0.08701423814894886\n",
      "1172 - Loss_train: 0.08844269754295508, Loss_test: 0.08701173606827817\n",
      "1173 - Loss_train: 0.08843988181856077, Loss_test: 0.08700923537274964\n",
      "1174 - Loss_train: 0.08843706755208353, Loss_test: 0.08700673605196649\n",
      "1175 - Loss_train: 0.08843425474037271, Loss_test: 0.08700423810667834\n",
      "1176 - Loss_train: 0.0884314433851021, Loss_test: 0.087001741537735\n",
      "1177 - Loss_train: 0.0884286334848647, Loss_test: 0.0869992463451029\n",
      "1178 - Loss_train: 0.08842582504252862, Loss_test: 0.08699675252774927\n",
      "1179 - Loss_train: 0.0884230180522749, Loss_test: 0.08699426008374372\n",
      "1180 - Loss_train: 0.08842021251494295, Loss_test: 0.08699176901167514\n",
      "1181 - Loss_train: 0.08841740843334447, Loss_test: 0.08698927931790185\n",
      "1182 - Loss_train: 0.0884146058039242, Loss_test: 0.08698679099305046\n",
      "1183 - Loss_train: 0.08841180462660136, Loss_test: 0.08698430404031342\n",
      "1184 - Loss_train: 0.08840900489622554, Loss_test: 0.08698181845791542\n",
      "1185 - Loss_train: 0.08840620661507385, Loss_test: 0.08697933424223635\n",
      "1186 - Loss_train: 0.0884034097832277, Loss_test: 0.08697685139756872\n",
      "1187 - Loss_train: 0.08840061440061996, Loss_test: 0.08697436992049941\n",
      "1188 - Loss_train: 0.08839782046481087, Loss_test: 0.08697188981227515\n",
      "1189 - Loss_train: 0.08839502797689791, Loss_test: 0.08696941106965274\n",
      "1190 - Loss_train: 0.08839223693625255, Loss_test: 0.08696693369730729\n",
      "1191 - Loss_train: 0.08838944733828498, Loss_test: 0.08696445768451344\n",
      "1192 - Loss_train: 0.08838665918489291, Loss_test: 0.0869619830379795\n",
      "1193 - Loss_train: 0.08838387247520388, Loss_test: 0.08695950975584756\n",
      "1194 - Loss_train: 0.08838108720851694, Loss_test: 0.08695703783582742\n",
      "1195 - Loss_train: 0.08837830338300806, Loss_test: 0.08695456728064599\n",
      "1196 - Loss_train: 0.08837552100033999, Loss_test: 0.08695209808329575\n",
      "1197 - Loss_train: 0.08837274005720273, Loss_test: 0.08694963025145747\n",
      "1198 - Loss_train: 0.08836996055474602, Loss_test: 0.08694716377594995\n",
      "1199 - Loss_train: 0.08836718249332541, Loss_test: 0.08694469866394457\n",
      "1200 - Loss_train: 0.08836440586901707, Loss_test: 0.08694223490849112\n",
      "1201 - Loss_train: 0.08836163068383208, Loss_test: 0.08693977251367707\n",
      "1202 - Loss_train: 0.08835885693713608, Loss_test: 0.0869373114759307\n",
      "1203 - Loss_train: 0.08835608462698147, Loss_test: 0.0869348517966855\n",
      "1204 - Loss_train: 0.08835331375235499, Loss_test: 0.08693239347094166\n",
      "1205 - Loss_train: 0.08835054431248428, Loss_test: 0.0869299365038485\n",
      "1206 - Loss_train: 0.08834777630369994, Loss_test: 0.08692748088798914\n",
      "1207 - Loss_train: 0.08834500972868042, Loss_test: 0.08692502662383253\n",
      "1208 - Loss_train: 0.08834224458645236, Loss_test: 0.08692257371506788\n",
      "1209 - Loss_train: 0.08833948087595997, Loss_test: 0.08692012215753354\n",
      "1210 - Loss_train: 0.08833671859769744, Loss_test: 0.08691767195350428\n",
      "1211 - Loss_train: 0.08833395775155256, Loss_test: 0.08691522310086702\n",
      "1212 - Loss_train: 0.0883311983324405, Loss_test: 0.08691277559814235\n",
      "1213 - Loss_train: 0.08832844034221429, Loss_test: 0.0869103294427434\n",
      "1214 - Loss_train: 0.08832568377995477, Loss_test: 0.08690788463686215\n",
      "1215 - Loss_train: 0.08832292864767144, Loss_test: 0.08690544118149555\n",
      "1216 - Loss_train: 0.0883201749428088, Loss_test: 0.08690299907294484\n",
      "1217 - Loss_train: 0.0883174226624096, Loss_test: 0.0869005583116228\n",
      "1218 - Loss_train: 0.08831467180698567, Loss_test: 0.08689811889480649\n",
      "1219 - Loss_train: 0.08831192237581477, Loss_test: 0.08689568082249298\n",
      "1220 - Loss_train: 0.08830917436685057, Loss_test: 0.08689324409471023\n",
      "1221 - Loss_train: 0.08830642778174941, Loss_test: 0.08689080871067574\n",
      "1222 - Loss_train: 0.08830368261959767, Loss_test: 0.08688837466980312\n",
      "1223 - Loss_train: 0.08830093888064651, Loss_test: 0.08688594197442995\n",
      "1224 - Loss_train: 0.08829819655984321, Loss_test: 0.08688351061658695\n",
      "1225 - Loss_train: 0.08829545565953628, Loss_test: 0.08688108059787268\n",
      "1226 - Loss_train: 0.08829271618229592, Loss_test: 0.08687865192926181\n",
      "1227 - Loss_train: 0.0882899781188298, Loss_test: 0.08687622459114178\n",
      "1228 - Loss_train: 0.0882872414770494, Loss_test: 0.08687379859665022\n",
      "1229 - Loss_train: 0.08828450624918073, Loss_test: 0.08687137393650682\n",
      "1230 - Loss_train: 0.08828177243779588, Loss_test: 0.08686895061447715\n",
      "1231 - Loss_train: 0.0882790400435235, Loss_test: 0.08686652862875495\n",
      "1232 - Loss_train: 0.08827630906568937, Loss_test: 0.08686410798414956\n",
      "1233 - Loss_train: 0.08827357949954592, Loss_test: 0.0868616886684861\n",
      "1234 - Loss_train: 0.0882708513488062, Loss_test: 0.08685927069087244\n",
      "1235 - Loss_train: 0.08826812460938803, Loss_test: 0.0868568540447972\n",
      "1236 - Loss_train: 0.08826539928457032, Loss_test: 0.08685443873424263\n",
      "1237 - Loss_train: 0.08826267536779393, Loss_test: 0.08685202475493942\n",
      "1238 - Loss_train: 0.08825995286229914, Loss_test: 0.08684961210612578\n",
      "1239 - Loss_train: 0.0882572317693446, Loss_test: 0.08684720079074751\n",
      "1240 - Loss_train: 0.08825451208261581, Loss_test: 0.08684479080351512\n",
      "1241 - Loss_train: 0.08825179380444015, Loss_test: 0.08684238214628386\n",
      "1242 - Loss_train: 0.08824907693329913, Loss_test: 0.08683997481664976\n",
      "1243 - Loss_train: 0.08824636146976529, Loss_test: 0.08683756881736862\n",
      "1244 - Loss_train: 0.08824364741232907, Loss_test: 0.08683516414236571\n",
      "1245 - Loss_train: 0.08824093475977855, Loss_test: 0.08683276079615346\n",
      "1246 - Loss_train: 0.08823822351271854, Loss_test: 0.08683035877620474\n",
      "1247 - Loss_train: 0.08823551366987402, Loss_test: 0.08682795808028348\n",
      "1248 - Loss_train: 0.08823280523247368, Loss_test: 0.08682555871274762\n",
      "1249 - Loss_train: 0.08823009819626462, Loss_test: 0.08682316066659437\n",
      "1250 - Loss_train: 0.08822739256154537, Loss_test: 0.08682076394267652\n",
      "1251 - Loss_train: 0.0882246883271463, Loss_test: 0.08681836854310852\n",
      "1252 - Loss_train: 0.08822198549593772, Loss_test: 0.08681597446524922\n",
      "1253 - Loss_train: 0.08821928406529175, Loss_test: 0.08681358171039683\n",
      "1254 - Loss_train: 0.08821658402983394, Loss_test: 0.08681119027346863\n",
      "1255 - Loss_train: 0.08821388539330668, Loss_test: 0.08680880015532419\n",
      "1256 - Loss_train: 0.0882111881574162, Loss_test: 0.08680641135975879\n",
      "1257 - Loss_train: 0.08820849231513064, Loss_test: 0.08680402388007104\n",
      "1258 - Loss_train: 0.08820579786920399, Loss_test: 0.08680163771631479\n",
      "1259 - Loss_train: 0.08820310481977478, Loss_test: 0.08679925287165953\n",
      "1260 - Loss_train: 0.08820041316547846, Loss_test: 0.08679686934331107\n",
      "1261 - Loss_train: 0.08819772290582181, Loss_test: 0.08679448713180726\n",
      "1262 - Loss_train: 0.08819503403721829, Loss_test: 0.08679210623145409\n",
      "1263 - Loss_train: 0.08819234656341976, Loss_test: 0.08678972664931905\n",
      "1264 - Loss_train: 0.08818966047989246, Loss_test: 0.08678734837759129\n",
      "1265 - Loss_train: 0.08818697578652242, Loss_test: 0.08678497141865642\n",
      "1266 - Loss_train: 0.08818429248437339, Loss_test: 0.08678259577262026\n",
      "1267 - Loss_train: 0.08818161057384866, Loss_test: 0.08678022143822166\n",
      "1268 - Loss_train: 0.08817893005201398, Loss_test: 0.08677784841682863\n",
      "1269 - Loss_train: 0.08817625091579351, Loss_test: 0.08677547670065497\n",
      "1270 - Loss_train: 0.08817357316968238, Loss_test: 0.08677310629724941\n",
      "1271 - Loss_train: 0.08817089681124082, Loss_test: 0.0867707372021938\n",
      "1272 - Loss_train: 0.08816822183796104, Loss_test: 0.08676836941552736\n",
      "1273 - Loss_train: 0.08816554824773967, Loss_test: 0.08676600293215295\n",
      "1274 - Loss_train: 0.08816287604308852, Loss_test: 0.08676363775706106\n",
      "1275 - Loss_train: 0.08816020522094836, Loss_test: 0.08676127388694171\n",
      "1276 - Loss_train: 0.08815753578445348, Loss_test: 0.08675891132449536\n",
      "1277 - Loss_train: 0.08815486773088603, Loss_test: 0.0867565500642095\n",
      "1278 - Loss_train: 0.08815220105904883, Loss_test: 0.08675419011015596\n",
      "1279 - Loss_train: 0.08814953576660757, Loss_test: 0.08675183145605127\n",
      "1280 - Loss_train: 0.08814687185342705, Loss_test: 0.08674947410433967\n",
      "1281 - Loss_train: 0.0881442093200887, Loss_test: 0.08674711805385102\n",
      "1282 - Loss_train: 0.08814154816596745, Loss_test: 0.0867447633042463\n",
      "1283 - Loss_train: 0.08813888839267801, Loss_test: 0.08674240985746083\n",
      "1284 - Loss_train: 0.08813622999498708, Loss_test: 0.08674005770801868\n",
      "1285 - Loss_train: 0.08813357297301788, Loss_test: 0.08673770685838007\n",
      "1286 - Loss_train: 0.08813091733028029, Loss_test: 0.08673535730748498\n",
      "1287 - Loss_train: 0.08812826306225724, Loss_test: 0.08673300905297233\n",
      "1288 - Loss_train: 0.08812561016648707, Loss_test: 0.08673066209510191\n",
      "1289 - Loss_train: 0.08812295864501472, Loss_test: 0.08672831643302067\n",
      "1290 - Loss_train: 0.08812030849610382, Loss_test: 0.08672597206488832\n",
      "1291 - Loss_train: 0.0881176597199712, Loss_test: 0.08672362899210821\n",
      "1292 - Loss_train: 0.08811501231529467, Loss_test: 0.08672128721261624\n",
      "1293 - Loss_train: 0.08811236628263677, Loss_test: 0.08671894672770329\n",
      "1294 - Loss_train: 0.08810972162273101, Loss_test: 0.08671660753763384\n",
      "1295 - Loss_train: 0.0881070783298104, Loss_test: 0.08671426963595505\n",
      "1296 - Loss_train: 0.08810443640563156, Loss_test: 0.08671193302540757\n",
      "1297 - Loss_train: 0.0881017958496542, Loss_test: 0.08670959770661854\n",
      "1298 - Loss_train: 0.08809915666164958, Loss_test: 0.08670726367724353\n",
      "1299 - Loss_train: 0.08809651884045491, Loss_test: 0.08670493093725823\n",
      "1300 - Loss_train: 0.08809388238534127, Loss_test: 0.08670259948372648\n",
      "1301 - Loss_train: 0.08809124729554925, Loss_test: 0.08670026932171378\n",
      "1302 - Loss_train: 0.08808861357121187, Loss_test: 0.08669794044320327\n",
      "1303 - Loss_train: 0.08808598121150042, Loss_test: 0.08669561285645544\n",
      "1304 - Loss_train: 0.08808335021394785, Loss_test: 0.08669328654980382\n",
      "1305 - Loss_train: 0.08808072058005934, Loss_test: 0.08669096153147764\n",
      "1306 - Loss_train: 0.08807809230658416, Loss_test: 0.08668863779532969\n",
      "1307 - Loss_train: 0.08807546539500535, Loss_test: 0.08668631534394793\n",
      "1308 - Loss_train: 0.08807283984641306, Loss_test: 0.08668399417739235\n",
      "1309 - Loss_train: 0.0880702156545893, Loss_test: 0.08668167429002396\n",
      "1310 - Loss_train: 0.08806759282557405, Loss_test: 0.08667935568845501\n",
      "1311 - Loss_train: 0.08806497135197554, Loss_test: 0.08667703836481343\n",
      "1312 - Loss_train: 0.08806235123727436, Loss_test: 0.0866747223219599\n",
      "1313 - Loss_train: 0.08805973248051398, Loss_test: 0.08667240756088147\n",
      "1314 - Loss_train: 0.08805711507740455, Loss_test: 0.08667009407487515\n",
      "1315 - Loss_train: 0.08805449903021488, Loss_test: 0.08666778186739157\n",
      "1316 - Loss_train: 0.0880518843396436, Loss_test: 0.08666547093972861\n",
      "1317 - Loss_train: 0.08804927100267092, Loss_test: 0.08666316128826458\n",
      "1318 - Loss_train: 0.08804665902095052, Loss_test: 0.08666085291344995\n",
      "1319 - Loss_train: 0.08804404839198299, Loss_test: 0.08665854581494281\n",
      "1320 - Loss_train: 0.08804143911239751, Loss_test: 0.08665623999120706\n",
      "1321 - Loss_train: 0.08803883118691866, Loss_test: 0.08665393543915455\n",
      "1322 - Loss_train: 0.08803622460963863, Loss_test: 0.0866516321628932\n",
      "1323 - Loss_train: 0.08803361938218564, Loss_test: 0.08664933015565797\n",
      "1324 - Loss_train: 0.08803101550446633, Loss_test: 0.08664702942221345\n",
      "1325 - Loss_train: 0.08802841297551094, Loss_test: 0.08664472995910985\n",
      "1326 - Loss_train: 0.08802581179444047, Loss_test: 0.0866424317685416\n",
      "1327 - Loss_train: 0.08802321196086046, Loss_test: 0.08664013484850007\n",
      "1328 - Loss_train: 0.08802061347323793, Loss_test: 0.08663783919692684\n",
      "1329 - Loss_train: 0.08801801633332737, Loss_test: 0.08663554481353662\n",
      "1330 - Loss_train: 0.08801542053718472, Loss_test: 0.08663325170083228\n",
      "1331 - Loss_train: 0.08801282608629497, Loss_test: 0.08663095985290192\n",
      "1332 - Loss_train: 0.08801023297820487, Loss_test: 0.0866286692722706\n",
      "1333 - Loss_train: 0.08800764121284486, Loss_test: 0.08662637995758762\n",
      "1334 - Loss_train: 0.08800505079031543, Loss_test: 0.08662409190717225\n",
      "1335 - Loss_train: 0.08800246171074014, Loss_test: 0.08662180512365678\n",
      "1336 - Loss_train: 0.08799987397295365, Loss_test: 0.08661951960492043\n",
      "1337 - Loss_train: 0.08799728757298338, Loss_test: 0.0866172353464455\n",
      "1338 - Loss_train: 0.08799470251412836, Loss_test: 0.08661495235284589\n",
      "1339 - Loss_train: 0.08799211879546126, Loss_test: 0.08661267062231787\n",
      "1340 - Loss_train: 0.08798953641215569, Loss_test: 0.08661039014999959\n",
      "1341 - Loss_train: 0.08798695536700796, Loss_test: 0.08660811093824204\n",
      "1342 - Loss_train: 0.08798437566119603, Loss_test: 0.08660583299001907\n",
      "1343 - Loss_train: 0.08798179728855222, Loss_test: 0.08660355629877178\n",
      "1344 - Loss_train: 0.08797922025111703, Loss_test: 0.08660128086530781\n",
      "1345 - Loss_train: 0.08797664454915533, Loss_test: 0.08659900668953271\n",
      "1346 - Loss_train: 0.08797407018350649, Loss_test: 0.08659673377463761\n",
      "1347 - Loss_train: 0.08797149714883999, Loss_test: 0.08659446211276181\n",
      "1348 - Loss_train: 0.08796892544729079, Loss_test: 0.08659219170717337\n",
      "1349 - Loss_train: 0.08796635507777963, Loss_test: 0.08658992255789369\n",
      "1350 - Loss_train: 0.0879637860391419, Loss_test: 0.08658765466107643\n",
      "1351 - Loss_train: 0.08796121833390987, Loss_test: 0.08658538802288315\n",
      "1352 - Loss_train: 0.08795865195880431, Loss_test: 0.08658312263647368\n",
      "1353 - Loss_train: 0.0879560869105056, Loss_test: 0.08658085850041272\n",
      "1354 - Loss_train: 0.08795352319141765, Loss_test: 0.08657859561845418\n",
      "1355 - Loss_train: 0.08795096080176475, Loss_test: 0.0865763339881024\n",
      "1356 - Loss_train: 0.08794839973679922, Loss_test: 0.0865740736054751\n",
      "1357 - Loss_train: 0.08794583999771863, Loss_test: 0.08657181447350938\n",
      "1358 - Loss_train: 0.08794328158541917, Loss_test: 0.08656955659214821\n",
      "1359 - Loss_train: 0.08794072450128856, Loss_test: 0.086567299958738\n",
      "1360 - Loss_train: 0.08793816873898785, Loss_test: 0.08656504457423976\n",
      "1361 - Loss_train: 0.0879356143018593, Loss_test: 0.08656279043562105\n",
      "1362 - Loss_train: 0.0879330611858969, Loss_test: 0.08656053754454401\n",
      "1363 - Loss_train: 0.08793050939279756, Loss_test: 0.08655828589811776\n",
      "1364 - Loss_train: 0.08792795892110075, Loss_test: 0.08655603549581367\n",
      "1365 - Loss_train: 0.08792540977021486, Loss_test: 0.08655378634049014\n",
      "1366 - Loss_train: 0.08792286194114442, Loss_test: 0.0865515384272957\n",
      "1367 - Loss_train: 0.08792031543312227, Loss_test: 0.08654929176274254\n",
      "1368 - Loss_train: 0.08791777024174743, Loss_test: 0.08654704633895713\n",
      "1369 - Loss_train: 0.08791522637129935, Loss_test: 0.08654480215233253\n",
      "1370 - Loss_train: 0.08791268381514818, Loss_test: 0.08654255921070941\n",
      "1371 - Loss_train: 0.08791014257932668, Loss_test: 0.08654031750928866\n",
      "1372 - Loss_train: 0.08790760266016202, Loss_test: 0.08653807704967285\n",
      "1373 - Loss_train: 0.08790506405351622, Loss_test: 0.08653583782597683\n",
      "1374 - Loss_train: 0.08790252676462537, Loss_test: 0.08653359984311695\n",
      "1375 - Loss_train: 0.08789999078648, Loss_test: 0.08653136309703219\n",
      "1376 - Loss_train: 0.08789745612279316, Loss_test: 0.08652912758775083\n",
      "1377 - Loss_train: 0.08789492277198499, Loss_test: 0.08652689331370596\n",
      "1378 - Loss_train: 0.08789239073304111, Loss_test: 0.08652466027772641\n",
      "1379 - Loss_train: 0.08788986000589649, Loss_test: 0.0865224284773853\n",
      "1380 - Loss_train: 0.08788733059024259, Loss_test: 0.08652019791003249\n",
      "1381 - Loss_train: 0.08788480248534725, Loss_test: 0.08651796857906704\n",
      "1382 - Loss_train: 0.08788227568935049, Loss_test: 0.08651574048093064\n",
      "1383 - Loss_train: 0.08787975020203573, Loss_test: 0.08651351361532808\n",
      "1384 - Loss_train: 0.08787722602495777, Loss_test: 0.08651128798293034\n",
      "1385 - Loss_train: 0.08787470315338676, Loss_test: 0.08650906357991214\n",
      "1386 - Loss_train: 0.0878721815881976, Loss_test: 0.08650684040913545\n",
      "1387 - Loss_train: 0.08786966133291785, Loss_test: 0.08650461846849111\n",
      "1388 - Loss_train: 0.08786714238184525, Loss_test: 0.08650239776006942\n",
      "1389 - Loss_train: 0.08786462473300605, Loss_test: 0.08650017827445472\n",
      "1390 - Loss_train: 0.08786210839099941, Loss_test: 0.08649796002232155\n",
      "1391 - Loss_train: 0.08785959335213832, Loss_test: 0.08649574299691327\n",
      "1392 - Loss_train: 0.08785707961386838, Loss_test: 0.0864935271965153\n",
      "1393 - Loss_train: 0.08785456717714414, Loss_test: 0.0864913126193811\n",
      "1394 - Loss_train: 0.08785205604478692, Loss_test: 0.08648909927263965\n",
      "1395 - Loss_train: 0.0878495462100201, Loss_test: 0.0864868871473153\n",
      "1396 - Loss_train: 0.08784703767604037, Loss_test: 0.08648467624701421\n",
      "1397 - Loss_train: 0.08784453044389282, Loss_test: 0.08648246657239306\n",
      "1398 - Loss_train: 0.08784202451059718, Loss_test: 0.08648025812183786\n",
      "1399 - Loss_train: 0.08783951987207173, Loss_test: 0.0864780508885924\n",
      "1400 - Loss_train: 0.0878370165318461, Loss_test: 0.0864758448777721\n",
      "1401 - Loss_train: 0.08783451448832236, Loss_test: 0.08647364008936267\n",
      "1402 - Loss_train: 0.08783201374225225, Loss_test: 0.08647143652151337\n",
      "1403 - Loss_train: 0.08782951428970706, Loss_test: 0.0864692341737764\n",
      "1404 - Loss_train: 0.0878270161325019, Loss_test: 0.08646703304227527\n",
      "1405 - Loss_train: 0.08782451927199109, Loss_test: 0.08646483313270971\n",
      "1406 - Loss_train: 0.08782202370375874, Loss_test: 0.08646263444231801\n",
      "1407 - Loss_train: 0.08781952942692052, Loss_test: 0.08646043696547129\n",
      "1408 - Loss_train: 0.08781703644260531, Loss_test: 0.08645824070621495\n",
      "1409 - Loss_train: 0.08781454475142803, Loss_test: 0.08645604566274055\n",
      "1410 - Loss_train: 0.08781205434844391, Loss_test: 0.08645385183335129\n",
      "1411 - Loss_train: 0.08780956523562324, Loss_test: 0.08645165921900488\n",
      "1412 - Loss_train: 0.08780707741398025, Loss_test: 0.0864494678196694\n",
      "1413 - Loss_train: 0.08780459087992372, Loss_test: 0.08644727763127676\n",
      "1414 - Loss_train: 0.0878021056342843, Loss_test: 0.08644508865840916\n",
      "1415 - Loss_train: 0.08779962167463798, Loss_test: 0.08644290089220368\n",
      "1416 - Loss_train: 0.08779713900209077, Loss_test: 0.08644071434210623\n",
      "1417 - Loss_train: 0.0877946576158771, Loss_test: 0.08643852899995366\n",
      "1418 - Loss_train: 0.0877921775148814, Loss_test: 0.08643634486792737\n",
      "1419 - Loss_train: 0.0877896987003854, Loss_test: 0.08643416194792809\n",
      "1420 - Loss_train: 0.08778722116824228, Loss_test: 0.08643198023336487\n",
      "1421 - Loss_train: 0.0877847449193743, Loss_test: 0.08642979972856499\n",
      "1422 - Loss_train: 0.08778226995603652, Loss_test: 0.0864276204308662\n",
      "1423 - Loss_train: 0.08777979627506487, Loss_test: 0.08642544234410207\n",
      "1424 - Loss_train: 0.08777732387501753, Loss_test: 0.08642326545913014\n",
      "1425 - Loss_train: 0.08777485275354172, Loss_test: 0.08642108978095855\n",
      "1426 - Loss_train: 0.08777238291421938, Loss_test: 0.0864189153074165\n",
      "1427 - Loss_train: 0.08776991435543806, Loss_test: 0.08641674203867993\n",
      "1428 - Loss_train: 0.0877674470742569, Loss_test: 0.08641456997318553\n",
      "1429 - Loss_train: 0.08776498106877213, Loss_test: 0.08641239910950901\n",
      "1430 - Loss_train: 0.08776251634095003, Loss_test: 0.08641022944532442\n",
      "1431 - Loss_train: 0.08776005289065456, Loss_test: 0.08640806098682631\n",
      "1432 - Loss_train: 0.08775759071589624, Loss_test: 0.08640589372623445\n",
      "1433 - Loss_train: 0.08775512981734725, Loss_test: 0.08640372766729204\n",
      "1434 - Loss_train: 0.08775267019256075, Loss_test: 0.08640156280770718\n",
      "1435 - Loss_train: 0.08775021184204615, Loss_test: 0.08639939914672311\n",
      "1436 - Loss_train: 0.08774775476498273, Loss_test: 0.08639723668436482\n",
      "1437 - Loss_train: 0.08774529896106728, Loss_test: 0.08639507541920756\n",
      "1438 - Loss_train: 0.08774284442983782, Loss_test: 0.08639291535213972\n",
      "1439 - Loss_train: 0.08774039117174813, Loss_test: 0.0863907564841998\n",
      "1440 - Loss_train: 0.08773793918398166, Loss_test: 0.08638859881033054\n",
      "1441 - Loss_train: 0.08773548846477967, Loss_test: 0.08638644232909833\n",
      "1442 - Loss_train: 0.08773303901798193, Loss_test: 0.08638428704588301\n",
      "1443 - Loss_train: 0.08773059083717714, Loss_test: 0.08638213295460925\n",
      "1444 - Loss_train: 0.08772814392480423, Loss_test: 0.08637998005887577\n",
      "1445 - Loss_train: 0.08772569827967515, Loss_test: 0.08637782835064807\n",
      "1446 - Loss_train: 0.08772325390380926, Loss_test: 0.08637567783966436\n",
      "1447 - Loss_train: 0.08772081079255592, Loss_test: 0.08637352851798666\n",
      "1448 - Loss_train: 0.0877183689484889, Loss_test: 0.08637138038819926\n",
      "1449 - Loss_train: 0.08771592836739757, Loss_test: 0.08636923344599044\n",
      "1450 - Loss_train: 0.0877134890506978, Loss_test: 0.08636708769445146\n",
      "1451 - Loss_train: 0.08771105100037796, Loss_test: 0.08636494313434849\n",
      "1452 - Loss_train: 0.08770861421099109, Loss_test: 0.08636279975971228\n",
      "1453 - Loss_train: 0.08770617868335925, Loss_test: 0.08636065757290945\n",
      "1454 - Loss_train: 0.08770374441968375, Loss_test: 0.08635851657466455\n",
      "1455 - Loss_train: 0.08770131141567811, Loss_test: 0.08635637676158793\n",
      "1456 - Loss_train: 0.08769887967285292, Loss_test: 0.08635423813446212\n",
      "1457 - Loss_train: 0.08769644918796933, Loss_test: 0.08635210069023222\n",
      "1458 - Loss_train: 0.08769401996302975, Loss_test: 0.0863499644324293\n",
      "1459 - Loss_train: 0.08769159199568718, Loss_test: 0.08634782935806022\n",
      "1460 - Loss_train: 0.0876891652873773, Loss_test: 0.08634569546641062\n",
      "1461 - Loss_train: 0.0876867398344474, Loss_test: 0.08634356275614848\n",
      "1462 - Loss_train: 0.0876843156397646, Loss_test: 0.08634143123022288\n",
      "1463 - Loss_train: 0.08768189269994957, Loss_test: 0.08633930088344557\n",
      "1464 - Loss_train: 0.08767947101465447, Loss_test: 0.08633717171628079\n",
      "1465 - Loss_train: 0.08767705058705899, Loss_test: 0.08633504373385753\n",
      "1466 - Loss_train: 0.0876746314143831, Loss_test: 0.0863329169303719\n",
      "1467 - Loss_train: 0.08767221349413484, Loss_test: 0.08633079130380528\n",
      "1468 - Loss_train: 0.08766979682315665, Loss_test: 0.08632866685574754\n",
      "1469 - Loss_train: 0.08766738140433204, Loss_test: 0.08632654358097411\n",
      "1470 - Loss_train: 0.08766496723819019, Loss_test: 0.08632442148783462\n",
      "1471 - Loss_train: 0.08766255432298965, Loss_test: 0.0863223005695604\n",
      "1472 - Loss_train: 0.08766014265660568, Loss_test: 0.0863201808258285\n",
      "1473 - Loss_train: 0.08765773224315108, Loss_test: 0.0863180622605102\n",
      "1474 - Loss_train: 0.08765532307522507, Loss_test: 0.0863159448678346\n",
      "1475 - Loss_train: 0.08765291515431223, Loss_test: 0.08631382864526489\n",
      "1476 - Loss_train: 0.08765050848438427, Loss_test: 0.08631171360124275\n",
      "1477 - Loss_train: 0.08764810305889426, Loss_test: 0.0863095997253403\n",
      "1478 - Loss_train: 0.08764569888180558, Loss_test: 0.0863074870263443\n",
      "1479 - Loss_train: 0.0876432959482788, Loss_test: 0.08630537549325189\n",
      "1480 - Loss_train: 0.08764089426183509, Loss_test: 0.08630326513560584\n",
      "1481 - Loss_train: 0.08763849381692386, Loss_test: 0.0863011559437925\n",
      "1482 - Loss_train: 0.08763609461573504, Loss_test: 0.08629904792274531\n",
      "1483 - Loss_train: 0.08763369666030499, Loss_test: 0.0862969410712006\n",
      "1484 - Loss_train: 0.0876312999479408, Loss_test: 0.08629483538924289\n",
      "1485 - Loss_train: 0.08762890447370965, Loss_test: 0.08629273087288165\n",
      "1486 - Loss_train: 0.08762651023946924, Loss_test: 0.0862906275225356\n",
      "1487 - Loss_train: 0.08762411724694708, Loss_test: 0.0862885253369526\n",
      "1488 - Loss_train: 0.087621725493856, Loss_test: 0.08628642431882376\n",
      "1489 - Loss_train: 0.08761933498262649, Loss_test: 0.08628432446697784\n",
      "1490 - Loss_train: 0.08761694570695874, Loss_test: 0.08628222577847033\n",
      "1491 - Loss_train: 0.08761455767167708, Loss_test: 0.08628012825381076\n",
      "1492 - Loss_train: 0.08761217087353448, Loss_test: 0.08627803189216851\n",
      "1493 - Loss_train: 0.08760978531176569, Loss_test: 0.08627593669616768\n",
      "1494 - Loss_train: 0.08760740098430482, Loss_test: 0.0862738426572346\n",
      "1495 - Loss_train: 0.08760501789422115, Loss_test: 0.0862717497836787\n",
      "1496 - Loss_train: 0.08760263603711009, Loss_test: 0.08626965806716741\n",
      "1497 - Loss_train: 0.08760025541377729, Loss_test: 0.08626756751215241\n",
      "1498 - Loss_train: 0.08759787602392767, Loss_test: 0.08626547811481057\n",
      "1499 - Loss_train: 0.08759549786577792, Loss_test: 0.08626338987799897\n",
      "1500 - Loss_train: 0.08759312094105942, Loss_test: 0.08626130279873075\n",
      "1501 - Loss_train: 0.08759074524689656, Loss_test: 0.08625921687553377\n",
      "1502 - Loss_train: 0.08758837078402547, Loss_test: 0.08625713211253437\n",
      "1503 - Loss_train: 0.08758599755015707, Loss_test: 0.08625504850228997\n",
      "1504 - Loss_train: 0.08758362554869746, Loss_test: 0.08625296605075705\n",
      "1505 - Loss_train: 0.08758125477432305, Loss_test: 0.08625088475299408\n",
      "1506 - Loss_train: 0.08757888523006935, Loss_test: 0.086248804610975\n",
      "1507 - Loss_train: 0.08757651691267587, Loss_test: 0.08624672562436926\n",
      "1508 - Loss_train: 0.0875741498219096, Loss_test: 0.08624464778593917\n",
      "1509 - Loss_train: 0.08757178395711576, Loss_test: 0.08624257110455671\n",
      "1510 - Loss_train: 0.08756941931742847, Loss_test: 0.0862404955714703\n",
      "1511 - Loss_train: 0.08756705590242289, Loss_test: 0.0862384211912454\n",
      "1512 - Loss_train: 0.0875646937126944, Loss_test: 0.08623634796121285\n",
      "1513 - Loss_train: 0.08756233274654332, Loss_test: 0.08623427588126929\n",
      "1514 - Loss_train: 0.08755997300563509, Loss_test: 0.08623220495464813\n",
      "1515 - Loss_train: 0.08755761448814275, Loss_test: 0.08623013517462297\n",
      "1516 - Loss_train: 0.08755525719172794, Loss_test: 0.08622806654408753\n",
      "1517 - Loss_train: 0.08755290111494617, Loss_test: 0.0862259990596943\n",
      "1518 - Loss_train: 0.08755054626060428, Loss_test: 0.08622393272290574\n",
      "1519 - Loss_train: 0.08754819262513765, Loss_test: 0.08622186753161404\n",
      "1520 - Loss_train: 0.08754584020792396, Loss_test: 0.0862198034869065\n",
      "1521 - Loss_train: 0.08754348901143003, Loss_test: 0.08621774058650239\n",
      "1522 - Loss_train: 0.08754113903053444, Loss_test: 0.08621567883228334\n",
      "1523 - Loss_train: 0.08753879026864927, Loss_test: 0.08621361821925914\n",
      "1524 - Loss_train: 0.08753644272276578, Loss_test: 0.08621155875293234\n",
      "1525 - Loss_train: 0.08753409639282239, Loss_test: 0.0862095004261683\n",
      "1526 - Loss_train: 0.08753175127944017, Loss_test: 0.08620744324359643\n",
      "1527 - Loss_train: 0.08752940738123946, Loss_test: 0.08620538720091123\n",
      "1528 - Loss_train: 0.08752706469689239, Loss_test: 0.08620333229965858\n",
      "1529 - Loss_train: 0.08752472322882243, Loss_test: 0.08620127854349391\n",
      "1530 - Loss_train: 0.08752238297324771, Loss_test: 0.08619922592317653\n",
      "1531 - Loss_train: 0.08752004392819411, Loss_test: 0.08619717444128093\n",
      "1532 - Loss_train: 0.08751770609781963, Loss_test: 0.08619512410074424\n",
      "1533 - Loss_train: 0.08751536947648877, Loss_test: 0.08619307489640243\n",
      "1534 - Loss_train: 0.08751303406776226, Loss_test: 0.08619102683129762\n",
      "1535 - Loss_train: 0.08751069986774278, Loss_test: 0.08618897990083678\n",
      "1536 - Loss_train: 0.08750836687566187, Loss_test: 0.086186934106634\n",
      "1537 - Loss_train: 0.08750603509201976, Loss_test: 0.0861848894450979\n",
      "1538 - Loss_train: 0.08750370451743054, Loss_test: 0.08618284592174218\n",
      "1539 - Loss_train: 0.0875013751506274, Loss_test: 0.08618080353166763\n",
      "1540 - Loss_train: 0.0874990469900291, Loss_test: 0.08617876227505036\n",
      "1541 - Loss_train: 0.08749672003818368, Loss_test: 0.08617672215419749\n",
      "1542 - Loss_train: 0.08749439429126382, Loss_test: 0.08617468316605181\n",
      "1543 - Loss_train: 0.08749206974847101, Loss_test: 0.08617264530634773\n",
      "1544 - Loss_train: 0.08748974640979013, Loss_test: 0.08617060858037817\n",
      "1545 - Loss_train: 0.08748742427647155, Loss_test: 0.08616857298360713\n",
      "1546 - Loss_train: 0.08748510334373126, Loss_test: 0.08616653851726341\n",
      "1547 - Loss_train: 0.0874827836145874, Loss_test: 0.08616450517921322\n",
      "1548 - Loss_train: 0.08748046508729262, Loss_test: 0.0861624729722003\n",
      "1549 - Loss_train: 0.0874781477607121, Loss_test: 0.08616044189050029\n",
      "1550 - Loss_train: 0.08747583163473126, Loss_test: 0.08615841193930378\n",
      "1551 - Loss_train: 0.08747351670882435, Loss_test: 0.0861563831117028\n",
      "1552 - Loss_train: 0.0874712029822776, Loss_test: 0.08615435541353343\n",
      "1553 - Loss_train: 0.08746889045466213, Loss_test: 0.08615232884100246\n",
      "1554 - Loss_train: 0.08746657912458998, Loss_test: 0.08615030339090828\n",
      "1555 - Loss_train: 0.08746426899270196, Loss_test: 0.08614827906855585\n",
      "1556 - Loss_train: 0.08746196006001664, Loss_test: 0.08614625586982305\n",
      "1557 - Loss_train: 0.08745965232094009, Loss_test: 0.08614423379320672\n",
      "1558 - Loss_train: 0.08745734577745348, Loss_test: 0.0861422128401214\n",
      "1559 - Loss_train: 0.08745504043183225, Loss_test: 0.08614019301277824\n",
      "1560 - Loss_train: 0.08745273628106028, Loss_test: 0.08613817430359552\n",
      "1561 - Loss_train: 0.08745043332161449, Loss_test: 0.08613615671601035\n",
      "1562 - Loss_train: 0.08744813155544816, Loss_test: 0.086134140249232\n",
      "1563 - Loss_train: 0.08744583098433306, Loss_test: 0.08613212490178442\n",
      "1564 - Loss_train: 0.0874435316051602, Loss_test: 0.0861301106770985\n",
      "1565 - Loss_train: 0.08744123341484794, Loss_test: 0.08612809756673007\n",
      "1566 - Loss_train: 0.0874389364153649, Loss_test: 0.08612608557510913\n",
      "1567 - Loss_train: 0.08743664060928516, Loss_test: 0.08612407470222475\n",
      "1568 - Loss_train: 0.08743434599241272, Loss_test: 0.08612206494789194\n",
      "1569 - Loss_train: 0.0874320525633784, Loss_test: 0.08612005630704654\n",
      "1570 - Loss_train: 0.08742976032351393, Loss_test: 0.08611804878479816\n",
      "1571 - Loss_train: 0.08742746927180138, Loss_test: 0.08611604237596872\n",
      "1572 - Loss_train: 0.0874251794037577, Loss_test: 0.08611403707980088\n",
      "1573 - Loss_train: 0.08742289072562441, Loss_test: 0.08611203290138145\n",
      "1574 - Loss_train: 0.08742060323059372, Loss_test: 0.08611002983115083\n",
      "1575 - Loss_train: 0.08741831692051978, Loss_test: 0.08610802787578425\n",
      "1576 - Loss_train: 0.08741603179674148, Loss_test: 0.08610602703310347\n",
      "1577 - Loss_train: 0.08741374785789212, Loss_test: 0.08610402730286948\n",
      "1578 - Loss_train: 0.08741146509948174, Loss_test: 0.08610202868028259\n",
      "1579 - Loss_train: 0.08740918352432338, Loss_test: 0.08610003116841579\n",
      "1580 - Loss_train: 0.08740690313132068, Loss_test: 0.0860980347677673\n",
      "1581 - Loss_train: 0.0874046239202061, Loss_test: 0.08609603947500154\n",
      "1582 - Loss_train: 0.08740234588993653, Loss_test: 0.08609404529157451\n",
      "1583 - Loss_train: 0.0874000690436295, Loss_test: 0.08609205221702843\n",
      "1584 - Loss_train: 0.08739779337438049, Loss_test: 0.08609006025234162\n",
      "1585 - Loss_train: 0.08739551888367157, Loss_test: 0.08608806938981961\n",
      "1586 - Loss_train: 0.08739324557139075, Loss_test: 0.08608607963391947\n",
      "1587 - Loss_train: 0.08739097343924182, Loss_test: 0.08608409098626667\n",
      "1588 - Loss_train: 0.08738870248442934, Loss_test: 0.08608210344413421\n",
      "1589 - Loss_train: 0.08738643270506141, Loss_test: 0.08608011700462412\n",
      "1590 - Loss_train: 0.08738416410068692, Loss_test: 0.08607813166801775\n",
      "1591 - Loss_train: 0.0873818966719579, Loss_test: 0.08607614743496406\n",
      "1592 - Loss_train: 0.08737963041914286, Loss_test: 0.0860741643041175\n",
      "1593 - Loss_train: 0.08737736533917238, Loss_test: 0.08607218227583179\n",
      "1594 - Loss_train: 0.08737510143385059, Loss_test: 0.08607020134966452\n",
      "1595 - Loss_train: 0.08737283870260645, Loss_test: 0.08606822152159471\n",
      "1596 - Loss_train: 0.08737057714445884, Loss_test: 0.08606624280162645\n",
      "1597 - Loss_train: 0.0873683167562207, Loss_test: 0.08606426517297787\n",
      "1598 - Loss_train: 0.08736605753905191, Loss_test: 0.08606228864687321\n",
      "1599 - Loss_train: 0.08736379949218563, Loss_test: 0.08606031321762816\n",
      "1600 - Loss_train: 0.08736154261580201, Loss_test: 0.0860583388878577\n",
      "1601 - Loss_train: 0.08735928691078851, Loss_test: 0.0860563656553911\n",
      "1602 - Loss_train: 0.08735703237263243, Loss_test: 0.08605439352062924\n",
      "1603 - Loss_train: 0.08735477900327501, Loss_test: 0.08605242247861386\n",
      "1604 - Loss_train: 0.08735252680092692, Loss_test: 0.08605045253499029\n",
      "1605 - Loss_train: 0.08735027576556881, Loss_test: 0.08604848368506535\n",
      "1606 - Loss_train: 0.08734802589582898, Loss_test: 0.08604651592941266\n",
      "1607 - Loss_train: 0.08734577719521597, Loss_test: 0.08604454926955618\n",
      "1608 - Loss_train: 0.08734352965803406, Loss_test: 0.08604258370259513\n",
      "1609 - Loss_train: 0.0873412832858645, Loss_test: 0.08604061922697356\n",
      "1610 - Loss_train: 0.08733903807767393, Loss_test: 0.08603865584532687\n",
      "1611 - Loss_train: 0.08733679403146477, Loss_test: 0.08603669355227613\n",
      "1612 - Loss_train: 0.08733455114909662, Loss_test: 0.08603473235214687\n",
      "1613 - Loss_train: 0.08733230943078732, Loss_test: 0.08603277224343081\n",
      "1614 - Loss_train: 0.0873300688746321, Loss_test: 0.08603081322521804\n",
      "1615 - Loss_train: 0.08732782947651863, Loss_test: 0.08602885529453307\n",
      "1616 - Loss_train: 0.08732559123869889, Loss_test: 0.08602689844980582\n",
      "1617 - Loss_train: 0.08732335416096935, Loss_test: 0.08602494269620373\n",
      "1618 - Loss_train: 0.08732111824462654, Loss_test: 0.08602298802927516\n",
      "1619 - Loss_train: 0.08731888348486005, Loss_test: 0.08602103444902948\n",
      "1620 - Loss_train: 0.08731664988353147, Loss_test: 0.08601908195494443\n",
      "1621 - Loss_train: 0.08731441744057654, Loss_test: 0.08601713054709755\n",
      "1622 - Loss_train: 0.0873121861541434, Loss_test: 0.08601518022604103\n",
      "1623 - Loss_train: 0.08730995602247114, Loss_test: 0.08601323098678047\n",
      "1624 - Loss_train: 0.08730772704878, Loss_test: 0.08601128283203723\n",
      "1625 - Loss_train: 0.08730549922803893, Loss_test: 0.08600933576159366\n",
      "1626 - Loss_train: 0.08730327256190412, Loss_test: 0.08600738977216073\n",
      "1627 - Loss_train: 0.0873010470494452, Loss_test: 0.08600544486545644\n",
      "1628 - Loss_train: 0.08729882269294041, Loss_test: 0.08600350104357224\n",
      "1629 - Loss_train: 0.08729659948691397, Loss_test: 0.0860015582997732\n",
      "1630 - Loss_train: 0.08729437743425428, Loss_test: 0.08599961663693195\n",
      "1631 - Loss_train: 0.08729215653104032, Loss_test: 0.08599767605516233\n",
      "1632 - Loss_train: 0.08728993677869598, Loss_test: 0.08599573654983224\n",
      "1633 - Loss_train: 0.08728771817725549, Loss_test: 0.08599379812378934\n",
      "1634 - Loss_train: 0.0872855007248203, Loss_test: 0.08599186077801084\n",
      "1635 - Loss_train: 0.08728328442211113, Loss_test: 0.08598992450826465\n",
      "1636 - Loss_train: 0.08728106926955911, Loss_test: 0.08598798931701208\n",
      "1637 - Loss_train: 0.08727885526519276, Loss_test: 0.08598605520274541\n",
      "1638 - Loss_train: 0.08727664240487037, Loss_test: 0.08598412216207685\n",
      "1639 - Loss_train: 0.08727443069379678, Loss_test: 0.08598219019761207\n",
      "1640 - Loss_train: 0.08727222012654579, Loss_test: 0.08598025930771488\n",
      "1641 - Loss_train: 0.08727001070497349, Loss_test: 0.08597832949201702\n",
      "1642 - Loss_train: 0.08726780243007232, Loss_test: 0.08597640074968106\n",
      "1643 - Loss_train: 0.08726559529813549, Loss_test: 0.08597447307922529\n",
      "1644 - Loss_train: 0.08726338931229508, Loss_test: 0.08597254648391134\n",
      "1645 - Loss_train: 0.08726118446967135, Loss_test: 0.08597062096187344\n",
      "1646 - Loss_train: 0.08725898076758355, Loss_test: 0.08596869650826815\n",
      "1647 - Loss_train: 0.08725677820802574, Loss_test: 0.08596677312550975\n",
      "1648 - Loss_train: 0.08725457679093787, Loss_test: 0.08596485081387134\n",
      "1649 - Loss_train: 0.08725237651255824, Loss_test: 0.08596292956935599\n",
      "1650 - Loss_train: 0.08725017737667243, Loss_test: 0.08596100939736215\n",
      "1651 - Loss_train: 0.08724797937790146, Loss_test: 0.08595909029118297\n",
      "1652 - Loss_train: 0.08724578252024096, Loss_test: 0.08595717225354613\n",
      "1653 - Loss_train: 0.08724358679842618, Loss_test: 0.08595525528254709\n",
      "1654 - Loss_train: 0.08724139221504715, Loss_test: 0.08595333937696346\n",
      "1655 - Loss_train: 0.08723919876891047, Loss_test: 0.08595142453797996\n",
      "1656 - Loss_train: 0.08723700646154009, Loss_test: 0.08594951076845132\n",
      "1657 - Loss_train: 0.08723481528810662, Loss_test: 0.08594759805958113\n",
      "1658 - Loss_train: 0.08723262525276664, Loss_test: 0.08594568642004646\n",
      "1659 - Loss_train: 0.08723043635220329, Loss_test: 0.0859437758404725\n",
      "1660 - Loss_train: 0.08722824858282052, Loss_test: 0.08594186632559794\n",
      "1661 - Loss_train: 0.08722606194799766, Loss_test: 0.08593995787048957\n",
      "1662 - Loss_train: 0.0872238764456656, Loss_test: 0.08593805047968969\n",
      "1663 - Loss_train: 0.08722169207899681, Loss_test: 0.08593614415010913\n",
      "1664 - Loss_train: 0.08721950884429282, Loss_test: 0.0859342388842183\n",
      "1665 - Loss_train: 0.08721732674099542, Loss_test: 0.0859323346777914\n",
      "1666 - Loss_train: 0.08721514576678588, Loss_test: 0.08593043153145241\n",
      "1667 - Loss_train: 0.0872129659247608, Loss_test: 0.0859285294437182\n",
      "1668 - Loss_train: 0.08721078720975596, Loss_test: 0.08592662841403713\n",
      "1669 - Loss_train: 0.08720860962310215, Loss_test: 0.08592472844094388\n",
      "1670 - Loss_train: 0.08720643316794241, Loss_test: 0.08592282952790463\n",
      "1671 - Loss_train: 0.08720425783829773, Loss_test: 0.08592093167111442\n",
      "1672 - Loss_train: 0.08720208363657589, Loss_test: 0.08591903486987192\n",
      "1673 - Loss_train: 0.0871999105607827, Loss_test: 0.08591713912666828\n",
      "1674 - Loss_train: 0.08719773861254944, Loss_test: 0.08591524443644878\n",
      "1675 - Loss_train: 0.087195567788316, Loss_test: 0.08591335080094759\n",
      "1676 - Loss_train: 0.08719339808986669, Loss_test: 0.08591145822180113\n",
      "1677 - Loss_train: 0.08719122951736048, Loss_test: 0.0859095666958815\n",
      "1678 - Loss_train: 0.08718906206675499, Loss_test: 0.08590767622239971\n",
      "1679 - Loss_train: 0.08718689573961506, Loss_test: 0.08590578680044103\n",
      "1680 - Loss_train: 0.08718473053525831, Loss_test: 0.08590389843193526\n",
      "1681 - Loss_train: 0.08718256645355583, Loss_test: 0.08590201111545519\n",
      "1682 - Loss_train: 0.08718040349229672, Loss_test: 0.08590012484824458\n",
      "1683 - Loss_train: 0.08717824165195424, Loss_test: 0.08589823963049767\n",
      "1684 - Loss_train: 0.08717608093329464, Loss_test: 0.08589635546234817\n",
      "1685 - Loss_train: 0.08717392133542919, Loss_test: 0.0858944723509805\n",
      "1686 - Loss_train: 0.08717176285423711, Loss_test: 0.08589259027932587\n",
      "1687 - Loss_train: 0.08716960549241998, Loss_test: 0.08589070925887872\n",
      "1688 - Loss_train: 0.0871674492482583, Loss_test: 0.08588882928532059\n",
      "1689 - Loss_train: 0.08716529412143564, Loss_test: 0.08588695035787172\n",
      "1690 - Loss_train: 0.08716314011139423, Loss_test: 0.08588507247815211\n",
      "1691 - Loss_train: 0.08716098721842688, Loss_test: 0.08588319564509737\n",
      "1692 - Loss_train: 0.08715883544050664, Loss_test: 0.08588131985510142\n",
      "1693 - Loss_train: 0.08715668477858049, Loss_test: 0.08587944511125925\n",
      "1694 - Loss_train: 0.08715453523038462, Loss_test: 0.08587757141055354\n",
      "1695 - Loss_train: 0.08715238679674356, Loss_test: 0.08587569875464322\n",
      "1696 - Loss_train: 0.08715023947525068, Loss_test: 0.0858738271398238\n",
      "1697 - Loss_train: 0.0871480932676113, Loss_test: 0.0858719565681458\n",
      "1698 - Loss_train: 0.08714594817195523, Loss_test: 0.08587008703970749\n",
      "1699 - Loss_train: 0.08714380418824716, Loss_test: 0.08586821855065709\n",
      "1700 - Loss_train: 0.08714166131604563, Loss_test: 0.08586635110675318\n",
      "1701 - Loss_train: 0.08713951955458019, Loss_test: 0.08586448469681357\n",
      "1702 - Loss_train: 0.08713737890428841, Loss_test: 0.08586261933275834\n",
      "1703 - Loss_train: 0.08713523936189706, Loss_test: 0.08586075500506789\n",
      "1704 - Loss_train: 0.08713310092843195, Loss_test: 0.08585889171707209\n",
      "1705 - Loss_train: 0.08713096360462762, Loss_test: 0.08585702946551896\n",
      "1706 - Loss_train: 0.0871288273884438, Loss_test: 0.08585516825330834\n",
      "1707 - Loss_train: 0.08712669227835652, Loss_test: 0.08585330807630144\n",
      "1708 - Loss_train: 0.08712455827627476, Loss_test: 0.08585144893806766\n",
      "1709 - Loss_train: 0.08712242537756959, Loss_test: 0.08584959083043407\n",
      "1710 - Loss_train: 0.08712029358407894, Loss_test: 0.08584773376351283\n",
      "1711 - Loss_train: 0.08711816289586409, Loss_test: 0.0858458777270062\n",
      "1712 - Loss_train: 0.08711603331165754, Loss_test: 0.08584402272602629\n",
      "1713 - Loss_train: 0.08711390483096074, Loss_test: 0.08584216875800178\n",
      "1714 - Loss_train: 0.08711177745575165, Loss_test: 0.08584031582541118\n",
      "1715 - Loss_train: 0.08710965118289314, Loss_test: 0.08583846392498663\n",
      "1716 - Loss_train: 0.08710752600988446, Loss_test: 0.08583661305434398\n",
      "1717 - Loss_train: 0.08710540193906201, Loss_test: 0.08583476321618673\n",
      "1718 - Loss_train: 0.08710327896911628, Loss_test: 0.08583291440998808\n",
      "1719 - Loss_train: 0.08710115709864356, Loss_test: 0.08583106663080825\n",
      "1720 - Loss_train: 0.08709903632811154, Loss_test: 0.085829219883102\n",
      "1721 - Loss_train: 0.08709691665820594, Loss_test: 0.08582737416687616\n",
      "1722 - Loss_train: 0.08709479808523173, Loss_test: 0.08582552947643816\n",
      "1723 - Loss_train: 0.08709268061009524, Loss_test: 0.08582368581338642\n",
      "1724 - Loss_train: 0.08709056423196046, Loss_test: 0.08582184317915957\n",
      "1725 - Loss_train: 0.08708844895118353, Loss_test: 0.0858200015701006\n",
      "1726 - Loss_train: 0.08708633476674255, Loss_test: 0.0858181609891914\n",
      "1727 - Loss_train: 0.08708422167775957, Loss_test: 0.08581632143402206\n",
      "1728 - Loss_train: 0.08708210968450365, Loss_test: 0.08581448290417007\n",
      "1729 - Loss_train: 0.08707999878527463, Loss_test: 0.0858126453990176\n",
      "1730 - Loss_train: 0.08707788898352867, Loss_test: 0.08581080892013322\n",
      "1731 - Loss_train: 0.0870757802728717, Loss_test: 0.0858089734635162\n",
      "1732 - Loss_train: 0.08707367265550879, Loss_test: 0.08580713903110217\n",
      "1733 - Loss_train: 0.08707156613128768, Loss_test: 0.08580530562067003\n",
      "1734 - Loss_train: 0.08706946069699598, Loss_test: 0.08580347323119025\n",
      "1735 - Loss_train: 0.08706735635322907, Loss_test: 0.08580164186188274\n",
      "1736 - Loss_train: 0.08706525310044663, Loss_test: 0.08579981151514697\n",
      "1737 - Loss_train: 0.08706315093977038, Loss_test: 0.08579798218969574\n",
      "1738 - Loss_train: 0.08706104986681631, Loss_test: 0.08579615388212444\n",
      "1739 - Loss_train: 0.08705894988441432, Loss_test: 0.08579432659720845\n",
      "1740 - Loss_train: 0.08705685098991087, Loss_test: 0.08579250032918899\n",
      "1741 - Loss_train: 0.08705475318357327, Loss_test: 0.08579067508012472\n",
      "1742 - Loss_train: 0.08705265646226235, Loss_test: 0.0857888508477504\n",
      "1743 - Loss_train: 0.08705056083031083, Loss_test: 0.08578702763123418\n",
      "1744 - Loss_train: 0.08704846628172164, Loss_test: 0.0857852054314752\n",
      "1745 - Loss_train: 0.08704637281934054, Loss_test: 0.08578338424836944\n",
      "1746 - Loss_train: 0.08704428044190886, Loss_test: 0.08578156408029125\n",
      "1747 - Loss_train: 0.08704218915099572, Loss_test: 0.0857797449292727\n",
      "1748 - Loss_train: 0.08704009894214045, Loss_test: 0.08577792679120011\n",
      "1749 - Loss_train: 0.08703800981612454, Loss_test: 0.08577610966565472\n",
      "1750 - Loss_train: 0.08703592177518005, Loss_test: 0.08577429355576624\n",
      "1751 - Loss_train: 0.08703383481653504, Loss_test: 0.08577247845979359\n",
      "1752 - Loss_train: 0.08703174893784331, Loss_test: 0.08577066437283884\n",
      "1753 - Loss_train: 0.08702966413918345, Loss_test: 0.08576885129762085\n",
      "1754 - Loss_train: 0.08702758042275568, Loss_test: 0.08576703923556545\n",
      "1755 - Loss_train: 0.0870254977844069, Loss_test: 0.0857652281810296\n",
      "1756 - Loss_train: 0.08702341622551978, Loss_test: 0.08576341813794001\n",
      "1757 - Loss_train: 0.0870213357493506, Loss_test: 0.08576160910549302\n",
      "1758 - Loss_train: 0.0870192563500558, Loss_test: 0.08575980108390245\n",
      "1759 - Loss_train: 0.0870171780281452, Loss_test: 0.08575799406742984\n",
      "1760 - Loss_train: 0.08701510078217459, Loss_test: 0.08575618806153501\n",
      "1761 - Loss_train: 0.08701302461363046, Loss_test: 0.08575438306030134\n",
      "1762 - Loss_train: 0.08701094952043185, Loss_test: 0.08575257906780981\n",
      "1763 - Loss_train: 0.08700887550200358, Loss_test: 0.08575077608032834\n",
      "1764 - Loss_train: 0.08700680255929365, Loss_test: 0.08574897409586703\n",
      "1765 - Loss_train: 0.08700473069297311, Loss_test: 0.08574717312205918\n",
      "1766 - Loss_train: 0.08700265989910717, Loss_test: 0.08574537315009795\n",
      "1767 - Loss_train: 0.0870005901803701, Loss_test: 0.08574357418678606\n",
      "1768 - Loss_train: 0.08699852153172061, Loss_test: 0.08574177622078837\n",
      "1769 - Loss_train: 0.08699645395526764, Loss_test: 0.08573997925999471\n",
      "1770 - Loss_train: 0.08699438745092986, Loss_test: 0.08573818330058135\n",
      "1771 - Loss_train: 0.08699232202041049, Loss_test: 0.08573638834766015\n",
      "1772 - Loss_train: 0.08699025765724741, Loss_test: 0.08573459439240862\n",
      "1773 - Loss_train: 0.08698819436356782, Loss_test: 0.08573280143809957\n",
      "1774 - Loss_train: 0.0869861321422157, Loss_test: 0.08573100948604614\n",
      "1775 - Loss_train: 0.08698407098682014, Loss_test: 0.08572921853138567\n",
      "1776 - Loss_train: 0.0869820109004273, Loss_test: 0.0857274285762893\n",
      "1777 - Loss_train: 0.08697995188244649, Loss_test: 0.08572563962135646\n",
      "1778 - Loss_train: 0.0869778939331513, Loss_test: 0.08572385166541976\n",
      "1779 - Loss_train: 0.08697583704845617, Loss_test: 0.08572206470546584\n",
      "1780 - Loss_train: 0.08697378122917976, Loss_test: 0.08572027874300903\n",
      "1781 - Loss_train: 0.08697172647577221, Loss_test: 0.08571849377608247\n",
      "1782 - Loss_train: 0.08696967279014942, Loss_test: 0.0857167098084761\n",
      "1783 - Loss_train: 0.08696762016636973, Loss_test: 0.08571492683422839\n",
      "1784 - Loss_train: 0.08696556860688158, Loss_test: 0.08571314485440787\n",
      "1785 - Loss_train: 0.086963518110457, Loss_test: 0.08571136386947578\n",
      "1786 - Loss_train: 0.08696146867707452, Loss_test: 0.08570958387978271\n",
      "1787 - Loss_train: 0.08695942030792073, Loss_test: 0.08570780488227164\n",
      "1788 - Loss_train: 0.08695737299793953, Loss_test: 0.08570602687999694\n",
      "1789 - Loss_train: 0.0869553267511579, Loss_test: 0.08570424986751013\n",
      "1790 - Loss_train: 0.08695328156532763, Loss_test: 0.08570247384940777\n",
      "1791 - Loss_train: 0.08695123743732136, Loss_test: 0.08570069881956477\n",
      "1792 - Loss_train: 0.08694919437178826, Loss_test: 0.0856989247828684\n",
      "1793 - Loss_train: 0.08694715236208139, Loss_test: 0.08569715173602818\n",
      "1794 - Loss_train: 0.08694511141187893, Loss_test: 0.0856953796769333\n",
      "1795 - Loss_train: 0.08694307151842993, Loss_test: 0.08569360861008045\n",
      "1796 - Loss_train: 0.0869410326851096, Loss_test: 0.0856918385301998\n",
      "1797 - Loss_train: 0.0869389949056815, Loss_test: 0.08569006943826346\n",
      "1798 - Loss_train: 0.08693695818498921, Loss_test: 0.08568830133679847\n",
      "1799 - Loss_train: 0.08693492251992772, Loss_test: 0.08568653421880795\n",
      "1800 - Loss_train: 0.08693288790858157, Loss_test: 0.08568476808787186\n",
      "1801 - Loss_train: 0.08693085435112018, Loss_test: 0.0856830029453551\n",
      "1802 - Loss_train: 0.08692882184759525, Loss_test: 0.08568123878354253\n",
      "1803 - Loss_train: 0.08692679040006542, Loss_test: 0.08567947561069045\n",
      "1804 - Loss_train: 0.08692476000275669, Loss_test: 0.08567771342162495\n",
      "1805 - Loss_train: 0.08692273065884354, Loss_test: 0.08567595221472223\n",
      "1806 - Loss_train: 0.08692070236842196, Loss_test: 0.08567419199426547\n",
      "1807 - Loss_train: 0.08691867512704422, Loss_test: 0.08567243275303427\n",
      "1808 - Loss_train: 0.08691664893669127, Loss_test: 0.08567067449595656\n",
      "1809 - Loss_train: 0.08691462379604287, Loss_test: 0.08566891722064003\n",
      "1810 - Loss_train: 0.08691259970553643, Loss_test: 0.08566716092559781\n",
      "1811 - Loss_train: 0.08691057666615887, Loss_test: 0.08566540561247182\n",
      "1812 - Loss_train: 0.08690855467472514, Loss_test: 0.08566365128059554\n",
      "1813 - Loss_train: 0.08690653373030716, Loss_test: 0.08566189792672933\n",
      "1814 - Loss_train: 0.08690451383333216, Loss_test: 0.0856601455525608\n",
      "1815 - Loss_train: 0.0869024949823442, Loss_test: 0.08565839415602432\n",
      "1816 - Loss_train: 0.08690047717869107, Loss_test: 0.08565664373727022\n",
      "1817 - Loss_train: 0.08689846042110483, Loss_test: 0.08565489429668793\n",
      "1818 - Loss_train: 0.08689644470892124, Loss_test: 0.08565314583323445\n",
      "1819 - Loss_train: 0.08689443004123568, Loss_test: 0.08565139834766508\n",
      "1820 - Loss_train: 0.08689241641791709, Loss_test: 0.08564965183709702\n",
      "1821 - Loss_train: 0.08689040383806741, Loss_test: 0.08564790630209956\n",
      "1822 - Loss_train: 0.0868883923039894, Loss_test: 0.08564616174327841\n",
      "1823 - Loss_train: 0.08688638180977555, Loss_test: 0.08564441815805982\n",
      "1824 - Loss_train: 0.08688437236005742, Loss_test: 0.0856426755466044\n",
      "1825 - Loss_train: 0.08688236394982261, Loss_test: 0.08564093390823019\n",
      "1826 - Loss_train: 0.08688035658132102, Loss_test: 0.08563919324309323\n",
      "1827 - Loss_train: 0.08687835025342061, Loss_test: 0.08563745354998704\n",
      "1828 - Loss_train: 0.08687634496574562, Loss_test: 0.08563571483053366\n",
      "1829 - Loss_train: 0.08687434071780659, Loss_test: 0.08563397708074344\n",
      "1830 - Loss_train: 0.08687233750786906, Loss_test: 0.08563224030268973\n",
      "1831 - Loss_train: 0.08687033533551448, Loss_test: 0.08563050449342527\n",
      "1832 - Loss_train: 0.08686833420154295, Loss_test: 0.08562876965396159\n",
      "1833 - Loss_train: 0.08686633410457974, Loss_test: 0.0856270357850171\n",
      "1834 - Loss_train: 0.08686433504478705, Loss_test: 0.08562530288254308\n",
      "1835 - Loss_train: 0.08686233702150288, Loss_test: 0.08562357095183741\n",
      "1836 - Loss_train: 0.0868603400358711, Loss_test: 0.08562183998784467\n",
      "1837 - Loss_train: 0.08685834408503618, Loss_test: 0.0856201099925067\n",
      "1838 - Loss_train: 0.08685634916786852, Loss_test: 0.08561838096206921\n",
      "1839 - Loss_train: 0.08685435528374487, Loss_test: 0.08561665289756201\n",
      "1840 - Loss_train: 0.08685236243307541, Loss_test: 0.08561492579876784\n",
      "1841 - Loss_train: 0.08685037061727523, Loss_test: 0.08561319966400072\n",
      "1842 - Loss_train: 0.08684837983217301, Loss_test: 0.08561147449573586\n",
      "1843 - Loss_train: 0.08684639008108504, Loss_test: 0.0856097502911036\n",
      "1844 - Loss_train: 0.0868444013599007, Loss_test: 0.08560802705147316\n",
      "1845 - Loss_train: 0.08684241367012657, Loss_test: 0.08560630476961002\n",
      "1846 - Loss_train: 0.08684042700999428, Loss_test: 0.08560458345726145\n",
      "1847 - Loss_train: 0.08683844137921204, Loss_test: 0.08560286310264856\n",
      "1848 - Loss_train: 0.08683645677887884, Loss_test: 0.08560114370920567\n",
      "1849 - Loss_train: 0.08683447320632447, Loss_test: 0.08559942527854304\n",
      "1850 - Loss_train: 0.08683249066285599, Loss_test: 0.08559770780814374\n",
      "1851 - Loss_train: 0.08683050914814977, Loss_test: 0.0855959912984092\n",
      "1852 - Loss_train: 0.08682852865836985, Loss_test: 0.08559427574637775\n",
      "1853 - Loss_train: 0.08682654919550285, Loss_test: 0.08559256115303002\n",
      "1854 - Loss_train: 0.0868245707585606, Loss_test: 0.08559084751881353\n",
      "1855 - Loss_train: 0.08682259334743458, Loss_test: 0.08558913484204263\n",
      "1856 - Loss_train: 0.08682061696293229, Loss_test: 0.08558742312504496\n",
      "1857 - Loss_train: 0.08681864160132594, Loss_test: 0.08558571236311312\n",
      "1858 - Loss_train: 0.08681666726360368, Loss_test: 0.08558400255591439\n",
      "1859 - Loss_train: 0.08681469394932743, Loss_test: 0.08558229370583155\n",
      "1860 - Loss_train: 0.08681272165997905, Loss_test: 0.08558058581324374\n",
      "1861 - Loss_train: 0.0868107503910296, Loss_test: 0.08557887887347344\n",
      "1862 - Loss_train: 0.08680878014600527, Loss_test: 0.08557717288880601\n",
      "1863 - Loss_train: 0.08680681092089333, Loss_test: 0.08557546785874955\n",
      "1864 - Loss_train: 0.08680484271883028, Loss_test: 0.08557376378230672\n",
      "1865 - Loss_train: 0.08680287553334515, Loss_test: 0.08557206065681501\n",
      "1866 - Loss_train: 0.08680090936891316, Loss_test: 0.0855703584830869\n",
      "1867 - Loss_train: 0.08679894422312022, Loss_test: 0.08556865726189394\n",
      "1868 - Loss_train: 0.08679698009600723, Loss_test: 0.08556695699085781\n",
      "1869 - Loss_train: 0.08679501698739277, Loss_test: 0.08556525767302386\n",
      "1870 - Loss_train: 0.08679305489844913, Loss_test: 0.0855635593045035\n",
      "1871 - Loss_train: 0.08679109382422771, Loss_test: 0.0855618618843251\n",
      "1872 - Loss_train: 0.086789133766531, Loss_test: 0.08556016541469504\n",
      "1873 - Loss_train: 0.08678717472719688, Loss_test: 0.0855584698943121\n",
      "1874 - Loss_train: 0.0867852167012969, Loss_test: 0.08555677532072294\n",
      "1875 - Loss_train: 0.0867832596924728, Loss_test: 0.08555508169724244\n",
      "1876 - Loss_train: 0.08678130369578403, Loss_test: 0.08555338901894763\n",
      "1877 - Loss_train: 0.08677934871331458, Loss_test: 0.08555169728654607\n",
      "1878 - Loss_train: 0.08677739474554452, Loss_test: 0.08555000649976181\n",
      "1879 - Loss_train: 0.08677544178945264, Loss_test: 0.08554831666135342\n",
      "1880 - Loss_train: 0.08677348984791998, Loss_test: 0.08554662776844105\n",
      "1881 - Loss_train: 0.08677153891573186, Loss_test: 0.08554493981917387\n",
      "1882 - Loss_train: 0.08676958899508497, Loss_test: 0.08554325281246895\n",
      "1883 - Loss_train: 0.08676764008533028, Loss_test: 0.08554156674982483\n",
      "1884 - Loss_train: 0.08676569218576526, Loss_test: 0.08553988163007299\n",
      "1885 - Loss_train: 0.08676374529899308, Loss_test: 0.08553819745540059\n",
      "1886 - Loss_train: 0.086761799420399, Loss_test: 0.08553651422251024\n",
      "1887 - Loss_train: 0.08675985455087334, Loss_test: 0.08553483193148909\n",
      "1888 - Loss_train: 0.08675791068756644, Loss_test: 0.08553315057870567\n",
      "1889 - Loss_train: 0.08675596783162758, Loss_test: 0.08553147016866637\n",
      "1890 - Loss_train: 0.08675402598407925, Loss_test: 0.08552979069964396\n",
      "1891 - Loss_train: 0.08675208514202362, Loss_test: 0.0855281121656724\n",
      "1892 - Loss_train: 0.08675014530653356, Loss_test: 0.08552643457254662\n",
      "1893 - Loss_train: 0.08674820647609083, Loss_test: 0.08552475791911329\n",
      "1894 - Loss_train: 0.08674626865074726, Loss_test: 0.08552308220390592\n",
      "1895 - Loss_train: 0.0867443318318656, Loss_test: 0.08552140742657072\n",
      "1896 - Loss_train: 0.08674239601743165, Loss_test: 0.0855197335871879\n",
      "1897 - Loss_train: 0.08674046120624683, Loss_test: 0.08551806068536766\n",
      "1898 - Loss_train: 0.0867385273968419, Loss_test: 0.08551638871613515\n",
      "1899 - Loss_train: 0.08673659458950826, Loss_test: 0.08551471768392127\n",
      "1900 - Loss_train: 0.08673466278400264, Loss_test: 0.08551304758640377\n",
      "1901 - Loss_train: 0.0867327319799116, Loss_test: 0.08551137842254618\n",
      "1902 - Loss_train: 0.0867308021795463, Loss_test: 0.08550971019515213\n",
      "1903 - Loss_train: 0.08672887337910153, Loss_test: 0.08550804290309913\n",
      "1904 - Loss_train: 0.08672694557840244, Loss_test: 0.08550637654196164\n",
      "1905 - Loss_train: 0.08672501877525797, Loss_test: 0.08550471111097008\n",
      "1906 - Loss_train: 0.0867230929708257, Loss_test: 0.08550304661443425\n",
      "1907 - Loss_train: 0.08672116816524493, Loss_test: 0.085501383048174\n",
      "1908 - Loss_train: 0.0867192443589466, Loss_test: 0.08549972041536559\n",
      "1909 - Loss_train: 0.08671732154799792, Loss_test: 0.08549805870922944\n",
      "1910 - Loss_train: 0.08671539973313984, Loss_test: 0.08549639793613074\n",
      "1911 - Loss_train: 0.08671347891476064, Loss_test: 0.0854947380883111\n",
      "1912 - Loss_train: 0.08671155909318377, Loss_test: 0.08549307917430408\n",
      "1913 - Loss_train: 0.08670964026581097, Loss_test: 0.0854914211870108\n",
      "1914 - Loss_train: 0.08670772243345923, Loss_test: 0.08548976412596992\n",
      "1915 - Loss_train: 0.08670580559614252, Loss_test: 0.08548810799721235\n",
      "1916 - Loss_train: 0.08670388975148959, Loss_test: 0.0854864527906947\n",
      "1917 - Loss_train: 0.08670197490009535, Loss_test: 0.08548479851394654\n",
      "1918 - Loss_train: 0.08670006104222974, Loss_test: 0.08548314516183105\n",
      "1919 - Loss_train: 0.08669814817597175, Loss_test: 0.08548149273507345\n",
      "1920 - Loss_train: 0.08669623630254873, Loss_test: 0.08547984123336393\n",
      "1921 - Loss_train: 0.08669432542128132, Loss_test: 0.08547819065991341\n",
      "1922 - Loss_train: 0.0866924155311136, Loss_test: 0.08547654100907127\n",
      "1923 - Loss_train: 0.08669050662953388, Loss_test: 0.08547489228210829\n",
      "1924 - Loss_train: 0.08668859871669939, Loss_test: 0.08547324447658411\n",
      "1925 - Loss_train: 0.08668669179226263, Loss_test: 0.08547159759253989\n",
      "1926 - Loss_train: 0.08668478585691698, Loss_test: 0.08546995163163507\n",
      "1927 - Loss_train: 0.08668288090883762, Loss_test: 0.08546830659166837\n",
      "1928 - Loss_train: 0.08668097694903422, Loss_test: 0.08546666247348723\n",
      "1929 - Loss_train: 0.08667907397603168, Loss_test: 0.0854650192744088\n",
      "1930 - Loss_train: 0.08667717199013868, Loss_test: 0.08546337699780467\n",
      "1931 - Loss_train: 0.08667527099029979, Loss_test: 0.0854617356396304\n",
      "1932 - Loss_train: 0.08667337097572486, Loss_test: 0.08546009520059258\n",
      "1933 - Loss_train: 0.08667147194627814, Loss_test: 0.085458455681512\n",
      "1934 - Loss_train: 0.08666957390184298, Loss_test: 0.0854568170790852\n",
      "1935 - Loss_train: 0.08666767684115399, Loss_test: 0.08545517939820843\n",
      "1936 - Loss_train: 0.08666578076399048, Loss_test: 0.08545354262967059\n",
      "1937 - Loss_train: 0.08666388567139592, Loss_test: 0.0854519067803163\n",
      "1938 - Loss_train: 0.0866619915594616, Loss_test: 0.0854502718466744\n",
      "1939 - Loss_train: 0.08666009843128318, Loss_test: 0.08544863783153782\n",
      "1940 - Loss_train: 0.08665820628329407, Loss_test: 0.08544700472816358\n",
      "1941 - Loss_train: 0.08665631511833491, Loss_test: 0.08544537254077904\n",
      "1942 - Loss_train: 0.08665442493072426, Loss_test: 0.08544374126835876\n",
      "1943 - Loss_train: 0.08665253572384835, Loss_test: 0.085442110906745\n",
      "1944 - Loss_train: 0.08665064749620673, Loss_test: 0.08544048146040242\n",
      "1945 - Loss_train: 0.08664876024758991, Loss_test: 0.08543885292605174\n",
      "1946 - Loss_train: 0.08664687397723621, Loss_test: 0.08543722530585232\n",
      "1947 - Loss_train: 0.08664498868548487, Loss_test: 0.0854355985963234\n",
      "1948 - Loss_train: 0.0866431043710824, Loss_test: 0.085433972797562\n",
      "1949 - Loss_train: 0.08664122103283464, Loss_test: 0.08543234790920297\n",
      "1950 - Loss_train: 0.08663933867257734, Loss_test: 0.08543072393171422\n",
      "1951 - Loss_train: 0.08663745728670486, Loss_test: 0.08542910086454582\n",
      "1952 - Loss_train: 0.08663557687665359, Loss_test: 0.08542747870871234\n",
      "1953 - Loss_train: 0.08663369744328318, Loss_test: 0.08542585745854021\n",
      "1954 - Loss_train: 0.08663181898226319, Loss_test: 0.08542423711928741\n",
      "1955 - Loss_train: 0.0866299414950287, Loss_test: 0.08542261768514364\n",
      "1956 - Loss_train: 0.08662806498119988, Loss_test: 0.08542099915876551\n",
      "1957 - Loss_train: 0.08662618944183854, Loss_test: 0.08541938154282086\n",
      "1958 - Loss_train: 0.08662431487336002, Loss_test: 0.08541776482892467\n",
      "1959 - Loss_train: 0.08662244127722417, Loss_test: 0.08541614902447829\n",
      "1960 - Loss_train: 0.08662056865344867, Loss_test: 0.08541453412568699\n",
      "1961 - Loss_train: 0.08661869699883298, Loss_test: 0.08541292012732353\n",
      "1962 - Loss_train: 0.08661682631457238, Loss_test: 0.0854113070372555\n",
      "1963 - Loss_train: 0.08661495660001163, Loss_test: 0.08540969484883616\n",
      "1964 - Loss_train: 0.08661308785487816, Loss_test: 0.08540808356501373\n",
      "1965 - Loss_train: 0.08661122007929153, Loss_test: 0.08540647318512877\n",
      "1966 - Loss_train: 0.08660935327196027, Loss_test: 0.08540486370565531\n",
      "1967 - Loss_train: 0.08660748743369284, Loss_test: 0.08540325513212628\n",
      "1968 - Loss_train: 0.08660562256094656, Loss_test: 0.08540164745774302\n",
      "1969 - Loss_train: 0.08660375865533908, Loss_test: 0.08540004068373068\n",
      "1970 - Loss_train: 0.08660189571765349, Loss_test: 0.08539843481251415\n",
      "1971 - Loss_train: 0.08660003374390335, Loss_test: 0.08539682983820737\n",
      "1972 - Loss_train: 0.08659817273835667, Loss_test: 0.08539522576715468\n",
      "1973 - Loss_train: 0.08659631269574079, Loss_test: 0.08539362259437806\n",
      "1974 - Loss_train: 0.08659445361646992, Loss_test: 0.0853920203183746\n",
      "1975 - Loss_train: 0.08659259550220352, Loss_test: 0.08539041894318412\n",
      "1976 - Loss_train: 0.08659073835013806, Loss_test: 0.0853888184624439\n",
      "1977 - Loss_train: 0.08658888216179493, Loss_test: 0.08538721888108225\n",
      "1978 - Loss_train: 0.08658702693647001, Loss_test: 0.08538562019736555\n",
      "1979 - Loss_train: 0.08658517267135576, Loss_test: 0.08538402240611254\n",
      "1980 - Loss_train: 0.08658331936848192, Loss_test: 0.08538242551340477\n",
      "1981 - Loss_train: 0.08658146702576856, Loss_test: 0.08538082951528235\n",
      "1982 - Loss_train: 0.0865796156452224, Loss_test: 0.0853792344162732\n",
      "1983 - Loss_train: 0.08657776522470415, Loss_test: 0.08537764020375631\n",
      "1984 - Loss_train: 0.08657591576180819, Loss_test: 0.08537604689297379\n",
      "1985 - Loss_train: 0.08657406725784152, Loss_test: 0.08537445446886838\n",
      "1986 - Loss_train: 0.08657221971195232, Loss_test: 0.08537286294112592\n",
      "1987 - Loss_train: 0.08657037312430062, Loss_test: 0.08537127230449285\n",
      "1988 - Loss_train: 0.08656852749390452, Loss_test: 0.08536968255970549\n",
      "1989 - Loss_train: 0.08656668282004104, Loss_test: 0.08536809370634157\n",
      "1990 - Loss_train: 0.08656483910531242, Loss_test: 0.08536650574750732\n",
      "1991 - Loss_train: 0.08656299634468038, Loss_test: 0.08536491867598896\n",
      "1992 - Loss_train: 0.08656115453900491, Loss_test: 0.08536333249454034\n",
      "1993 - Loss_train: 0.08655931368801965, Loss_test: 0.08536174720379733\n",
      "1994 - Loss_train: 0.08655747379161792, Loss_test: 0.0853601627990411\n",
      "1995 - Loss_train: 0.08655563484980562, Loss_test: 0.08535857928615853\n",
      "1996 - Loss_train: 0.08655379686277012, Loss_test: 0.08535699666249502\n",
      "1997 - Loss_train: 0.08655195982712051, Loss_test: 0.08535541492281715\n",
      "1998 - Loss_train: 0.08655012374384342, Loss_test: 0.08535383407328954\n",
      "1999 - Loss_train: 0.08654828861193573, Loss_test: 0.0853522541077768\n",
      "2000 - Loss_train: 0.08654645443397503, Loss_test: 0.08535067503209513\n",
      "2001 - Loss_train: 0.08654462120673102, Loss_test: 0.08534909684126218\n",
      "2002 - Loss_train: 0.08654278892802779, Loss_test: 0.08534751953474916\n",
      "2003 - Loss_train: 0.0865409575994012, Loss_test: 0.08534594311298292\n",
      "2004 - Loss_train: 0.08653912722021614, Loss_test: 0.08534436757388049\n",
      "2005 - Loss_train: 0.08653729779183977, Loss_test: 0.08534279292230844\n",
      "2006 - Loss_train: 0.08653546931041256, Loss_test: 0.08534121915124038\n",
      "2007 - Loss_train: 0.08653364177936515, Loss_test: 0.0853396462645492\n",
      "2008 - Loss_train: 0.08653181519299415, Loss_test: 0.08533807426137167\n",
      "2009 - Loss_train: 0.08652998955385068, Loss_test: 0.08533650313680351\n",
      "2010 - Loss_train: 0.0865281648641393, Loss_test: 0.08533493289624902\n",
      "2011 - Loss_train: 0.08652634111817051, Loss_test: 0.08533336353652457\n",
      "2012 - Loss_train: 0.08652451831767743, Loss_test: 0.08533179505583105\n",
      "2013 - Loss_train: 0.08652269646375656, Loss_test: 0.08533022745705263\n",
      "2014 - Loss_train: 0.08652087555308398, Loss_test: 0.08532866073539185\n",
      "2015 - Loss_train: 0.08651905558648144, Loss_test: 0.08532709489447916\n",
      "2016 - Loss_train: 0.08651723656333186, Loss_test: 0.08532552993198213\n",
      "2017 - Loss_train: 0.08651541848272652, Loss_test: 0.08532396584747406\n",
      "2018 - Loss_train: 0.08651360134542448, Loss_test: 0.08532240264112394\n",
      "2019 - Loss_train: 0.08651178515002375, Loss_test: 0.085320840310731\n",
      "2020 - Loss_train: 0.08650996989817432, Loss_test: 0.08531927885838413\n",
      "2021 - Loss_train: 0.08650815558703458, Loss_test: 0.08531771828395099\n",
      "2022 - Loss_train: 0.0865063422152553, Loss_test: 0.08531615858191241\n",
      "2023 - Loss_train: 0.0865045297839087, Loss_test: 0.08531459975799549\n",
      "2024 - Loss_train: 0.08650271829302665, Loss_test: 0.08531304180630278\n",
      "2025 - Loss_train: 0.08650090774314236, Loss_test: 0.0853114847330472\n",
      "2026 - Loss_train: 0.08649909812976253, Loss_test: 0.08530992853124622\n",
      "2027 - Loss_train: 0.08649728945379744, Loss_test: 0.0853083732007117\n",
      "2028 - Loss_train: 0.08649548171806017, Loss_test: 0.08530681874763851\n",
      "2029 - Loss_train: 0.08649367491777751, Loss_test: 0.08530526516522244\n",
      "2030 - Loss_train: 0.08649186905345248, Loss_test: 0.08530371245280892\n",
      "2031 - Loss_train: 0.08649006412616471, Loss_test: 0.08530216061270761\n",
      "2032 - Loss_train: 0.08648826013676093, Loss_test: 0.08530060964663146\n",
      "2033 - Loss_train: 0.08648645708065994, Loss_test: 0.08529905954823866\n",
      "2034 - Loss_train: 0.08648465495845974, Loss_test: 0.08529751031959575\n",
      "2035 - Loss_train: 0.08648285377210868, Loss_test: 0.08529596196185586\n",
      "2036 - Loss_train: 0.08648105352084573, Loss_test: 0.0852944144738326\n",
      "2037 - Loss_train: 0.08647925420328811, Loss_test: 0.08529286785804362\n",
      "2038 - Loss_train: 0.08647745581680134, Loss_test: 0.08529132210503308\n",
      "2039 - Loss_train: 0.08647565836474388, Loss_test: 0.08528977722330244\n",
      "2040 - Loss_train: 0.08647386184395031, Loss_test: 0.08528823320784731\n",
      "2041 - Loss_train: 0.08647206625365593, Loss_test: 0.08528669005969448\n",
      "2042 - Loss_train: 0.08647027159619669, Loss_test: 0.08528514777674662\n",
      "2043 - Loss_train: 0.08646847786715744, Loss_test: 0.08528360636122569\n",
      "2044 - Loss_train: 0.08646668507019249, Loss_test: 0.08528206581141787\n",
      "2045 - Loss_train: 0.08646489320129747, Loss_test: 0.08528052612460482\n",
      "2046 - Loss_train: 0.08646310226345795, Loss_test: 0.08527898730371058\n",
      "2047 - Loss_train: 0.08646131225231261, Loss_test: 0.0852774493487347\n",
      "2048 - Loss_train: 0.08645952316923874, Loss_test: 0.08527591225423886\n",
      "2049 - Loss_train: 0.08645773501394657, Loss_test: 0.08527437602476431\n",
      "2050 - Loss_train: 0.08645594778558754, Loss_test: 0.08527284065657875\n",
      "2051 - Loss_train: 0.08645416148346634, Loss_test: 0.08527130615105519\n",
      "2052 - Loss_train: 0.08645237610818472, Loss_test: 0.08526977250665663\n",
      "2053 - Loss_train: 0.08645059165829037, Loss_test: 0.085268239726289\n",
      "2054 - Loss_train: 0.08644880813456667, Loss_test: 0.08526670780402842\n",
      "2055 - Loss_train: 0.08644702553766467, Loss_test: 0.08526517674505173\n",
      "2056 - Loss_train: 0.08644524386228618, Loss_test: 0.0852636465444464\n",
      "2057 - Loss_train: 0.08644346311341455, Loss_test: 0.0852621172070471\n",
      "2058 - Loss_train: 0.08644168328620389, Loss_test: 0.08526058872351039\n",
      "2059 - Loss_train: 0.08643990438043368, Loss_test: 0.08525906110085\n",
      "2060 - Loss_train: 0.08643812640018994, Loss_test: 0.08525753433668962\n",
      "2061 - Loss_train: 0.08643634933895103, Loss_test: 0.0852560084302812\n",
      "2062 - Loss_train: 0.0864345731988893, Loss_test: 0.08525448337788621\n",
      "2063 - Loss_train: 0.08643279798030858, Loss_test: 0.08525295918327842\n",
      "2064 - Loss_train: 0.08643102368438742, Loss_test: 0.0852514358482897\n",
      "2065 - Loss_train: 0.0864292503062088, Loss_test: 0.08524991336514502\n",
      "2066 - Loss_train: 0.0864274778496982, Loss_test: 0.08524839174125132\n",
      "2067 - Loss_train: 0.08642570631196697, Loss_test: 0.08524687097109582\n",
      "2068 - Loss_train: 0.08642393569323234, Loss_test: 0.08524535105652793\n",
      "2069 - Loss_train: 0.08642216599212411, Loss_test: 0.08524383199287379\n",
      "2070 - Loss_train: 0.08642039720706782, Loss_test: 0.08524231378431056\n",
      "2071 - Loss_train: 0.08641862933872056, Loss_test: 0.08524079642772102\n",
      "2072 - Loss_train: 0.08641686238793822, Loss_test: 0.0852392799234455\n",
      "2073 - Loss_train: 0.08641509635257398, Loss_test: 0.08523776427129719\n",
      "2074 - Loss_train: 0.08641333123526002, Loss_test: 0.0852362494745447\n",
      "2075 - Loss_train: 0.0864115670331212, Loss_test: 0.08523473552732505\n",
      "2076 - Loss_train: 0.08640980374467383, Loss_test: 0.0852332224296615\n",
      "2077 - Loss_train: 0.08640804137271327, Loss_test: 0.08523171018407061\n",
      "2078 - Loss_train: 0.08640627991181825, Loss_test: 0.0852301987880327\n",
      "2079 - Loss_train: 0.0864045193651588, Loss_test: 0.08522868823873017\n",
      "2080 - Loss_train: 0.0864027597304436, Loss_test: 0.08522717853922816\n",
      "2081 - Loss_train: 0.0864010010099391, Loss_test: 0.0852256696882765\n",
      "2082 - Loss_train: 0.08639924319948043, Loss_test: 0.08522416168836836\n",
      "2083 - Loss_train: 0.08639748630210944, Loss_test: 0.08522265452950253\n",
      "2084 - Loss_train: 0.08639573031441121, Loss_test: 0.08522114822660933\n",
      "2085 - Loss_train: 0.08639397523961574, Loss_test: 0.08521964276530467\n",
      "2086 - Loss_train: 0.0863922210745854, Loss_test: 0.08521813815140797\n",
      "2087 - Loss_train: 0.08639046781883267, Loss_test: 0.08521663438487713\n",
      "2088 - Loss_train: 0.0863887154706273, Loss_test: 0.0852151314607577\n",
      "2089 - Loss_train: 0.08638696403044097, Loss_test: 0.08521362938119112\n",
      "2090 - Loss_train: 0.08638521349801559, Loss_test: 0.08521212814548138\n",
      "2091 - Loss_train: 0.08638346387587154, Loss_test: 0.08521062775626989\n",
      "2092 - Loss_train: 0.0863817151587333, Loss_test: 0.08520912820870251\n",
      "2093 - Loss_train: 0.08637996734947159, Loss_test: 0.08520762950418517\n",
      "2094 - Loss_train: 0.08637822044768484, Loss_test: 0.08520613164371038\n",
      "2095 - Loss_train: 0.08637647445206834, Loss_test: 0.0852046346267484\n",
      "2096 - Loss_train: 0.08637472935985145, Loss_test: 0.08520313844716353\n",
      "2097 - Loss_train: 0.08637298517201322, Loss_test: 0.0852016431107579\n",
      "2098 - Loss_train: 0.08637124188921078, Loss_test: 0.08520014861533809\n",
      "2099 - Loss_train: 0.08636949951032323, Loss_test: 0.08519865495950334\n",
      "2100 - Loss_train: 0.08636775803700016, Loss_test: 0.08519716214760752\n",
      "2101 - Loss_train: 0.08636601746511852, Loss_test: 0.0851956701695213\n",
      "2102 - Loss_train: 0.08636427779727583, Loss_test: 0.08519417903542369\n",
      "2103 - Loss_train: 0.08636253903176838, Loss_test: 0.08519268873857327\n",
      "2104 - Loss_train: 0.08636080116548596, Loss_test: 0.08519119927769732\n",
      "2105 - Loss_train: 0.0863590642022608, Loss_test: 0.08518971065615287\n",
      "2106 - Loss_train: 0.0863573281380044, Loss_test: 0.08518822287080202\n",
      "2107 - Loss_train: 0.08635559297625804, Loss_test: 0.08518673592176843\n",
      "2108 - Loss_train: 0.0863538587133267, Loss_test: 0.0851852498094411\n",
      "2109 - Loss_train: 0.08635212534964985, Loss_test: 0.08518376453383963\n",
      "2110 - Loss_train: 0.08635039288377747, Loss_test: 0.0851822800931293\n",
      "2111 - Loss_train: 0.08634866131630124, Loss_test: 0.08518079648748363\n",
      "2112 - Loss_train: 0.08634693064622506, Loss_test: 0.08517931371479563\n",
      "2113 - Loss_train: 0.08634520087517993, Loss_test: 0.08517783177535065\n",
      "2114 - Loss_train: 0.08634347199999952, Loss_test: 0.08517635067498223\n",
      "2115 - Loss_train: 0.08634174402138309, Loss_test: 0.08517487039994988\n",
      "2116 - Loss_train: 0.08634001693815499, Loss_test: 0.08517339096274332\n",
      "2117 - Loss_train: 0.08633829075095108, Loss_test: 0.08517191235369301\n",
      "2118 - Loss_train: 0.08633656546094533, Loss_test: 0.08517043458257885\n",
      "2119 - Loss_train: 0.08633484106365848, Loss_test: 0.08516895763875543\n",
      "2120 - Loss_train: 0.08633311756070453, Loss_test: 0.08516748152639657\n",
      "2121 - Loss_train: 0.0863313949511909, Loss_test: 0.0851660062439526\n",
      "2122 - Loss_train: 0.08632967323549134, Loss_test: 0.08516453179413375\n",
      "2123 - Loss_train: 0.08632795241248263, Loss_test: 0.08516305816985673\n",
      "2124 - Loss_train: 0.08632623248216731, Loss_test: 0.08516158537816802\n",
      "2125 - Loss_train: 0.08632451344503957, Loss_test: 0.08516011341486056\n",
      "2126 - Loss_train: 0.08632279529783365, Loss_test: 0.08515864227707554\n",
      "2127 - Loss_train: 0.08632107804205795, Loss_test: 0.08515717196970758\n",
      "2128 - Loss_train: 0.08631936167586832, Loss_test: 0.08515570249055797\n",
      "2129 - Loss_train: 0.08631764619996654, Loss_test: 0.08515423383283759\n",
      "2130 - Loss_train: 0.08631593161455853, Loss_test: 0.08515276600840543\n",
      "2131 - Loss_train: 0.08631421791808097, Loss_test: 0.08515129900541121\n",
      "2132 - Loss_train: 0.08631250511064502, Loss_test: 0.08514983282968643\n",
      "2133 - Loss_train: 0.08631079319094022, Loss_test: 0.08514836748076768\n",
      "2134 - Loss_train: 0.08630908216007584, Loss_test: 0.08514690295502986\n",
      "2135 - Loss_train: 0.08630737201429328, Loss_test: 0.08514543925200337\n",
      "2136 - Loss_train: 0.0863056627564324, Loss_test: 0.08514397637576172\n",
      "2137 - Loss_train: 0.08630395438642456, Loss_test: 0.08514251431900388\n",
      "2138 - Loss_train: 0.08630224689985444, Loss_test: 0.08514105308746468\n",
      "2139 - Loss_train: 0.08630054029931467, Loss_test: 0.08513959267861239\n",
      "2140 - Loss_train: 0.08629883458559316, Loss_test: 0.085138133090756\n",
      "2141 - Loss_train: 0.0862971297556527, Loss_test: 0.08513667432678178\n",
      "2142 - Loss_train: 0.08629542580871137, Loss_test: 0.08513521638137272\n",
      "2143 - Loss_train: 0.08629372274564352, Loss_test: 0.08513375925716518\n",
      "2144 - Loss_train: 0.0862920205655231, Loss_test: 0.08513230295152577\n",
      "2145 - Loss_train: 0.08629031926826732, Loss_test: 0.08513084746859005\n",
      "2146 - Loss_train: 0.08628861885446662, Loss_test: 0.08512939280513067\n",
      "2147 - Loss_train: 0.08628691932110735, Loss_test: 0.0851279389581292\n",
      "2148 - Loss_train: 0.08628522066931196, Loss_test: 0.08512648593167418\n",
      "2149 - Loss_train: 0.08628352289922651, Loss_test: 0.08512503372107692\n",
      "2150 - Loss_train: 0.08628182601040764, Loss_test: 0.0851235823318612\n",
      "2151 - Loss_train: 0.08628012999969864, Loss_test: 0.0851221317580318\n",
      "2152 - Loss_train: 0.08627843486798788, Loss_test: 0.08512068199938116\n",
      "2153 - Loss_train: 0.08627674061570807, Loss_test: 0.0851192330576021\n",
      "2154 - Loss_train: 0.08627504724420132, Loss_test: 0.08511778493306296\n",
      "2155 - Loss_train: 0.08627335474957984, Loss_test: 0.08511633762174133\n",
      "2156 - Loss_train: 0.0862716631338475, Loss_test: 0.08511489112975296\n",
      "2157 - Loss_train: 0.08626997239521733, Loss_test: 0.0851134454484217\n",
      "2158 - Loss_train: 0.08626828253347485, Loss_test: 0.08511200058442499\n",
      "2159 - Loss_train: 0.08626659354652357, Loss_test: 0.08511055652983703\n",
      "2160 - Loss_train: 0.08626490543589346, Loss_test: 0.08510911329304466\n",
      "2161 - Loss_train: 0.0862632181994507, Loss_test: 0.08510767086353678\n",
      "2162 - Loss_train: 0.08626153183872468, Loss_test: 0.08510622924847228\n",
      "2163 - Loss_train: 0.08625984635351464, Loss_test: 0.08510478844743774\n",
      "2164 - Loss_train: 0.08625816174102986, Loss_test: 0.08510334845610641\n",
      "2165 - Loss_train: 0.08625647800243388, Loss_test: 0.0851019092741576\n",
      "2166 - Loss_train: 0.0862547951370747, Loss_test: 0.08510047090372078\n",
      "2167 - Loss_train: 0.08625311314403095, Loss_test: 0.0850990333456961\n",
      "2168 - Loss_train: 0.08625143202526339, Loss_test: 0.08509759659625768\n",
      "2169 - Loss_train: 0.08624975177644255, Loss_test: 0.08509616065609513\n",
      "2170 - Loss_train: 0.08624807240089824, Loss_test: 0.08509472552503003\n",
      "2171 - Loss_train: 0.08624639389438463, Loss_test: 0.08509329120198048\n",
      "2172 - Loss_train: 0.0862447162602326, Loss_test: 0.08509185768882435\n",
      "2173 - Loss_train: 0.08624303949452604, Loss_test: 0.08509042497951474\n",
      "2174 - Loss_train: 0.08624136359811599, Loss_test: 0.08508899307946333\n",
      "2175 - Loss_train: 0.08623968857122712, Loss_test: 0.08508756198517745\n",
      "2176 - Loss_train: 0.08623801441340956, Loss_test: 0.08508613169710473\n",
      "2177 - Loss_train: 0.08623634112345399, Loss_test: 0.08508470221628796\n",
      "2178 - Loss_train: 0.08623466870101357, Loss_test: 0.08508327353922625\n",
      "2179 - Loss_train: 0.08623299714647423, Loss_test: 0.08508184566798102\n",
      "2180 - Loss_train: 0.08623132646018061, Loss_test: 0.08508041860268074\n",
      "2181 - Loss_train: 0.08622965664018187, Loss_test: 0.08507899234140456\n",
      "2182 - Loss_train: 0.0862279876844333, Loss_test: 0.08507756688269431\n",
      "2183 - Loss_train: 0.0862263195941503, Loss_test: 0.08507614222611316\n",
      "2184 - Loss_train: 0.08622465236903948, Loss_test: 0.08507471837241476\n",
      "2185 - Loss_train: 0.08622298600819564, Loss_test: 0.08507329532102458\n",
      "2186 - Loss_train: 0.08622132051158003, Loss_test: 0.08507187307128049\n",
      "2187 - Loss_train: 0.08621965588075443, Loss_test: 0.08507045162525788\n",
      "2188 - Loss_train: 0.08621799211153895, Loss_test: 0.08506903097966748\n",
      "2189 - Loss_train: 0.08621632920515501, Loss_test: 0.08506761113386642\n",
      "2190 - Loss_train: 0.0862146671612282, Loss_test: 0.08506619208873437\n",
      "2191 - Loss_train: 0.08621300597934721, Loss_test: 0.08506477384129262\n",
      "2192 - Loss_train: 0.08621134565834639, Loss_test: 0.08506335639619202\n",
      "2193 - Loss_train: 0.08620968620137447, Loss_test: 0.08506193974970347\n",
      "2194 - Loss_train: 0.08620802760226881, Loss_test: 0.08506052389895154\n",
      "2195 - Loss_train: 0.08620636986482892, Loss_test: 0.08505910885011786\n",
      "2196 - Loss_train: 0.08620471298554686, Loss_test: 0.08505769459625558\n",
      "2197 - Loss_train: 0.0862030569675549, Loss_test: 0.08505628114176074\n",
      "2198 - Loss_train: 0.0862014018088377, Loss_test: 0.08505486848169794\n",
      "2199 - Loss_train: 0.08619974750704618, Loss_test: 0.08505345662095293\n",
      "2200 - Loss_train: 0.08619809406498373, Loss_test: 0.08505204555509267\n",
      "2201 - Loss_train: 0.0861964414799201, Loss_test: 0.08505063528500581\n",
      "2202 - Loss_train: 0.08619478975028866, Loss_test: 0.08504922580915357\n",
      "2203 - Loss_train: 0.08619313887691875, Loss_test: 0.08504781712739172\n",
      "2204 - Loss_train: 0.08619148886146409, Loss_test: 0.08504640924029404\n",
      "2205 - Loss_train: 0.08618983970237211, Loss_test: 0.08504500214657984\n",
      "2206 - Loss_train: 0.08618819139610756, Loss_test: 0.08504359584892925\n",
      "2207 - Loss_train: 0.08618654394479199, Loss_test: 0.0850421903389451\n",
      "2208 - Loss_train: 0.08618489734813514, Loss_test: 0.08504078562238597\n",
      "2209 - Loss_train: 0.08618325160528914, Loss_test: 0.08503938170072081\n",
      "2210 - Loss_train: 0.08618160671763726, Loss_test: 0.0850379785692956\n",
      "2211 - Loss_train: 0.08617996268340634, Loss_test: 0.08503657623070202\n",
      "2212 - Loss_train: 0.0861783195002007, Loss_test: 0.0850351746807074\n",
      "2213 - Loss_train: 0.08617667716847034, Loss_test: 0.08503377392172778\n",
      "2214 - Loss_train: 0.0861750356891608, Loss_test: 0.08503237395167339\n",
      "2215 - Loss_train: 0.08617339506061407, Loss_test: 0.08503097477143248\n",
      "2216 - Loss_train: 0.08617175528382434, Loss_test: 0.08502957638106845\n",
      "2217 - Loss_train: 0.08617011635723468, Loss_test: 0.08502817877974303\n",
      "2218 - Loss_train: 0.08616847828224514, Loss_test: 0.08502678196779997\n",
      "2219 - Loss_train: 0.08616684105457204, Loss_test: 0.0850253859409079\n",
      "2220 - Loss_train: 0.08616520467614112, Loss_test: 0.08502399070094725\n",
      "2221 - Loss_train: 0.0861635691481756, Loss_test: 0.08502259625049467\n",
      "2222 - Loss_train: 0.08616193446663595, Loss_test: 0.08502120258716928\n",
      "2223 - Loss_train: 0.0861603006332558, Loss_test: 0.08501980970424074\n",
      "2224 - Loss_train: 0.08615866764721113, Loss_test: 0.08501841761322482\n",
      "2225 - Loss_train: 0.08615703550780536, Loss_test: 0.08501702630345986\n",
      "2226 - Loss_train: 0.08615540421674356, Loss_test: 0.0850156357819568\n",
      "2227 - Loss_train: 0.086153773769675, Loss_test: 0.08501424604273766\n",
      "2228 - Loss_train: 0.08615214416833225, Loss_test: 0.08501285708838345\n",
      "2229 - Loss_train: 0.08615051541260231, Loss_test: 0.08501146891626322\n",
      "2230 - Loss_train: 0.08614888750167204, Loss_test: 0.0850100815282137\n",
      "2231 - Loss_train: 0.08614726043640948, Loss_test: 0.0850086949249435\n",
      "2232 - Loss_train: 0.08614563421346529, Loss_test: 0.085007309102046\n",
      "2233 - Loss_train: 0.08614400883416147, Loss_test: 0.08500592406051394\n",
      "2234 - Loss_train: 0.08614238429757913, Loss_test: 0.08500453980113858\n",
      "2235 - Loss_train: 0.08614076060369383, Loss_test: 0.08500315632091611\n",
      "2236 - Loss_train: 0.08613913775188507, Loss_test: 0.08500177362473421\n",
      "2237 - Loss_train: 0.08613751574194084, Loss_test: 0.08500039170487321\n",
      "2238 - Loss_train: 0.08613589457455209, Loss_test: 0.08499901056960928\n",
      "2239 - Loss_train: 0.08613427424672994, Loss_test: 0.0849976302108043\n",
      "2240 - Loss_train: 0.08613265476083876, Loss_test: 0.08499625063185852\n",
      "2241 - Loss_train: 0.08613103611530619, Loss_test: 0.08499487183243139\n",
      "2242 - Loss_train: 0.08612941830819698, Loss_test: 0.08499349381134683\n",
      "2243 - Loss_train: 0.08612780134010634, Loss_test: 0.08499211656690639\n",
      "2244 - Loss_train: 0.08612618521115761, Loss_test: 0.08499074009900993\n",
      "2245 - Loss_train: 0.08612456991899747, Loss_test: 0.08498936440869122\n",
      "2246 - Loss_train: 0.08612295546464958, Loss_test: 0.08498798949359139\n",
      "2247 - Loss_train: 0.0861213418473791, Loss_test: 0.08498661535475399\n",
      "2248 - Loss_train: 0.08611972906760014, Loss_test: 0.08498524199285491\n",
      "2249 - Loss_train: 0.08611811712385388, Loss_test: 0.08498386940497368\n",
      "2250 - Loss_train: 0.08611650601608915, Loss_test: 0.08498249758992428\n",
      "2251 - Loss_train: 0.08611489574425538, Loss_test: 0.0849811265513418\n",
      "2252 - Loss_train: 0.08611328630699472, Loss_test: 0.08497975628588751\n",
      "2253 - Loss_train: 0.08611167770475724, Loss_test: 0.08497838679661761\n",
      "2254 - Loss_train: 0.08611006993921919, Loss_test: 0.08497701807862325\n",
      "2255 - Loss_train: 0.0861084630070932, Loss_test: 0.08497565013501215\n",
      "2256 - Loss_train: 0.08610685690692887, Loss_test: 0.08497428296133303\n",
      "2257 - Loss_train: 0.08610525163965256, Loss_test: 0.0849729165599771\n",
      "2258 - Loss_train: 0.08610364720479, Loss_test: 0.08497155092948247\n",
      "2259 - Loss_train: 0.08610204360177365, Loss_test: 0.08497018607129249\n",
      "2260 - Loss_train: 0.08610044083077233, Loss_test: 0.08496882198203569\n",
      "2261 - Loss_train: 0.08609883889273934, Loss_test: 0.08496745866518075\n",
      "2262 - Loss_train: 0.086097237783787, Loss_test: 0.08496609611711296\n",
      "2263 - Loss_train: 0.08609563750546859, Loss_test: 0.08496473433877655\n",
      "2264 - Loss_train: 0.08609403805703775, Loss_test: 0.08496337332789634\n",
      "2265 - Loss_train: 0.0860924394376556, Loss_test: 0.08496201308728728\n",
      "2266 - Loss_train: 0.0860908416473587, Loss_test: 0.08496065361192516\n",
      "2267 - Loss_train: 0.08608924468646931, Loss_test: 0.08495929490609797\n",
      "2268 - Loss_train: 0.08608764855538152, Loss_test: 0.08495793696963617\n",
      "2269 - Loss_train: 0.08608605325076454, Loss_test: 0.0849565797969098\n",
      "2270 - Loss_train: 0.08608445877396408, Loss_test: 0.08495522339323333\n",
      "2271 - Loss_train: 0.08608286512390267, Loss_test: 0.0849538677531345\n",
      "2272 - Loss_train: 0.08608127230035219, Loss_test: 0.08495251288251765\n",
      "2273 - Loss_train: 0.08607968030249481, Loss_test: 0.0849511587708444\n",
      "2274 - Loss_train: 0.08607808913141134, Loss_test: 0.08494980542986359\n",
      "2275 - Loss_train: 0.0860764987847176, Loss_test: 0.08494845285124653\n",
      "2276 - Loss_train: 0.08607490926288437, Loss_test: 0.08494710103486208\n",
      "2277 - Loss_train: 0.0860733205664214, Loss_test: 0.08494574998408413\n",
      "2278 - Loss_train: 0.08607173269291879, Loss_test: 0.08494439969656614\n",
      "2279 - Loss_train: 0.08607014564369088, Loss_test: 0.08494305017216379\n",
      "2280 - Loss_train: 0.08606855941715362, Loss_test: 0.08494170140844053\n",
      "2281 - Loss_train: 0.08606697401596888, Loss_test: 0.08494035340668367\n",
      "2282 - Loss_train: 0.08606538943402475, Loss_test: 0.08493900616959435\n",
      "2283 - Loss_train: 0.08606380567626278, Loss_test: 0.08493765969127474\n",
      "2284 - Loss_train: 0.08606222273817718, Loss_test: 0.08493631397391817\n",
      "2285 - Loss_train: 0.08606064062112269, Loss_test: 0.0849349690167818\n",
      "2286 - Loss_train: 0.08605905932459662, Loss_test: 0.08493362481841257\n",
      "2287 - Loss_train: 0.0860574788501373, Loss_test: 0.08493228137958074\n",
      "2288 - Loss_train: 0.08605589919315593, Loss_test: 0.0849309387063943\n",
      "2289 - Loss_train: 0.08605432035603226, Loss_test: 0.0849295967809193\n",
      "2290 - Loss_train: 0.08605274233782276, Loss_test: 0.08492825562092425\n",
      "2291 - Loss_train: 0.08605116514021031, Loss_test: 0.08492691521710762\n",
      "2292 - Loss_train: 0.0860495887580302, Loss_test: 0.08492557557066106\n",
      "2293 - Loss_train: 0.08604801319392247, Loss_test: 0.08492423668167007\n",
      "2294 - Loss_train: 0.08604643844855787, Loss_test: 0.08492289854804358\n",
      "2295 - Loss_train: 0.0860448645188785, Loss_test: 0.08492156117202823\n",
      "2296 - Loss_train: 0.08604329140512054, Loss_test: 0.08492022455013395\n",
      "2297 - Loss_train: 0.08604171910908173, Loss_test: 0.08491888868711481\n",
      "2298 - Loss_train: 0.08604014762782529, Loss_test: 0.08491755357529307\n",
      "2299 - Loss_train: 0.08603857696257297, Loss_test: 0.08491621921947214\n",
      "2300 - Loss_train: 0.0860370071101852, Loss_test: 0.08491488562044334\n",
      "2301 - Loss_train: 0.08603543807198999, Loss_test: 0.0849135527709132\n",
      "2302 - Loss_train: 0.08603386984924807, Loss_test: 0.08491222067718261\n",
      "2303 - Loss_train: 0.08603230243780594, Loss_test: 0.08491088933530361\n",
      "2304 - Loss_train: 0.08603073584047356, Loss_test: 0.08490955874494817\n",
      "2305 - Loss_train: 0.08602917005706745, Loss_test: 0.08490822891077243\n",
      "2306 - Loss_train: 0.08602760508575041, Loss_test: 0.08490689982501062\n",
      "2307 - Loss_train: 0.08602604092468345, Loss_test: 0.08490557149149472\n",
      "2308 - Loss_train: 0.08602447757656455, Loss_test: 0.08490424391101214\n",
      "2309 - Loss_train: 0.08602291503921336, Loss_test: 0.08490291707830791\n",
      "2310 - Loss_train: 0.08602135331060012, Loss_test: 0.08490159099695777\n",
      "2311 - Loss_train: 0.08601979239364266, Loss_test: 0.08490026566533858\n",
      "2312 - Loss_train: 0.08601823228647232, Loss_test: 0.08489894108338415\n",
      "2313 - Loss_train: 0.08601667298639652, Loss_test: 0.08489761724752064\n",
      "2314 - Loss_train: 0.08601511449504677, Loss_test: 0.0848962941621098\n",
      "2315 - Loss_train: 0.08601355681437217, Loss_test: 0.0848949718243856\n",
      "2316 - Loss_train: 0.0860119999405582, Loss_test: 0.08489365023619344\n",
      "2317 - Loss_train: 0.08601044387334059, Loss_test: 0.08489232939231289\n",
      "2318 - Loss_train: 0.08600888861427244, Loss_test: 0.0848910092968747\n",
      "2319 - Loss_train: 0.08600733416223022, Loss_test: 0.08488968994713796\n",
      "2320 - Loss_train: 0.0860057805153344, Loss_test: 0.08488837134555986\n",
      "2321 - Loss_train: 0.08600422767430092, Loss_test: 0.08488705348631322\n",
      "2322 - Loss_train: 0.08600267563887089, Loss_test: 0.08488573637386124\n",
      "2323 - Loss_train: 0.08600112440848744, Loss_test: 0.08488442000620781\n",
      "2324 - Loss_train: 0.08599957398298347, Loss_test: 0.08488310438272464\n",
      "2325 - Loss_train: 0.08599802436128791, Loss_test: 0.0848817895035935\n",
      "2326 - Loss_train: 0.08599647554515884, Loss_test: 0.08488047537022252\n",
      "2327 - Loss_train: 0.08599492753070367, Loss_test: 0.08487916197804504\n",
      "2328 - Loss_train: 0.08599338032069875, Loss_test: 0.08487784933277974\n",
      "2329 - Loss_train: 0.0859918339124627, Loss_test: 0.08487653742271956\n",
      "2330 - Loss_train: 0.08599028830790485, Loss_test: 0.08487522626102482\n",
      "2331 - Loss_train: 0.08598874350452912, Loss_test: 0.08487391583946619\n",
      "2332 - Loss_train: 0.08598719950070229, Loss_test: 0.08487260615811185\n",
      "2333 - Loss_train: 0.08598565629976093, Loss_test: 0.08487129721821518\n",
      "2334 - Loss_train: 0.08598411389740314, Loss_test: 0.08486998901828179\n",
      "2335 - Loss_train: 0.08598257229531753, Loss_test: 0.08486868155792446\n",
      "2336 - Loss_train: 0.0859810314925883, Loss_test: 0.08486737483757176\n",
      "2337 - Loss_train: 0.08597949148926777, Loss_test: 0.08486606885644189\n",
      "2338 - Loss_train: 0.08597795228469222, Loss_test: 0.08486476361487202\n",
      "2339 - Loss_train: 0.08597641387848426, Loss_test: 0.0848634591094074\n",
      "2340 - Loss_train: 0.08597487627073722, Loss_test: 0.08486215534405077\n",
      "2341 - Loss_train: 0.08597333946083106, Loss_test: 0.08486085231708888\n",
      "2342 - Loss_train: 0.08597180344904619, Loss_test: 0.08485955002697815\n",
      "2343 - Loss_train: 0.08597026823276907, Loss_test: 0.08485824847309678\n",
      "2344 - Loss_train: 0.08596873381270481, Loss_test: 0.0848569476573874\n",
      "2345 - Loss_train: 0.08596720018892742, Loss_test: 0.08485564757663822\n",
      "2346 - Loss_train: 0.08596566736014155, Loss_test: 0.0848543482299662\n",
      "2347 - Loss_train: 0.08596413532689397, Loss_test: 0.08485304962072178\n",
      "2348 - Loss_train: 0.0859626040879521, Loss_test: 0.08485175174413434\n",
      "2349 - Loss_train: 0.08596107364478603, Loss_test: 0.08485045460602575\n",
      "2350 - Loss_train: 0.08595954399385819, Loss_test: 0.08484915820052535\n",
      "2351 - Loss_train: 0.08595801513623287, Loss_test: 0.08484786252569536\n",
      "2352 - Loss_train: 0.08595648707205567, Loss_test: 0.08484656758682958\n",
      "2353 - Loss_train: 0.08595495980175538, Loss_test: 0.08484527338101809\n",
      "2354 - Loss_train: 0.08595343332220483, Loss_test: 0.08484397990708695\n",
      "2355 - Loss_train: 0.08595190763447942, Loss_test: 0.08484268716321064\n",
      "2356 - Loss_train: 0.08595038274007294, Loss_test: 0.08484139515718843\n",
      "2357 - Loss_train: 0.0859488586347524, Loss_test: 0.08484010387706746\n",
      "2358 - Loss_train: 0.08594733532003786, Loss_test: 0.08483881332916335\n",
      "2359 - Loss_train: 0.08594581279744638, Loss_test: 0.0848375235128423\n",
      "2360 - Loss_train: 0.08594429106419049, Loss_test: 0.08483623442684332\n",
      "2361 - Loss_train: 0.08594277011891406, Loss_test: 0.08483494606960244\n",
      "2362 - Loss_train: 0.0859412499642779, Loss_test: 0.0848336584423951\n",
      "2363 - Loss_train: 0.08593973059652577, Loss_test: 0.08483237154300696\n",
      "2364 - Loss_train: 0.08593821201786211, Loss_test: 0.08483108537439743\n",
      "2365 - Loss_train: 0.0859366942279086, Loss_test: 0.08482979993259986\n",
      "2366 - Loss_train: 0.08593517722523895, Loss_test: 0.08482851522127088\n",
      "2367 - Loss_train: 0.0859336610086211, Loss_test: 0.0848272312344804\n",
      "2368 - Loss_train: 0.08593214557801925, Loss_test: 0.08482594797443319\n",
      "2369 - Loss_train: 0.0859306309352895, Loss_test: 0.08482466544327197\n",
      "2370 - Loss_train: 0.0859291170778718, Loss_test: 0.0848233836370545\n",
      "2371 - Loss_train: 0.08592760400410347, Loss_test: 0.0848221025580136\n",
      "2372 - Loss_train: 0.08592609171518284, Loss_test: 0.0848208222010388\n",
      "2373 - Loss_train: 0.08592458021059804, Loss_test: 0.08481954257065415\n",
      "2374 - Loss_train: 0.0859230694901336, Loss_test: 0.08481826366479508\n",
      "2375 - Loss_train: 0.08592155955319015, Loss_test: 0.08481698548281441\n",
      "2376 - Loss_train: 0.0859200504008962, Loss_test: 0.08481570802592321\n",
      "2377 - Loss_train: 0.08591854203009916, Loss_test: 0.0848144312925892\n",
      "2378 - Loss_train: 0.08591703444396304, Loss_test: 0.08481315528117396\n",
      "2379 - Loss_train: 0.08591552763948986, Loss_test: 0.08481187999503709\n",
      "2380 - Loss_train: 0.08591402161530295, Loss_test: 0.08481060543138574\n",
      "2381 - Loss_train: 0.0859125163717324, Loss_test: 0.08480933158627499\n",
      "2382 - Loss_train: 0.08591101190897199, Loss_test: 0.08480805846352134\n",
      "2383 - Loss_train: 0.08590950822736103, Loss_test: 0.08480678606234626\n",
      "2384 - Loss_train: 0.08590800532687536, Loss_test: 0.08480551438325525\n",
      "2385 - Loss_train: 0.08590650320447828, Loss_test: 0.08480424342302997\n",
      "2386 - Loss_train: 0.08590500186114719, Loss_test: 0.08480297318079483\n",
      "2387 - Loss_train: 0.0859035012980348, Loss_test: 0.0848017036636283\n",
      "2388 - Loss_train: 0.08590200151215244, Loss_test: 0.08480043486148918\n",
      "2389 - Loss_train: 0.08590050250402574, Loss_test: 0.08479916677865523\n",
      "2390 - Loss_train: 0.08589900427366211, Loss_test: 0.08479789941566583\n",
      "2391 - Loss_train: 0.08589750682074684, Loss_test: 0.08479663276827382\n",
      "2392 - Loss_train: 0.08589601014469997, Loss_test: 0.08479536683858947\n",
      "2393 - Loss_train: 0.0858945142447308, Loss_test: 0.08479410163203387\n",
      "2394 - Loss_train: 0.0858930191209604, Loss_test: 0.08479283713508674\n",
      "2395 - Loss_train: 0.08589152477260979, Loss_test: 0.08479157335952786\n",
      "2396 - Loss_train: 0.0858900312010316, Loss_test: 0.0847903102985113\n",
      "2397 - Loss_train: 0.08588853840442968, Loss_test: 0.0847890479530912\n",
      "2398 - Loss_train: 0.08588704638194393, Loss_test: 0.08478778632486371\n",
      "2399 - Loss_train: 0.0858855551318621, Loss_test: 0.0847865254099167\n",
      "2400 - Loss_train: 0.08588406465712638, Loss_test: 0.08478526520930725\n",
      "2401 - Loss_train: 0.0858825749539108, Loss_test: 0.08478400572394455\n",
      "2402 - Loss_train: 0.08588108602401205, Loss_test: 0.08478274695124703\n",
      "2403 - Loss_train: 0.08587959786735107, Loss_test: 0.08478148889256092\n",
      "2404 - Loss_train: 0.08587811048141951, Loss_test: 0.0847802315470661\n",
      "2405 - Loss_train: 0.08587662386921373, Loss_test: 0.08477897491183435\n",
      "2406 - Loss_train: 0.08587513802617694, Loss_test: 0.08477771899384104\n",
      "2407 - Loss_train: 0.0858736529544862, Loss_test: 0.08477646378338166\n",
      "2408 - Loss_train: 0.08587216865296343, Loss_test: 0.0847752092888855\n",
      "2409 - Loss_train: 0.08587068512072439, Loss_test: 0.08477395550043443\n",
      "2410 - Loss_train: 0.08586920235976965, Loss_test: 0.08477270242542309\n",
      "2411 - Loss_train: 0.08586772036635784, Loss_test: 0.08477145005816808\n",
      "2412 - Loss_train: 0.08586623914401983, Loss_test: 0.08477019840742511\n",
      "2413 - Loss_train: 0.08586475868974978, Loss_test: 0.08476894745987268\n",
      "2414 - Loss_train: 0.08586327900172752, Loss_test: 0.08476769722532967\n",
      "2415 - Loss_train: 0.08586180008150199, Loss_test: 0.08476644769643078\n",
      "2416 - Loss_train: 0.08586032192853538, Loss_test: 0.08476519887696078\n",
      "2417 - Loss_train: 0.08585884454251075, Loss_test: 0.0847639507635613\n",
      "2418 - Loss_train: 0.08585736792316814, Loss_test: 0.08476270336213763\n",
      "2419 - Loss_train: 0.08585589207100743, Loss_test: 0.08476145666573294\n",
      "2420 - Loss_train: 0.08585441698275012, Loss_test: 0.08476021067639132\n",
      "2421 - Loss_train: 0.0858529426602961, Loss_test: 0.08475896539250527\n",
      "2422 - Loss_train: 0.08585146910192938, Loss_test: 0.08475772081676639\n",
      "2423 - Loss_train: 0.08584999630793132, Loss_test: 0.084756476946664\n",
      "2424 - Loss_train: 0.0858485242781286, Loss_test: 0.08475523377934066\n",
      "2425 - Loss_train: 0.08584705301357372, Loss_test: 0.08475399132012965\n",
      "2426 - Loss_train: 0.08584558251091624, Loss_test: 0.08475274956370343\n",
      "2427 - Loss_train: 0.08584411277254435, Loss_test: 0.08475150851401073\n",
      "2428 - Loss_train: 0.08584264379505531, Loss_test: 0.08475026816504896\n",
      "2429 - Loss_train: 0.08584117557978689, Loss_test: 0.08474902852005863\n",
      "2430 - Loss_train: 0.08583970812618481, Loss_test: 0.08474778957961634\n",
      "2431 - Loss_train: 0.0858382414352724, Loss_test: 0.0847465513417137\n",
      "2432 - Loss_train: 0.08583677550373447, Loss_test: 0.08474531380506817\n",
      "2433 - Loss_train: 0.08583531033281601, Loss_test: 0.08474407697113895\n",
      "2434 - Loss_train: 0.08583384592204872, Loss_test: 0.08474284083914972\n",
      "2435 - Loss_train: 0.08583238227160592, Loss_test: 0.08474160540787498\n",
      "2436 - Loss_train: 0.08583091937954027, Loss_test: 0.0847403706764007\n",
      "2437 - Loss_train: 0.08582945724722987, Loss_test: 0.0847391366464693\n",
      "2438 - Loss_train: 0.08582799587464429, Loss_test: 0.08473790331898826\n",
      "2439 - Loss_train: 0.08582653525859298, Loss_test: 0.0847366706878388\n",
      "2440 - Loss_train: 0.0858250754015882, Loss_test: 0.08473543875833098\n",
      "2441 - Loss_train: 0.08582361630039251, Loss_test: 0.08473420752570432\n",
      "2442 - Loss_train: 0.08582215795598237, Loss_test: 0.08473297699257384\n",
      "2443 - Loss_train: 0.08582070036852944, Loss_test: 0.08473174715620821\n",
      "2444 - Loss_train: 0.08581924353750983, Loss_test: 0.08473051802006815\n",
      "2445 - Loss_train: 0.08581778746163621, Loss_test: 0.08472928958028969\n",
      "2446 - Loss_train: 0.08581633214156997, Loss_test: 0.08472806183763916\n",
      "2447 - Loss_train: 0.08581487757788711, Loss_test: 0.08472683479112257\n",
      "2448 - Loss_train: 0.08581342376731473, Loss_test: 0.08472560844298806\n",
      "2449 - Loss_train: 0.08581197071221647, Loss_test: 0.08472438278965312\n",
      "2450 - Loss_train: 0.08581051840968004, Loss_test: 0.08472315783003147\n",
      "2451 - Loss_train: 0.08580906686035691, Loss_test: 0.08472193356783889\n",
      "2452 - Loss_train: 0.08580761606568817, Loss_test: 0.08472070999932341\n",
      "2453 - Loss_train: 0.08580616602388998, Loss_test: 0.08471948712807914\n",
      "2454 - Loss_train: 0.08580471673419544, Loss_test: 0.08471826494639899\n",
      "2455 - Loss_train: 0.08580326819471411, Loss_test: 0.08471704346138383\n",
      "2456 - Loss_train: 0.08580182040705575, Loss_test: 0.08471582266880429\n",
      "2457 - Loss_train: 0.08580037337042683, Loss_test: 0.08471460256934216\n",
      "2458 - Loss_train: 0.0857989270862802, Loss_test: 0.08471338316421231\n",
      "2459 - Loss_train: 0.08579748155051438, Loss_test: 0.08471216444728466\n",
      "2460 - Loss_train: 0.08579603676517236, Loss_test: 0.0847109464236976\n",
      "2461 - Loss_train: 0.08579459272891234, Loss_test: 0.08470972909288571\n",
      "2462 - Loss_train: 0.08579314944360282, Loss_test: 0.08470851245192662\n",
      "2463 - Loss_train: 0.0857917069051629, Loss_test: 0.08470729650222109\n",
      "2464 - Loss_train: 0.08579026511489521, Loss_test: 0.08470608124299114\n",
      "2465 - Loss_train: 0.08578882407479038, Loss_test: 0.0847048666727005\n",
      "2466 - Loss_train: 0.08578738378182563, Loss_test: 0.08470365279432222\n",
      "2467 - Loss_train: 0.08578594423479781, Loss_test: 0.08470243960132745\n",
      "2468 - Loss_train: 0.0857845054349445, Loss_test: 0.08470122709951954\n",
      "2469 - Loss_train: 0.08578306738277015, Loss_test: 0.08470001528916576\n",
      "2470 - Loss_train: 0.0857816300765013, Loss_test: 0.08469880416157649\n",
      "2471 - Loss_train: 0.08578019351437584, Loss_test: 0.08469759372525147\n",
      "2472 - Loss_train: 0.08577875769910837, Loss_test: 0.08469638397479255\n",
      "2473 - Loss_train: 0.08577732262780928, Loss_test: 0.08469517491314484\n",
      "2474 - Loss_train: 0.08577588830069169, Loss_test: 0.08469396653392931\n",
      "2475 - Loss_train: 0.08577445471761848, Loss_test: 0.08469275884367614\n",
      "2476 - Loss_train: 0.08577302187951157, Loss_test: 0.08469155183986633\n",
      "2477 - Loss_train: 0.08577158978521077, Loss_test: 0.08469034551947402\n",
      "2478 - Loss_train: 0.08577015843203838, Loss_test: 0.08468913988792559\n",
      "2479 - Loss_train: 0.08576872782129034, Loss_test: 0.0846879349363782\n",
      "2480 - Loss_train: 0.08576729795448075, Loss_test: 0.08468673067324535\n",
      "2481 - Loss_train: 0.08576586882965684, Loss_test: 0.08468552709144096\n",
      "2482 - Loss_train: 0.08576444044513948, Loss_test: 0.08468432419477696\n",
      "2483 - Loss_train: 0.08576301280037946, Loss_test: 0.08468312198165512\n",
      "2484 - Loss_train: 0.08576158589720889, Loss_test: 0.08468192044694846\n",
      "2485 - Loss_train: 0.08576015973561887, Loss_test: 0.08468071960081919\n",
      "2486 - Loss_train: 0.08575873431379485, Loss_test: 0.08467951943340273\n",
      "2487 - Loss_train: 0.08575730963026554, Loss_test: 0.08467831995065783\n",
      "2488 - Loss_train: 0.08575588568728004, Loss_test: 0.08467712114553837\n",
      "2489 - Loss_train: 0.08575446248158676, Loss_test: 0.08467592302435353\n",
      "2490 - Loss_train: 0.08575304001416348, Loss_test: 0.08467472558238358\n",
      "2491 - Loss_train: 0.0857516182870443, Loss_test: 0.08467352882202826\n",
      "2492 - Loss_train: 0.08575019729677841, Loss_test: 0.08467233274433213\n",
      "2493 - Loss_train: 0.08574877704400599, Loss_test: 0.08467113734002253\n",
      "2494 - Loss_train: 0.08574735752858159, Loss_test: 0.0846699426202516\n",
      "2495 - Loss_train: 0.08574593874770366, Loss_test: 0.0846687485776568\n",
      "2496 - Loss_train: 0.08574452070286868, Loss_test: 0.0846675552102161\n",
      "2497 - Loss_train: 0.08574310339359427, Loss_test: 0.08466636252413505\n",
      "2498 - Loss_train: 0.08574168682041643, Loss_test: 0.08466517051410431\n",
      "2499 - Loss_train: 0.08574027098139096, Loss_test: 0.08466397918255912\n",
      "2500 - Loss_train: 0.08573885587739281, Loss_test: 0.08466278852764077\n",
      "2501 - Loss_train: 0.08573744150685898, Loss_test: 0.08466159854997922\n",
      "2502 - Loss_train: 0.08573602787053762, Loss_test: 0.08466040924660115\n",
      "2503 - Loss_train: 0.08573461496947983, Loss_test: 0.08465922062270333\n",
      "2504 - Loss_train: 0.08573320279963585, Loss_test: 0.08465803267145393\n",
      "2505 - Loss_train: 0.0857317913622161, Loss_test: 0.0846568453977928\n",
      "2506 - Loss_train: 0.08573038065742783, Loss_test: 0.08465565879558788\n",
      "2507 - Loss_train: 0.0857289706838827, Loss_test: 0.08465447287003365\n",
      "2508 - Loss_train: 0.08572756144260477, Loss_test: 0.08465328761732026\n",
      "2509 - Loss_train: 0.08572615293249003, Loss_test: 0.08465210303963408\n",
      "2510 - Loss_train: 0.08572474515342302, Loss_test: 0.08465091913389552\n",
      "2511 - Loss_train: 0.0857233381046972, Loss_test: 0.08464973590327234\n",
      "2512 - Loss_train: 0.08572193178681296, Loss_test: 0.08464855334591058\n",
      "2513 - Loss_train: 0.08572052619742246, Loss_test: 0.08464737145808608\n",
      "2514 - Loss_train: 0.08571912133851324, Loss_test: 0.0846461902444266\n",
      "2515 - Loss_train: 0.08571771720724514, Loss_test: 0.08464500970009534\n",
      "2516 - Loss_train: 0.08571631380590757, Loss_test: 0.08464382983113879\n",
      "2517 - Loss_train: 0.08571491113069678, Loss_test: 0.0846426506276184\n",
      "2518 - Loss_train: 0.08571350918510107, Loss_test: 0.08464147209910491\n",
      "2519 - Loss_train: 0.0857121079652072, Loss_test: 0.08464029423804209\n",
      "2520 - Loss_train: 0.08571070747251931, Loss_test: 0.08463911704615468\n",
      "2521 - Loss_train: 0.08570930770596279, Loss_test: 0.08463794052553612\n",
      "2522 - Loss_train: 0.08570790866629631, Loss_test: 0.08463676467150286\n",
      "2523 - Loss_train: 0.08570651035244761, Loss_test: 0.08463558948798211\n",
      "2524 - Loss_train: 0.08570511276598343, Loss_test: 0.08463441497388377\n",
      "2525 - Loss_train: 0.08570371590437956, Loss_test: 0.08463324112519573\n",
      "2526 - Loss_train: 0.08570231976603407, Loss_test: 0.0846320679475466\n",
      "2527 - Loss_train: 0.08570092435371568, Loss_test: 0.08463089543683808\n",
      "2528 - Loss_train: 0.08569952966382381, Loss_test: 0.08462972358999855\n",
      "2529 - Loss_train: 0.08569813569733142, Loss_test: 0.08462855241136412\n",
      "2530 - Loss_train: 0.08569674245452653, Loss_test: 0.08462738189673334\n",
      "2531 - Loss_train: 0.08569534993388181, Loss_test: 0.08462621204891159\n",
      "2532 - Loss_train: 0.08569395813604952, Loss_test: 0.08462504286757686\n",
      "2533 - Loss_train: 0.08569256706029059, Loss_test: 0.08462387434942599\n",
      "2534 - Loss_train: 0.08569117670629717, Loss_test: 0.08462270649832983\n",
      "2535 - Loss_train: 0.08568978707380183, Loss_test: 0.0846215393099158\n",
      "2536 - Loss_train: 0.0856883981621773, Loss_test: 0.08462037278604385\n",
      "2537 - Loss_train: 0.08568700997163314, Loss_test: 0.08461920692526799\n",
      "2538 - Loss_train: 0.08568562250247878, Loss_test: 0.08461804172954306\n",
      "2539 - Loss_train: 0.08568423575182513, Loss_test: 0.08461687719490499\n",
      "2540 - Loss_train: 0.08568284972058471, Loss_test: 0.08461571332340424\n",
      "2541 - Loss_train: 0.0856814644084684, Loss_test: 0.08461455011366052\n",
      "2542 - Loss_train: 0.08568007981645341, Loss_test: 0.08461338756781156\n",
      "2543 - Loss_train: 0.08567869594155832, Loss_test: 0.08461222568184032\n",
      "2544 - Loss_train: 0.08567731278472242, Loss_test: 0.08461106445585513\n",
      "2545 - Loss_train: 0.08567593034589843, Loss_test: 0.08460990389219068\n",
      "2546 - Loss_train: 0.08567454862430952, Loss_test: 0.08460874398775381\n",
      "2547 - Loss_train: 0.08567316761888509, Loss_test: 0.08460758474386171\n",
      "2548 - Loss_train: 0.08567178733201312, Loss_test: 0.08460642616170833\n",
      "2549 - Loss_train: 0.0856704077595945, Loss_test: 0.08460526823764085\n",
      "2550 - Loss_train: 0.08566902890315846, Loss_test: 0.08460411097012248\n",
      "2551 - Loss_train: 0.08566765076178186, Loss_test: 0.08460295436345058\n",
      "2552 - Loss_train: 0.08566627333695202, Loss_test: 0.08460179841703937\n",
      "2553 - Loss_train: 0.08566489662531367, Loss_test: 0.08460064312479158\n",
      "2554 - Loss_train: 0.08566352062833059, Loss_test: 0.08459948849167814\n",
      "2555 - Loss_train: 0.08566214534461276, Loss_test: 0.08459833451409947\n",
      "2556 - Loss_train: 0.08566077077519751, Loss_test: 0.08459718119492315\n",
      "2557 - Loss_train: 0.08565939691884494, Loss_test: 0.08459602853088502\n",
      "2558 - Loss_train: 0.0856580237758959, Loss_test: 0.08459487652452896\n",
      "2559 - Loss_train: 0.08565665134638605, Loss_test: 0.08459372517641126\n",
      "2560 - Loss_train: 0.08565527962878612, Loss_test: 0.08459257448095793\n",
      "2561 - Loss_train: 0.08565390862148238, Loss_test: 0.08459142444141696\n",
      "2562 - Loss_train: 0.08565253832550528, Loss_test: 0.08459027505464042\n",
      "2563 - Loss_train: 0.08565116874185864, Loss_test: 0.08458912632455554\n",
      "2564 - Loss_train: 0.08564979986762204, Loss_test: 0.08458797824639416\n",
      "2565 - Loss_train: 0.0856484317035691, Loss_test: 0.08458683082374463\n",
      "2566 - Loss_train: 0.08564706425122122, Loss_test: 0.0845856840528056\n",
      "2567 - Loss_train: 0.08564569750662109, Loss_test: 0.08458453793560738\n",
      "2568 - Loss_train: 0.08564433147275738, Loss_test: 0.08458339247234001\n",
      "2569 - Loss_train: 0.08564296614568034, Loss_test: 0.08458224765908451\n",
      "2570 - Loss_train: 0.08564160152810328, Loss_test: 0.08458110350006409\n",
      "2571 - Loss_train: 0.0856402376177751, Loss_test: 0.08457995998802996\n",
      "2572 - Loss_train: 0.08563887441653635, Loss_test: 0.08457881713147078\n",
      "2573 - Loss_train: 0.08563751192335033, Loss_test: 0.08457767492550074\n",
      "2574 - Loss_train: 0.08563615013549546, Loss_test: 0.0845765333707205\n",
      "2575 - Loss_train: 0.08563478905375466, Loss_test: 0.08457539246333269\n",
      "2576 - Loss_train: 0.0856334286784135, Loss_test: 0.08457425220564069\n",
      "2577 - Loss_train: 0.08563206900894291, Loss_test: 0.08457311259835736\n",
      "2578 - Loss_train: 0.08563071004448064, Loss_test: 0.08457197363963373\n",
      "2579 - Loss_train: 0.08562935178683022, Loss_test: 0.0845708353308556\n",
      "2580 - Loss_train: 0.08562799423256667, Loss_test: 0.08456969766949364\n",
      "2581 - Loss_train: 0.08562663738282607, Loss_test: 0.08456856065610704\n",
      "2582 - Loss_train: 0.08562528123705074, Loss_test: 0.08456742429152135\n",
      "2583 - Loss_train: 0.08562392579507003, Loss_test: 0.08456628857136675\n",
      "2584 - Loss_train: 0.08562257105649955, Loss_test: 0.08456515350081241\n",
      "2585 - Loss_train: 0.08562121702205787, Loss_test: 0.08456401907622388\n",
      "2586 - Loss_train: 0.08561986369014375, Loss_test: 0.08456288530053836\n",
      "2587 - Loss_train: 0.0856185110591684, Loss_test: 0.08456175216676295\n",
      "2588 - Loss_train: 0.08561715912962947, Loss_test: 0.08456061967895424\n",
      "2589 - Loss_train: 0.08561580790203605, Loss_test: 0.08455948783657946\n",
      "2590 - Loss_train: 0.08561445737527909, Loss_test: 0.08455835663994589\n",
      "2591 - Loss_train: 0.08561310754957385, Loss_test: 0.08455722608897101\n",
      "2592 - Loss_train: 0.08561175842373817, Loss_test: 0.08455609618016996\n",
      "2593 - Loss_train: 0.08561040999955974, Loss_test: 0.08455496691683771\n",
      "2594 - Loss_train: 0.08560906227394503, Loss_test: 0.08455383829521385\n",
      "2595 - Loss_train: 0.08560771524725537, Loss_test: 0.08455271031718542\n",
      "2596 - Loss_train: 0.08560636891987003, Loss_test: 0.08455158298206061\n",
      "2597 - Loss_train: 0.08560502329278896, Loss_test: 0.0845504562905861\n",
      "2598 - Loss_train: 0.08560367836204394, Loss_test: 0.084549330241257\n",
      "2599 - Loss_train: 0.08560233413056542, Loss_test: 0.08454820483253633\n",
      "2600 - Loss_train: 0.08560099059694405, Loss_test: 0.08454708006687121\n",
      "2601 - Loss_train: 0.08559964775881736, Loss_test: 0.0845459559419414\n",
      "2602 - Loss_train: 0.0855983056188288, Loss_test: 0.08454483245577887\n",
      "2603 - Loss_train: 0.0855969641753274, Loss_test: 0.08454370961295776\n",
      "2604 - Loss_train: 0.08559562342792247, Loss_test: 0.08454258740781086\n",
      "2605 - Loss_train: 0.08559428337447025, Loss_test: 0.08454146584266888\n",
      "2606 - Loss_train: 0.08559294401842427, Loss_test: 0.08454034491850995\n",
      "2607 - Loss_train: 0.08559160535574045, Loss_test: 0.08453922463013958\n",
      "2608 - Loss_train: 0.08559026738770181, Loss_test: 0.08453810498250687\n",
      "2609 - Loss_train: 0.08558893011432508, Loss_test: 0.0845369859723609\n",
      "2610 - Loss_train: 0.08558759353424467, Loss_test: 0.08453586759942584\n",
      "2611 - Loss_train: 0.08558625764822547, Loss_test: 0.084534749865163\n",
      "2612 - Loss_train: 0.08558492245492141, Loss_test: 0.08453363276753668\n",
      "2613 - Loss_train: 0.08558358795480801, Loss_test: 0.08453251630739587\n",
      "2614 - Loss_train: 0.08558225414701932, Loss_test: 0.08453140048209562\n",
      "2615 - Loss_train: 0.08558092103239358, Loss_test: 0.08453028529662125\n",
      "2616 - Loss_train: 0.08557958860879114, Loss_test: 0.084529170744529\n",
      "2617 - Loss_train: 0.08557825687569293, Loss_test: 0.08452805682792204\n",
      "2618 - Loss_train: 0.085576925834081, Loss_test: 0.0845269435467883\n",
      "2619 - Loss_train: 0.08557559548257158, Loss_test: 0.08452583089939973\n",
      "2620 - Loss_train: 0.0855742658219565, Loss_test: 0.08452471888804965\n",
      "2621 - Loss_train: 0.08557293685288586, Loss_test: 0.08452360751117663\n",
      "2622 - Loss_train: 0.0855716085731023, Loss_test: 0.08452249676787404\n",
      "2623 - Loss_train: 0.08557028098120222, Loss_test: 0.08452138665801047\n",
      "2624 - Loss_train: 0.08556895407972857, Loss_test: 0.0845202771815909\n",
      "2625 - Loss_train: 0.08556762786690256, Loss_test: 0.08451916833779449\n",
      "2626 - Loss_train: 0.08556630234033248, Loss_test: 0.08451806012721466\n",
      "2627 - Loss_train: 0.08556497750200596, Loss_test: 0.0845169525462599\n",
      "2628 - Loss_train: 0.08556365335101465, Loss_test: 0.08451584559787273\n",
      "2629 - Loss_train: 0.08556232988726063, Loss_test: 0.08451473928006073\n",
      "2630 - Loss_train: 0.0855610071100659, Loss_test: 0.08451363359483331\n",
      "2631 - Loss_train: 0.08555968501920759, Loss_test: 0.08451252853888282\n",
      "2632 - Loss_train: 0.0855583636146514, Loss_test: 0.08451142411401105\n",
      "2633 - Loss_train: 0.08555704289705388, Loss_test: 0.08451032032034764\n",
      "2634 - Loss_train: 0.08555572286329727, Loss_test: 0.0845092171560736\n",
      "2635 - Loss_train: 0.08555440351431312, Loss_test: 0.08450811461929536\n",
      "2636 - Loss_train: 0.08555308485168642, Loss_test: 0.08450701271396248\n",
      "2637 - Loss_train: 0.08555176687331516, Loss_test: 0.08450591143813437\n",
      "2638 - Loss_train: 0.0855504495785376, Loss_test: 0.08450481078903083\n",
      "2639 - Loss_train: 0.08554913296666969, Loss_test: 0.08450371076640208\n",
      "2640 - Loss_train: 0.08554781703714534, Loss_test: 0.08450261137520702\n",
      "2641 - Loss_train: 0.08554650179249149, Loss_test: 0.08450151260830946\n",
      "2642 - Loss_train: 0.08554518722862371, Loss_test: 0.08450041446761156\n",
      "2643 - Loss_train: 0.08554387334738345, Loss_test: 0.08449931695617699\n",
      "2644 - Loss_train: 0.08554256014743257, Loss_test: 0.08449822006944681\n",
      "2645 - Loss_train: 0.08554124762908806, Loss_test: 0.08449712380706235\n",
      "2646 - Loss_train: 0.08553993579126518, Loss_test: 0.08449602817108898\n",
      "2647 - Loss_train: 0.08553862463638673, Loss_test: 0.08449493316498263\n",
      "2648 - Loss_train: 0.08553731416157473, Loss_test: 0.08449383878079515\n",
      "2649 - Loss_train: 0.085536004365446, Loss_test: 0.08449274501942335\n",
      "2650 - Loss_train: 0.08553469525030231, Loss_test: 0.08449165188569903\n",
      "2651 - Loss_train: 0.08553338681304846, Loss_test: 0.08449055937309624\n",
      "2652 - Loss_train: 0.0855320790552372, Loss_test: 0.08448946748327274\n",
      "2653 - Loss_train: 0.08553077197620917, Loss_test: 0.08448837622000331\n",
      "2654 - Loss_train: 0.08552946557459862, Loss_test: 0.0844872855770638\n",
      "2655 - Loss_train: 0.0855281598514874, Loss_test: 0.08448619555913163\n",
      "2656 - Loss_train: 0.08552685480530174, Loss_test: 0.08448510615975345\n",
      "2657 - Loss_train: 0.08552555043620373, Loss_test: 0.0844840173845505\n",
      "2658 - Loss_train: 0.0855242467445496, Loss_test: 0.08448292923016087\n",
      "2659 - Loss_train: 0.08552294372937237, Loss_test: 0.08448184169719676\n",
      "2660 - Loss_train: 0.08552164139008099, Loss_test: 0.08448075478548683\n",
      "2661 - Loss_train: 0.08552033972749645, Loss_test: 0.0844796684947505\n",
      "2662 - Loss_train: 0.08551903873938005, Loss_test: 0.08447858282323771\n",
      "2663 - Loss_train: 0.08551773842722699, Loss_test: 0.08447749777246957\n",
      "2664 - Loss_train: 0.08551643878916786, Loss_test: 0.08447641334128644\n",
      "2665 - Loss_train: 0.08551513982688261, Loss_test: 0.0844753295282515\n",
      "2666 - Loss_train: 0.0855138415377851, Loss_test: 0.08447424633646981\n",
      "2667 - Loss_train: 0.08551254392213067, Loss_test: 0.0844731637600099\n",
      "2668 - Loss_train: 0.08551124697960823, Loss_test: 0.08447208180394453\n",
      "2669 - Loss_train: 0.0855099507114285, Loss_test: 0.08447100046652868\n",
      "2670 - Loss_train: 0.08550865511524369, Loss_test: 0.08446991974456036\n",
      "2671 - Loss_train: 0.08550736019072287, Loss_test: 0.08446883964112414\n",
      "2672 - Loss_train: 0.08550606593817105, Loss_test: 0.08446776015361217\n",
      "2673 - Loss_train: 0.08550477235724278, Loss_test: 0.08446668128241633\n",
      "2674 - Loss_train: 0.08550347944774193, Loss_test: 0.08446560302628733\n",
      "2675 - Loss_train: 0.08550218720945131, Loss_test: 0.08446452538937471\n",
      "2676 - Loss_train: 0.08550089564185609, Loss_test: 0.08446344836543276\n",
      "2677 - Loss_train: 0.0854996047439981, Loss_test: 0.08446237195957416\n",
      "2678 - Loss_train: 0.08549831451749801, Loss_test: 0.0844612961664096\n",
      "2679 - Loss_train: 0.08549702496053004, Loss_test: 0.08446022098980971\n",
      "2680 - Loss_train: 0.08549573607269083, Loss_test: 0.08445914642542154\n",
      "2681 - Loss_train: 0.08549444785299086, Loss_test: 0.08445807247547682\n",
      "2682 - Loss_train: 0.08549316030143723, Loss_test: 0.08445699913927246\n",
      "2683 - Loss_train: 0.08549187341739341, Loss_test: 0.08445592641410445\n",
      "2684 - Loss_train: 0.08549058720179444, Loss_test: 0.08445485430487913\n",
      "2685 - Loss_train: 0.08548930165343886, Loss_test: 0.08445378280706156\n",
      "2686 - Loss_train: 0.08548801677319254, Loss_test: 0.08445271192191417\n",
      "2687 - Loss_train: 0.08548673255988772, Loss_test: 0.08445164164765087\n",
      "2688 - Loss_train: 0.08548544901160214, Loss_test: 0.08445057198334564\n",
      "2689 - Loss_train: 0.08548416612919156, Loss_test: 0.08444950293520959\n",
      "2690 - Loss_train: 0.08548288391374771, Loss_test: 0.08444843449169553\n",
      "2691 - Loss_train: 0.08548160236308389, Loss_test: 0.08444736666364357\n",
      "2692 - Loss_train: 0.08548032147751518, Loss_test: 0.08444629944402483\n",
      "2693 - Loss_train: 0.08547904125647413, Loss_test: 0.08444523283314678\n",
      "2694 - Loss_train: 0.0854777617011996, Loss_test: 0.08444416683519596\n",
      "2695 - Loss_train: 0.08547648280879042, Loss_test: 0.08444310144392268\n",
      "2696 - Loss_train: 0.08547520458037368, Loss_test: 0.08444203666225258\n",
      "2697 - Loss_train: 0.08547392701471555, Loss_test: 0.08444097248902296\n",
      "2698 - Loss_train: 0.08547265011323038, Loss_test: 0.08443990892510018\n",
      "2699 - Loss_train: 0.08547137387327133, Loss_test: 0.08443884596675798\n",
      "2700 - Loss_train: 0.08547009829695391, Loss_test: 0.08443778361896441\n",
      "2701 - Loss_train: 0.08546882338125072, Loss_test: 0.08443672187647526\n",
      "2702 - Loss_train: 0.08546754912853943, Loss_test: 0.08443566074142862\n",
      "2703 - Loss_train: 0.08546627553561349, Loss_test: 0.08443460021223119\n",
      "2704 - Loss_train: 0.085465002604509, Loss_test: 0.08443354029113043\n",
      "2705 - Loss_train: 0.0854637303334732, Loss_test: 0.08443248097349676\n",
      "2706 - Loss_train: 0.08546245872214259, Loss_test: 0.08443142226387178\n",
      "2707 - Loss_train: 0.0854611877709838, Loss_test: 0.08443036415768149\n",
      "2708 - Loss_train: 0.08545991747918101, Loss_test: 0.08442930665723233\n",
      "2709 - Loss_train: 0.08545864784672273, Loss_test: 0.08442824976047902\n",
      "2710 - Loss_train: 0.08545737887337018, Loss_test: 0.08442719346756325\n",
      "2711 - Loss_train: 0.08545611055886347, Loss_test: 0.08442613778094214\n",
      "2712 - Loss_train: 0.08545484290219686, Loss_test: 0.08442508269540855\n",
      "2713 - Loss_train: 0.08545357590501879, Loss_test: 0.08442402821630729\n",
      "2714 - Loss_train: 0.08545230956367877, Loss_test: 0.08442297433928146\n",
      "2715 - Loss_train: 0.08545104387971608, Loss_test: 0.08442192106457327\n",
      "2716 - Loss_train: 0.08544977885291416, Loss_test: 0.08442086839194836\n",
      "2717 - Loss_train: 0.08544851448231693, Loss_test: 0.08441981632001301\n",
      "2718 - Loss_train: 0.0854472507679898, Loss_test: 0.08441876485246949\n",
      "2719 - Loss_train: 0.0854459877103774, Loss_test: 0.08441771398547335\n",
      "2720 - Loss_train: 0.08544472530675669, Loss_test: 0.08441666371883243\n",
      "2721 - Loss_train: 0.08544346356032302, Loss_test: 0.08441561405363164\n",
      "2722 - Loss_train: 0.08544220246673534, Loss_test: 0.0844145649889585\n",
      "2723 - Loss_train: 0.08544094202776216, Loss_test: 0.08441351652196645\n",
      "2724 - Loss_train: 0.08543968224414153, Loss_test: 0.0844124686595228\n",
      "2725 - Loss_train: 0.08543842311260663, Loss_test: 0.08441142139158794\n",
      "2726 - Loss_train: 0.08543716463508884, Loss_test: 0.08441037472440459\n",
      "2727 - Loss_train: 0.08543590681051484, Loss_test: 0.08440932865608756\n",
      "2728 - Loss_train: 0.0854346496391668, Loss_test: 0.08440828318493152\n",
      "2729 - Loss_train: 0.085433393119813, Loss_test: 0.08440723831335259\n",
      "2730 - Loss_train: 0.08543213725391648, Loss_test: 0.08440619404122442\n",
      "2731 - Loss_train: 0.08543088203852332, Loss_test: 0.08440515036441315\n",
      "2732 - Loss_train: 0.08542962747464569, Loss_test: 0.08440410728359138\n",
      "2733 - Loss_train: 0.08542837356336294, Loss_test: 0.08440306480302787\n",
      "2734 - Loss_train: 0.08542712030106779, Loss_test: 0.0844020229166556\n",
      "2735 - Loss_train: 0.08542586768908735, Loss_test: 0.08440098162739305\n",
      "2736 - Loss_train: 0.08542461572738494, Loss_test: 0.08439994093178338\n",
      "2737 - Loss_train: 0.08542336441647201, Loss_test: 0.08439890083577029\n",
      "2738 - Loss_train: 0.08542211375411202, Loss_test: 0.084397861330976\n",
      "2739 - Loss_train: 0.08542086374173337, Loss_test: 0.08439682242349501\n",
      "2740 - Loss_train: 0.08541961437734928, Loss_test: 0.08439578411015243\n",
      "2741 - Loss_train: 0.08541836566072548, Loss_test: 0.08439474638945041\n",
      "2742 - Loss_train: 0.08541711759390784, Loss_test: 0.08439370926530575\n",
      "2743 - Loss_train: 0.08541587017270778, Loss_test: 0.08439267273435096\n",
      "2744 - Loss_train: 0.08541462339901008, Loss_test: 0.08439163679324206\n",
      "2745 - Loss_train: 0.08541337727267412, Loss_test: 0.08439060144656774\n",
      "2746 - Loss_train: 0.08541213179288326, Loss_test: 0.08438956669417473\n",
      "2747 - Loss_train: 0.08541088696127067, Loss_test: 0.08438853253281652\n",
      "2748 - Loss_train: 0.08540964277481979, Loss_test: 0.08438749896547186\n",
      "2749 - Loss_train: 0.08540839923370151, Loss_test: 0.08438646598693476\n",
      "2750 - Loss_train: 0.08540715633775668, Loss_test: 0.08438543360234266\n",
      "2751 - Loss_train: 0.08540591408729624, Loss_test: 0.08438440180650744\n",
      "2752 - Loss_train: 0.08540467248229029, Loss_test: 0.08438337060488452\n",
      "2753 - Loss_train: 0.08540343152040361, Loss_test: 0.08438233999036959\n",
      "2754 - Loss_train: 0.08540219120240863, Loss_test: 0.08438130996541145\n",
      "2755 - Loss_train: 0.0854009515281901, Loss_test: 0.08438028053204848\n",
      "2756 - Loss_train: 0.0853997124985362, Loss_test: 0.08437925168904738\n",
      "2757 - Loss_train: 0.08539847411073659, Loss_test: 0.08437822343206185\n",
      "2758 - Loss_train: 0.08539723636642087, Loss_test: 0.08437719576685287\n",
      "2759 - Loss_train: 0.08539599926381691, Loss_test: 0.08437616868758756\n",
      "2760 - Loss_train: 0.0853947628042313, Loss_test: 0.08437514219887678\n",
      "2761 - Loss_train: 0.08539352698654912, Loss_test: 0.08437411629792183\n",
      "2762 - Loss_train: 0.08539229180919906, Loss_test: 0.0843730909826602\n",
      "2763 - Loss_train: 0.08539105727237374, Loss_test: 0.08437206625614298\n",
      "2764 - Loss_train: 0.08538982337635052, Loss_test: 0.08437104211376764\n",
      "2765 - Loss_train: 0.08538859012150464, Loss_test: 0.08437001855957528\n",
      "2766 - Loss_train: 0.0853873575056128, Loss_test: 0.08436899559218938\n",
      "2767 - Loss_train: 0.08538612552952919, Loss_test: 0.08436797321019206\n",
      "2768 - Loss_train: 0.08538489419468567, Loss_test: 0.08436695141335344\n",
      "2769 - Loss_train: 0.08538366349774126, Loss_test: 0.08436593020341304\n",
      "2770 - Loss_train: 0.08538243344033151, Loss_test: 0.08436490957803733\n",
      "2771 - Loss_train: 0.08538120402011452, Loss_test: 0.08436388953389887\n",
      "2772 - Loss_train: 0.0853799752378645, Loss_test: 0.08436287007969552\n",
      "2773 - Loss_train: 0.08537874709346974, Loss_test: 0.08436185120395347\n",
      "2774 - Loss_train: 0.08537751958679575, Loss_test: 0.08436083291678784\n",
      "2775 - Loss_train: 0.08537629271682533, Loss_test: 0.08435981520822482\n",
      "2776 - Loss_train: 0.08537506648409914, Loss_test: 0.08435879808704008\n",
      "2777 - Loss_train: 0.08537384088765272, Loss_test: 0.08435778154835095\n",
      "2778 - Loss_train: 0.08537261592758073, Loss_test: 0.08435676558933511\n",
      "2779 - Loss_train: 0.0853713916027818, Loss_test: 0.08435575021454228\n",
      "2780 - Loss_train: 0.08537016791440807, Loss_test: 0.08435473542075081\n",
      "2781 - Loss_train: 0.08536894486039288, Loss_test: 0.08435372120942178\n",
      "2782 - Loss_train: 0.08536772244057834, Loss_test: 0.0843527075767391\n",
      "2783 - Loss_train: 0.08536650065529455, Loss_test: 0.0843516945277537\n",
      "2784 - Loss_train: 0.08536527950371671, Loss_test: 0.08435068205589276\n",
      "2785 - Loss_train: 0.08536405898599332, Loss_test: 0.084349670166842\n",
      "2786 - Loss_train: 0.0853628391021997, Loss_test: 0.08434865885761596\n",
      "2787 - Loss_train: 0.08536161985155391, Loss_test: 0.08434764812665935\n",
      "2788 - Loss_train: 0.08536040123574873, Loss_test: 0.08434663797830617\n",
      "2789 - Loss_train: 0.08535918325107977, Loss_test: 0.08434562840567254\n",
      "2790 - Loss_train: 0.0853579658972578, Loss_test: 0.084344619412339\n",
      "2791 - Loss_train: 0.08535674917563224, Loss_test: 0.08434361099837781\n",
      "2792 - Loss_train: 0.08535553308520402, Loss_test: 0.08434260316031109\n",
      "2793 - Loss_train: 0.08535431762706357, Loss_test: 0.08434159590174278\n",
      "2794 - Loss_train: 0.08535310279863234, Loss_test: 0.08434058922148431\n",
      "2795 - Loss_train: 0.08535188860091171, Loss_test: 0.08433958311440878\n",
      "2796 - Loss_train: 0.0853506750351584, Loss_test: 0.08433857758754813\n",
      "2797 - Loss_train: 0.0853494620990668, Loss_test: 0.08433757263870932\n",
      "2798 - Loss_train: 0.08534824979240754, Loss_test: 0.08433656826426479\n",
      "2799 - Loss_train: 0.08534703811390898, Loss_test: 0.0843355644632875\n",
      "2800 - Loss_train: 0.08534582706393466, Loss_test: 0.08433456124086036\n",
      "2801 - Loss_train: 0.08534461664266536, Loss_test: 0.08433355859167957\n",
      "2802 - Loss_train: 0.08534340684896419, Loss_test: 0.08433255651979307\n",
      "2803 - Loss_train: 0.0853421976849286, Loss_test: 0.08433155501960336\n",
      "2804 - Loss_train: 0.08534098914690928, Loss_test: 0.08433055409447097\n",
      "2805 - Loss_train: 0.08533978123757698, Loss_test: 0.08432955374450218\n",
      "2806 - Loss_train: 0.08533857395467252, Loss_test: 0.08432855396771843\n",
      "2807 - Loss_train: 0.08533736729896088, Loss_test: 0.08432755476596465\n",
      "2808 - Loss_train: 0.0853361612695408, Loss_test: 0.08432655613367585\n",
      "2809 - Loss_train: 0.08533495586579791, Loss_test: 0.08432555807875555\n",
      "2810 - Loss_train: 0.0853337510864137, Loss_test: 0.08432456059207363\n",
      "2811 - Loss_train: 0.08533254693245253, Loss_test: 0.08432356367738839\n",
      "2812 - Loss_train: 0.08533134340465942, Loss_test: 0.08432256733707663\n",
      "2813 - Loss_train: 0.08533014050067134, Loss_test: 0.08432157156723478\n",
      "2814 - Loss_train: 0.08532893822088476, Loss_test: 0.08432057636720772\n",
      "2815 - Loss_train: 0.08532773656494302, Loss_test: 0.0843195817389357\n",
      "2816 - Loss_train: 0.08532653553282121, Loss_test: 0.08431858767987459\n",
      "2817 - Loss_train: 0.08532533512368748, Loss_test: 0.08431759419337549\n",
      "2818 - Loss_train: 0.08532413533766996, Loss_test: 0.08431660127545074\n",
      "2819 - Loss_train: 0.08532293617465458, Loss_test: 0.08431560892689656\n",
      "2820 - Loss_train: 0.08532173763390749, Loss_test: 0.08431461714879239\n",
      "2821 - Loss_train: 0.08532053971532032, Loss_test: 0.0843136259370266\n",
      "2822 - Loss_train: 0.08531934241838939, Loss_test: 0.08431263529807032\n",
      "2823 - Loss_train: 0.08531814574289037, Loss_test: 0.08431164522404322\n",
      "2824 - Loss_train: 0.08531694968858847, Loss_test: 0.0843106557202718\n",
      "2825 - Loss_train: 0.08531575425500094, Loss_test: 0.08430966678272674\n",
      "2826 - Loss_train: 0.08531455944174184, Loss_test: 0.08430867841359106\n",
      "2827 - Loss_train: 0.08531336524873762, Loss_test: 0.08430769061227854\n",
      "2828 - Loss_train: 0.08531217167620361, Loss_test: 0.08430670337661511\n",
      "2829 - Loss_train: 0.0853109787225809, Loss_test: 0.08430571670649797\n",
      "2830 - Loss_train: 0.08530978638817391, Loss_test: 0.08430473060541269\n",
      "2831 - Loss_train: 0.0853085946722983, Loss_test: 0.08430374506826167\n",
      "2832 - Loss_train: 0.08530740357516785, Loss_test: 0.08430276009712291\n",
      "2833 - Loss_train: 0.08530621309749552, Loss_test: 0.08430177569138611\n",
      "2834 - Loss_train: 0.0853050232375925, Loss_test: 0.08430079185328543\n",
      "2835 - Loss_train: 0.08530383399532027, Loss_test: 0.0842998085763544\n",
      "2836 - Loss_train: 0.08530264537000722, Loss_test: 0.08429882586605746\n",
      "2837 - Loss_train: 0.08530145736217785, Loss_test: 0.08429784372047484\n",
      "2838 - Loss_train: 0.08530026996965376, Loss_test: 0.08429686213406706\n",
      "2839 - Loss_train: 0.08529908319327716, Loss_test: 0.08429588111689988\n",
      "2840 - Loss_train: 0.08529789703409713, Loss_test: 0.08429490065964977\n",
      "2841 - Loss_train: 0.08529671148916569, Loss_test: 0.08429392076605038\n",
      "2842 - Loss_train: 0.08529552655973195, Loss_test: 0.08429294143194019\n",
      "2843 - Loss_train: 0.08529434224575408, Loss_test: 0.08429196266309487\n",
      "2844 - Loss_train: 0.08529315854603328, Loss_test: 0.08429098445655024\n",
      "2845 - Loss_train: 0.08529197546065045, Loss_test: 0.08429000680975281\n",
      "2846 - Loss_train: 0.08529079299051391, Loss_test: 0.08428902972563904\n",
      "2847 - Loss_train: 0.08528961113278828, Loss_test: 0.08428805320225556\n",
      "2848 - Loss_train: 0.08528842988856704, Loss_test: 0.08428707723977412\n",
      "2849 - Loss_train: 0.08528724925739113, Loss_test: 0.084286101836725\n",
      "2850 - Loss_train: 0.08528606923923673, Loss_test: 0.0842851269928183\n",
      "2851 - Loss_train: 0.08528488983478129, Loss_test: 0.08428415271319588\n",
      "2852 - Loss_train: 0.08528371104248883, Loss_test: 0.0842831789908472\n",
      "2853 - Loss_train: 0.08528253286074494, Loss_test: 0.0842822058272152\n",
      "2854 - Loss_train: 0.08528135529184379, Loss_test: 0.08428123322317116\n",
      "2855 - Loss_train: 0.08528017833369186, Loss_test: 0.08428026117771947\n",
      "2856 - Loss_train: 0.08527900198497813, Loss_test: 0.08427928968776996\n",
      "2857 - Loss_train: 0.08527782624717145, Loss_test: 0.08427831875826616\n",
      "2858 - Loss_train: 0.08527665112011983, Loss_test: 0.0842773483858869\n",
      "2859 - Loss_train: 0.0852754766029127, Loss_test: 0.08427637857030243\n",
      "2860 - Loss_train: 0.08527430269647296, Loss_test: 0.08427540931561729\n",
      "2861 - Loss_train: 0.08527312939807183, Loss_test: 0.08427444061382947\n",
      "2862 - Loss_train: 0.08527195670917449, Loss_test: 0.08427347246891193\n",
      "2863 - Loss_train: 0.08527078462890356, Loss_test: 0.08427250487956998\n",
      "2864 - Loss_train: 0.08526961315786064, Loss_test: 0.08427153784967961\n",
      "2865 - Loss_train: 0.08526844229376311, Loss_test: 0.08427057137226654\n",
      "2866 - Loss_train: 0.08526727203721439, Loss_test: 0.08426960545080403\n",
      "2867 - Loss_train: 0.08526610238783333, Loss_test: 0.08426864008414804\n",
      "2868 - Loss_train: 0.0852649333458146, Loss_test: 0.08426767527022595\n",
      "2869 - Loss_train: 0.08526376491051184, Loss_test: 0.08426671101309537\n",
      "2870 - Loss_train: 0.08526259708198655, Loss_test: 0.08426574731001346\n",
      "2871 - Loss_train: 0.08526142985947531, Loss_test: 0.08426478415998971\n",
      "2872 - Loss_train: 0.08526026324224283, Loss_test: 0.0842638215627508\n",
      "2873 - Loss_train: 0.08525909723083136, Loss_test: 0.08426285951986183\n",
      "2874 - Loss_train: 0.08525793182581358, Loss_test: 0.08426189802865952\n",
      "2875 - Loss_train: 0.08525676702581483, Loss_test: 0.08426093709276894\n",
      "2876 - Loss_train: 0.0852556028290838, Loss_test: 0.08425997670750303\n",
      "2877 - Loss_train: 0.0852544392379217, Loss_test: 0.08425901687432834\n",
      "2878 - Loss_train: 0.08525327624986405, Loss_test: 0.08425805759101782\n",
      "2879 - Loss_train: 0.08525211386576566, Loss_test: 0.08425709886287346\n",
      "2880 - Loss_train: 0.08525095208562182, Loss_test: 0.08425614068354183\n",
      "2881 - Loss_train: 0.08524979090753347, Loss_test: 0.08425518305529658\n",
      "2882 - Loss_train: 0.0852486303333509, Loss_test: 0.0842542259747908\n",
      "2883 - Loss_train: 0.085247470359932, Loss_test: 0.0842532694496002\n",
      "2884 - Loss_train: 0.08524631098886996, Loss_test: 0.08425231347051829\n",
      "2885 - Loss_train: 0.08524515221953036, Loss_test: 0.08425135803887174\n",
      "2886 - Loss_train: 0.08524399405178044, Loss_test: 0.08425040316359159\n",
      "2887 - Loss_train: 0.08524283648658934, Loss_test: 0.08424944883131522\n",
      "2888 - Loss_train: 0.08524167952183896, Loss_test: 0.0842484950523361\n",
      "2889 - Loss_train: 0.08524052315616538, Loss_test: 0.0842475418173983\n",
      "2890 - Loss_train: 0.08523936739253545, Loss_test: 0.08424658913309659\n",
      "2891 - Loss_train: 0.08523821222739901, Loss_test: 0.08424563699467008\n",
      "2892 - Loss_train: 0.08523705766317145, Loss_test: 0.08424468540660723\n",
      "2893 - Loss_train: 0.08523590369712174, Loss_test: 0.08424373436306179\n",
      "2894 - Loss_train: 0.08523475032979692, Loss_test: 0.08424278386706571\n",
      "2895 - Loss_train: 0.08523359756319848, Loss_test: 0.08424183391929686\n",
      "2896 - Loss_train: 0.0852324453930996, Loss_test: 0.08424088451688568\n",
      "2897 - Loss_train: 0.08523129382239981, Loss_test: 0.08423993566035676\n",
      "2898 - Loss_train: 0.08523014284800542, Loss_test: 0.08423898734916398\n",
      "2899 - Loss_train: 0.08522899247089863, Loss_test: 0.08423803958257363\n",
      "2900 - Loss_train: 0.08522784269245191, Loss_test: 0.08423709236301456\n",
      "2901 - Loss_train: 0.08522669350987869, Loss_test: 0.08423614568656919\n",
      "2902 - Loss_train: 0.08522554492331967, Loss_test: 0.08423519955610775\n",
      "2903 - Loss_train: 0.08522439693422645, Loss_test: 0.08423425396829776\n",
      "2904 - Loss_train: 0.08522324953934542, Loss_test: 0.084233308925954\n",
      "2905 - Loss_train: 0.08522210274039664, Loss_test: 0.08423236442307676\n",
      "2906 - Loss_train: 0.08522095653671402, Loss_test: 0.08423142046759896\n",
      "2907 - Loss_train: 0.08521981092831514, Loss_test: 0.08423047705573619\n",
      "2908 - Loss_train: 0.08521866591415765, Loss_test: 0.08422953418302184\n",
      "2909 - Loss_train: 0.08521752149452068, Loss_test: 0.0842285918549773\n",
      "2910 - Loss_train: 0.08521637766983033, Loss_test: 0.08422765006955552\n",
      "2911 - Loss_train: 0.08521523443772194, Loss_test: 0.08422670882596596\n",
      "2912 - Loss_train: 0.08521409179902058, Loss_test: 0.08422576812194647\n",
      "2913 - Loss_train: 0.08521294975412697, Loss_test: 0.08422482796147306\n",
      "2914 - Loss_train: 0.08521180830246869, Loss_test: 0.08422388834089717\n",
      "2915 - Loss_train: 0.0852106674425825, Loss_test: 0.08422294926144666\n",
      "2916 - Loss_train: 0.08520952717588147, Loss_test: 0.0842220107219147\n",
      "2917 - Loss_train: 0.08520838750127402, Loss_test: 0.08422107272428236\n",
      "2918 - Loss_train: 0.08520724841693528, Loss_test: 0.08422013526744436\n",
      "2919 - Loss_train: 0.08520610992366356, Loss_test: 0.08421919834509065\n",
      "2920 - Loss_train: 0.08520497202115145, Loss_test: 0.08421826196407041\n",
      "2921 - Loss_train: 0.08520383470967613, Loss_test: 0.0842173261226492\n",
      "2922 - Loss_train: 0.08520269798824089, Loss_test: 0.08421639081767511\n",
      "2923 - Loss_train: 0.08520156185745004, Loss_test: 0.08421545605327278\n",
      "2924 - Loss_train: 0.0852004263173372, Loss_test: 0.08421452182699009\n",
      "2925 - Loss_train: 0.08519929136533401, Loss_test: 0.08421358813901944\n",
      "2926 - Loss_train: 0.0851981570031367, Loss_test: 0.08421265498642463\n",
      "2927 - Loss_train: 0.08519702322881792, Loss_test: 0.08421172237215307\n",
      "2928 - Loss_train: 0.08519589004377594, Loss_test: 0.0842107902947343\n",
      "2929 - Loss_train: 0.08519475744774954, Loss_test: 0.08420985875550442\n",
      "2930 - Loss_train: 0.08519362543827609, Loss_test: 0.08420892775040126\n",
      "2931 - Loss_train: 0.08519249401669905, Loss_test: 0.08420799727910994\n",
      "2932 - Loss_train: 0.08519136318237726, Loss_test: 0.08420706734787675\n",
      "2933 - Loss_train: 0.08519023293653641, Loss_test: 0.08420613795165972\n",
      "2934 - Loss_train: 0.0851891032772467, Loss_test: 0.08420520908999563\n",
      "2935 - Loss_train: 0.08518797420428185, Loss_test: 0.0842042807642398\n",
      "2936 - Loss_train: 0.0851868457158872, Loss_test: 0.08420335296842395\n",
      "2937 - Loss_train: 0.0851857178135867, Loss_test: 0.08420242571104734\n",
      "2938 - Loss_train: 0.08518459049809571, Loss_test: 0.0842014989857788\n",
      "2939 - Loss_train: 0.08518346376666394, Loss_test: 0.08420057279513908\n",
      "2940 - Loss_train: 0.08518233762065924, Loss_test: 0.08419964713930152\n",
      "2941 - Loss_train: 0.08518121205839141, Loss_test: 0.08419872201483708\n",
      "2942 - Loss_train: 0.08518008708164944, Loss_test: 0.0841977974222954\n",
      "2943 - Loss_train: 0.08517896268729744, Loss_test: 0.08419687336348455\n",
      "2944 - Loss_train: 0.08517783887676496, Loss_test: 0.08419594983501295\n",
      "2945 - Loss_train: 0.08517671564984942, Loss_test: 0.08419502684023922\n",
      "2946 - Loss_train: 0.08517559300620958, Loss_test: 0.08419410437678386\n",
      "2947 - Loss_train: 0.08517447094521155, Loss_test: 0.08419318244504125\n",
      "2948 - Loss_train: 0.08517334946647452, Loss_test: 0.08419226104172065\n",
      "2949 - Loss_train: 0.08517222857053085, Loss_test: 0.08419134017510499\n",
      "2950 - Loss_train: 0.08517110825575894, Loss_test: 0.08419041983289298\n",
      "2951 - Loss_train: 0.08516998852287395, Loss_test: 0.08418950002424842\n",
      "2952 - Loss_train: 0.08516886937234046, Loss_test: 0.08418858074451559\n",
      "2953 - Loss_train: 0.08516775080142182, Loss_test: 0.0841876619944457\n",
      "2954 - Loss_train: 0.08516663281219355, Loss_test: 0.08418674377577075\n",
      "2955 - Loss_train: 0.08516551540190974, Loss_test: 0.0841858260833194\n",
      "2956 - Loss_train: 0.08516439857203661, Loss_test: 0.08418490891997196\n",
      "2957 - Loss_train: 0.0851632823225895, Loss_test: 0.08418399228588683\n",
      "2958 - Loss_train: 0.08516216665253283, Loss_test: 0.084183076180093\n",
      "2959 - Loss_train: 0.08516105156219073, Loss_test: 0.08418216060136118\n",
      "2960 - Loss_train: 0.08515993705053565, Loss_test: 0.0841812455530724\n",
      "2961 - Loss_train: 0.08515882311645209, Loss_test: 0.08418033102794426\n",
      "2962 - Loss_train: 0.0851577097606847, Loss_test: 0.08417941703366644\n",
      "2963 - Loss_train: 0.0851565969828418, Loss_test: 0.08417850356218334\n",
      "2964 - Loss_train: 0.0851554847822018, Loss_test: 0.08417759061820848\n",
      "2965 - Loss_train: 0.08515437315930974, Loss_test: 0.08417667820104587\n",
      "2966 - Loss_train: 0.08515326211314339, Loss_test: 0.08417576631053418\n",
      "2967 - Loss_train: 0.08515215164420185, Loss_test: 0.08417485494570238\n",
      "2968 - Loss_train: 0.08515104175157995, Loss_test: 0.08417394410437018\n",
      "2969 - Loss_train: 0.08514993243520576, Loss_test: 0.08417303378974597\n",
      "2970 - Loss_train: 0.08514882369568019, Loss_test: 0.08417212400047368\n",
      "2971 - Loss_train: 0.08514771553127415, Loss_test: 0.08417121473388518\n",
      "2972 - Loss_train: 0.08514660794124629, Loss_test: 0.08417030599327906\n",
      "2973 - Loss_train: 0.0851455009264396, Loss_test: 0.08416939777380737\n",
      "2974 - Loss_train: 0.08514439448618231, Loss_test: 0.08416849008041352\n",
      "2975 - Loss_train: 0.08514328862140497, Loss_test: 0.08416758291177803\n",
      "2976 - Loss_train: 0.08514218332959735, Loss_test: 0.08416667626254766\n",
      "2977 - Loss_train: 0.08514107861174794, Loss_test: 0.0841657701363074\n",
      "2978 - Loss_train: 0.08513997446866406, Loss_test: 0.08416486453511506\n",
      "2979 - Loss_train: 0.08513887089718168, Loss_test: 0.08416395945507935\n",
      "2980 - Loss_train: 0.08513776789878182, Loss_test: 0.08416305489653723\n",
      "2981 - Loss_train: 0.085136665474685, Loss_test: 0.0841621508601856\n",
      "2982 - Loss_train: 0.08513556362111913, Loss_test: 0.08416124734570757\n",
      "2983 - Loss_train: 0.08513446233945936, Loss_test: 0.08416034435061831\n",
      "2984 - Loss_train: 0.08513336162966036, Loss_test: 0.08415944187583671\n",
      "2985 - Loss_train: 0.08513226149218742, Loss_test: 0.0841585399237813\n",
      "2986 - Loss_train: 0.08513116192472321, Loss_test: 0.08415763849093383\n",
      "2987 - Loss_train: 0.08513006292829534, Loss_test: 0.08415673757695005\n",
      "2988 - Loss_train: 0.08512896450219634, Loss_test: 0.08415583718465955\n",
      "2989 - Loss_train: 0.08512786664761865, Loss_test: 0.08415493731143128\n",
      "2990 - Loss_train: 0.08512676936179565, Loss_test: 0.08415403795681027\n",
      "2991 - Loss_train: 0.08512567264563907, Loss_test: 0.0841531391201406\n",
      "2992 - Loss_train: 0.08512457649849282, Loss_test: 0.08415224080164103\n",
      "2993 - Loss_train: 0.08512348092070653, Loss_test: 0.08415134300208348\n",
      "2994 - Loss_train: 0.08512238591162181, Loss_test: 0.08415044572012384\n",
      "2995 - Loss_train: 0.08512129147068741, Loss_test: 0.08414954895694335\n",
      "2996 - Loss_train: 0.08512019759827363, Loss_test: 0.08414865271019742\n",
      "2997 - Loss_train: 0.08511910429343438, Loss_test: 0.08414775698057868\n",
      "2998 - Loss_train: 0.08511801155780478, Loss_test: 0.08414686176949022\n",
      "2999 - Loss_train: 0.08511691938786232, Loss_test: 0.08414596707421595\n",
      "3000 - Loss_train: 0.0851158277849107, Loss_test: 0.0841450728934468\n",
      "3001 - Loss_train: 0.08511473674842457, Loss_test: 0.08414417922922673\n",
      "3002 - Loss_train: 0.08511364627853604, Loss_test: 0.08414328608129461\n",
      "3003 - Loss_train: 0.0851125563760173, Loss_test: 0.0841423934479848\n",
      "3004 - Loss_train: 0.08511146703850254, Loss_test: 0.08414150133118035\n",
      "3005 - Loss_train: 0.0851103782661309, Loss_test: 0.08414060972808575\n",
      "3006 - Loss_train: 0.08510929005895791, Loss_test: 0.08413971864044897\n",
      "3007 - Loss_train: 0.08510820241640754, Loss_test: 0.08413882806566586\n",
      "3008 - Loss_train: 0.08510711533874954, Loss_test: 0.08413793800715093\n",
      "3009 - Loss_train: 0.08510602882560968, Loss_test: 0.08413704846114826\n",
      "3010 - Loss_train: 0.0851049428767574, Loss_test: 0.08413615942777897\n",
      "3011 - Loss_train: 0.0851038574911542, Loss_test: 0.084135270910019\n",
      "3012 - Loss_train: 0.08510277266897497, Loss_test: 0.08413438290251016\n",
      "3013 - Loss_train: 0.08510168840993568, Loss_test: 0.0841334954047327\n",
      "3014 - Loss_train: 0.08510060471508361, Loss_test: 0.08413260842826153\n",
      "3015 - Loss_train: 0.08509952158154521, Loss_test: 0.08413172195742721\n",
      "3016 - Loss_train: 0.0850984390103591, Loss_test: 0.08413083599977278\n",
      "3017 - Loss_train: 0.08509735700115424, Loss_test: 0.08412995055298444\n",
      "3018 - Loss_train: 0.08509627555429337, Loss_test: 0.0841290656188276\n",
      "3019 - Loss_train: 0.08509519466947897, Loss_test: 0.08412818119497804\n",
      "3020 - Loss_train: 0.08509411434473325, Loss_test: 0.08412729728234984\n",
      "3021 - Loss_train: 0.08509303458060334, Loss_test: 0.0841264138781823\n",
      "3022 - Loss_train: 0.08509195537724865, Loss_test: 0.08412553098638466\n",
      "3023 - Loss_train: 0.08509087673392325, Loss_test: 0.08412464860141476\n",
      "3024 - Loss_train: 0.08508979865057008, Loss_test: 0.0841237667285765\n",
      "3025 - Loss_train: 0.08508872112688258, Loss_test: 0.08412288536447075\n",
      "3026 - Loss_train: 0.0850876441622546, Loss_test: 0.08412200450799642\n",
      "3027 - Loss_train: 0.08508656775822301, Loss_test: 0.08412112416270072\n",
      "3028 - Loss_train: 0.08508549191199888, Loss_test: 0.08412024432412443\n",
      "3029 - Loss_train: 0.08508441662407061, Loss_test: 0.08411936499551388\n",
      "3030 - Loss_train: 0.08508334189431485, Loss_test: 0.08411848617156374\n",
      "3031 - Loss_train: 0.08508226772232158, Loss_test: 0.08411760785993248\n",
      "3032 - Loss_train: 0.08508119410924175, Loss_test: 0.08411673004967768\n",
      "3033 - Loss_train: 0.08508012105385364, Loss_test: 0.08411585275308624\n",
      "3034 - Loss_train: 0.08507904855529745, Loss_test: 0.08411497595845517\n",
      "3035 - Loss_train: 0.08507797661243677, Loss_test: 0.08411409967326272\n",
      "3036 - Loss_train: 0.0850769052262419, Loss_test: 0.08411322389141385\n",
      "3037 - Loss_train: 0.08507583439615679, Loss_test: 0.08411234861696662\n",
      "3038 - Loss_train: 0.08507476412367704, Loss_test: 0.08411147385000944\n",
      "3039 - Loss_train: 0.0850736944065408, Loss_test: 0.08411059958662631\n",
      "3040 - Loss_train: 0.08507262524350903, Loss_test: 0.08410972582874573\n",
      "3041 - Loss_train: 0.0850715566355349, Loss_test: 0.08410885257604918\n",
      "3042 - Loss_train: 0.08507048858319043, Loss_test: 0.08410797982737793\n",
      "3043 - Loss_train: 0.08506942108415237, Loss_test: 0.0841071075844843\n",
      "3044 - Loss_train: 0.08506835413945152, Loss_test: 0.08410623584112394\n",
      "3045 - Loss_train: 0.0850672877502386, Loss_test: 0.0841053646084043\n",
      "3046 - Loss_train: 0.08506622191300901, Loss_test: 0.08410449387375156\n",
      "3047 - Loss_train: 0.08506515662907828, Loss_test: 0.0841036236436064\n",
      "3048 - Loss_train: 0.0850640918984656, Loss_test: 0.08410275391715813\n",
      "3049 - Loss_train: 0.0850630277217528, Loss_test: 0.08410188469216445\n",
      "3050 - Loss_train: 0.08506196409721438, Loss_test: 0.08410101597258882\n",
      "3051 - Loss_train: 0.08506090102421625, Loss_test: 0.08410014775223605\n",
      "3052 - Loss_train: 0.08505983850372627, Loss_test: 0.08409928003407596\n",
      "3053 - Loss_train: 0.08505877653366327, Loss_test: 0.08409841281822639\n",
      "3054 - Loss_train: 0.08505771511552443, Loss_test: 0.084097546104223\n",
      "3055 - Loss_train: 0.08505665424913691, Loss_test: 0.08409667988936531\n",
      "3056 - Loss_train: 0.08505559393198202, Loss_test: 0.0840958141772132\n",
      "3057 - Loss_train: 0.08505453416714086, Loss_test: 0.08409494896359002\n",
      "3058 - Loss_train: 0.08505347495094277, Loss_test: 0.08409408425074039\n",
      "3059 - Loss_train: 0.08505241628461395, Loss_test: 0.08409322003793615\n",
      "3060 - Loss_train: 0.08505135816907086, Loss_test: 0.08409235632473573\n",
      "3061 - Loss_train: 0.08505030060182131, Loss_test: 0.08409149311075269\n",
      "3062 - Loss_train: 0.08504924358444, Loss_test: 0.08409063039638842\n",
      "3063 - Loss_train: 0.08504818711497975, Loss_test: 0.08408976818239182\n",
      "3064 - Loss_train: 0.08504713119408588, Loss_test: 0.08408890646340926\n",
      "3065 - Loss_train: 0.08504607582153573, Loss_test: 0.08408804524727516\n",
      "3066 - Loss_train: 0.08504502099782066, Loss_test: 0.08408718452488965\n",
      "3067 - Loss_train: 0.08504396672206564, Loss_test: 0.08408632430354293\n",
      "3068 - Loss_train: 0.0850429129923307, Loss_test: 0.08408546457604762\n",
      "3069 - Loss_train: 0.08504185980935035, Loss_test: 0.08408460535069999\n",
      "3070 - Loss_train: 0.08504080717383265, Loss_test: 0.08408374661799706\n",
      "3071 - Loss_train: 0.08503975508547788, Loss_test: 0.08408288838304952\n",
      "3072 - Loss_train: 0.08503870354203272, Loss_test: 0.08408203064718307\n",
      "3073 - Loss_train: 0.08503765254458114, Loss_test: 0.08408117340296903\n",
      "3074 - Loss_train: 0.08503660209250609, Loss_test: 0.0840803166580054\n",
      "3075 - Loss_train: 0.08503555218600048, Loss_test: 0.08407946040627588\n",
      "3076 - Loss_train: 0.08503450282588407, Loss_test: 0.08407860465109519\n",
      "3077 - Loss_train: 0.08503345400913002, Loss_test: 0.08407774939104162\n",
      "3078 - Loss_train: 0.08503240573696347, Loss_test: 0.08407689462594807\n",
      "3079 - Loss_train: 0.08503135800944746, Loss_test: 0.08407604035297743\n",
      "3080 - Loss_train: 0.08503031082597744, Loss_test: 0.08407518657640205\n",
      "3081 - Loss_train: 0.08502926418698346, Loss_test: 0.08407433329770704\n",
      "3082 - Loss_train: 0.08502821809029781, Loss_test: 0.08407348050591326\n",
      "3083 - Loss_train: 0.08502717253641649, Loss_test: 0.08407262821137128\n",
      "3084 - Loss_train: 0.08502612752531033, Loss_test: 0.08407177640599035\n",
      "3085 - Loss_train: 0.08502508305684227, Loss_test: 0.08407092509775006\n",
      "3086 - Loss_train: 0.08502403913087685, Loss_test: 0.08407007427996127\n",
      "3087 - Loss_train: 0.08502299574670182, Loss_test: 0.08406922395675835\n",
      "3088 - Loss_train: 0.0850219529056485, Loss_test: 0.08406837412355754\n",
      "3089 - Loss_train: 0.08502091060578462, Loss_test: 0.08406752478404238\n",
      "3090 - Loss_train: 0.08501986884589902, Loss_test: 0.08406667593753946\n",
      "3091 - Loss_train: 0.08501882762691372, Loss_test: 0.08406582757741958\n",
      "3092 - Loss_train: 0.08501778694850948, Loss_test: 0.08406497971099923\n",
      "3093 - Loss_train: 0.08501674681041287, Loss_test: 0.08406413233358266\n",
      "3094 - Loss_train: 0.08501570721226712, Loss_test: 0.08406328544889306\n",
      "3095 - Loss_train: 0.08501466815373977, Loss_test: 0.08406243905294479\n",
      "3096 - Loss_train: 0.08501362963499819, Loss_test: 0.0840615931478752\n",
      "3097 - Loss_train: 0.08501259165512877, Loss_test: 0.08406074773236974\n",
      "3098 - Loss_train: 0.08501155421452404, Loss_test: 0.0840599028050176\n",
      "3099 - Loss_train: 0.08501051731246308, Loss_test: 0.08405905836888633\n",
      "3100 - Loss_train: 0.08500948094848186, Loss_test: 0.08405821441998819\n",
      "3101 - Loss_train: 0.08500844512388218, Loss_test: 0.084057370960496\n",
      "3102 - Loss_train: 0.08500740983547062, Loss_test: 0.08405652799016404\n",
      "3103 - Loss_train: 0.08500637508578354, Loss_test: 0.08405568550955392\n",
      "3104 - Loss_train: 0.08500534087242566, Loss_test: 0.08405484351067397\n",
      "3105 - Loss_train: 0.08500430719672351, Loss_test: 0.08405400200451497\n",
      "3106 - Loss_train: 0.08500327405838051, Loss_test: 0.08405316098359446\n",
      "3107 - Loss_train: 0.08500224145512678, Loss_test: 0.0840523204509905\n",
      "3108 - Loss_train: 0.0850012093878229, Loss_test: 0.08405148040404332\n",
      "3109 - Loss_train: 0.08500017785785205, Loss_test: 0.0840506408455545\n",
      "3110 - Loss_train: 0.08499914686215497, Loss_test: 0.08404980177091254\n",
      "3111 - Loss_train: 0.08499811640285136, Loss_test: 0.0840489631840272\n",
      "3112 - Loss_train: 0.08499708647736695, Loss_test: 0.08404812508103823\n",
      "3113 - Loss_train: 0.08499605708762764, Loss_test: 0.08404728746624225\n",
      "3114 - Loss_train: 0.0849950282313365, Loss_test: 0.08404645033355636\n",
      "3115 - Loss_train: 0.08499399991022523, Loss_test: 0.08404561368773944\n",
      "3116 - Loss_train: 0.08499297212259442, Loss_test: 0.08404477752683787\n",
      "3117 - Loss_train: 0.08499194486800118, Loss_test: 0.0840439418483939\n",
      "3118 - Loss_train: 0.08499091814714366, Loss_test: 0.08404310665609219\n",
      "3119 - Loss_train: 0.08498989195993586, Loss_test: 0.08404227194548512\n",
      "3120 - Loss_train: 0.08498886630436528, Loss_test: 0.08404143771975649\n",
      "3121 - Loss_train: 0.08498784118153599, Loss_test: 0.08404060397711753\n",
      "3122 - Loss_train: 0.08498681659101483, Loss_test: 0.08403977071575228\n",
      "3123 - Loss_train: 0.08498579253209472, Loss_test: 0.08403893794015978\n",
      "3124 - Loss_train: 0.08498476900594858, Loss_test: 0.08403810564622652\n",
      "3125 - Loss_train: 0.08498374601054093, Loss_test: 0.08403727383268089\n",
      "3126 - Loss_train: 0.08498272354729781, Loss_test: 0.08403644250478867\n",
      "3127 - Loss_train: 0.08498170161467339, Loss_test: 0.08403561165783778\n",
      "3128 - Loss_train: 0.08498068021158249, Loss_test: 0.08403478129007788\n",
      "3129 - Loss_train: 0.08497965933872347, Loss_test: 0.08403395140468113\n",
      "3130 - Loss_train: 0.0849786389958724, Loss_test: 0.08403312199828171\n",
      "3131 - Loss_train: 0.08497761918314452, Loss_test: 0.08403229307444016\n",
      "3132 - Loss_train: 0.08497659989966468, Loss_test: 0.08403146463163227\n",
      "3133 - Loss_train: 0.08497558114536151, Loss_test: 0.08403063666681583\n",
      "3134 - Loss_train: 0.08497456292052177, Loss_test: 0.08402980918376708\n",
      "3135 - Loss_train: 0.08497354522360384, Loss_test: 0.08402898218192288\n",
      "3136 - Loss_train: 0.08497252805510185, Loss_test: 0.08402815565530661\n",
      "3137 - Loss_train: 0.08497151141518713, Loss_test: 0.0840273296096446\n",
      "3138 - Loss_train: 0.08497049530280006, Loss_test: 0.08402650404465474\n",
      "3139 - Loss_train: 0.08496947971893837, Loss_test: 0.08402567895537079\n",
      "3140 - Loss_train: 0.08496846466242236, Loss_test: 0.08402485434789168\n",
      "3141 - Loss_train: 0.08496745013192912, Loss_test: 0.08402403021413242\n",
      "3142 - Loss_train: 0.08496643612894852, Loss_test: 0.08402320656185107\n",
      "3143 - Loss_train: 0.08496542265187823, Loss_test: 0.08402238338601269\n",
      "3144 - Loss_train: 0.08496440970076866, Loss_test: 0.08402156068652544\n",
      "3145 - Loss_train: 0.08496339727559957, Loss_test: 0.08402073846473906\n",
      "3146 - Loss_train: 0.08496238537722628, Loss_test: 0.0840199167219201\n",
      "3147 - Loss_train: 0.08496137400335627, Loss_test: 0.08401909545198646\n",
      "3148 - Loss_train: 0.08496036315552392, Loss_test: 0.08401827466046527\n",
      "3149 - Loss_train: 0.08495935283302412, Loss_test: 0.08401745434445047\n",
      "3150 - Loss_train: 0.0849583430350296, Loss_test: 0.08401663450638085\n",
      "3151 - Loss_train: 0.08495733375978343, Loss_test: 0.08401581514179049\n",
      "3152 - Loss_train: 0.08495632500983681, Loss_test: 0.08401499625229711\n",
      "3153 - Loss_train: 0.08495531678415011, Loss_test: 0.08401417783980082\n",
      "3154 - Loss_train: 0.08495430908039804, Loss_test: 0.0840133598995129\n",
      "3155 - Loss_train: 0.08495330189950326, Loss_test: 0.08401254243304038\n",
      "3156 - Loss_train: 0.084952295243435, Loss_test: 0.08401172544170943\n",
      "3157 - Loss_train: 0.08495128910788619, Loss_test: 0.08401090892356536\n",
      "3158 - Loss_train: 0.08495028349634953, Loss_test: 0.08401009288287234\n",
      "3159 - Loss_train: 0.08494927840606778, Loss_test: 0.08400927731107578\n",
      "3160 - Loss_train: 0.08494827383844203, Loss_test: 0.08400846221679922\n",
      "3161 - Loss_train: 0.08494726979088119, Loss_test: 0.08400764758980586\n",
      "3162 - Loss_train: 0.08494626626521357, Loss_test: 0.08400683343818885\n",
      "3163 - Loss_train: 0.08494526326132884, Loss_test: 0.08400601976030599\n",
      "3164 - Loss_train: 0.0849442607775218, Loss_test: 0.08400520655432629\n",
      "3165 - Loss_train: 0.08494325881436374, Loss_test: 0.08400439381813678\n",
      "3166 - Loss_train: 0.08494225737098686, Loss_test: 0.08400358155299742\n",
      "3167 - Loss_train: 0.08494125644753604, Loss_test: 0.08400276976175264\n",
      "3168 - Loss_train: 0.0849402560437748, Loss_test: 0.08400195843858084\n",
      "3169 - Loss_train: 0.08493925616083169, Loss_test: 0.08400114758971249\n",
      "3170 - Loss_train: 0.08493825679716835, Loss_test: 0.08400033720989458\n",
      "3171 - Loss_train: 0.08493725795195543, Loss_test: 0.08399952730197156\n",
      "3172 - Loss_train: 0.08493625962571394, Loss_test: 0.08399871786261678\n",
      "3173 - Loss_train: 0.08493526181734867, Loss_test: 0.08399790889412158\n",
      "3174 - Loss_train: 0.0849342645264621, Loss_test: 0.08399710039307741\n",
      "3175 - Loss_train: 0.08493326775346619, Loss_test: 0.08399629236361472\n",
      "3176 - Loss_train: 0.08493227149880953, Loss_test: 0.08399548480030579\n",
      "3177 - Loss_train: 0.08493127576132808, Loss_test: 0.08399467771035643\n",
      "3178 - Loss_train: 0.08493028053981089, Loss_test: 0.0839938710861501\n",
      "3179 - Loss_train: 0.08492928583536563, Loss_test: 0.08399306492748873\n",
      "3180 - Loss_train: 0.08492829164726516, Loss_test: 0.08399225924017518\n",
      "3181 - Loss_train: 0.08492729797625172, Loss_test: 0.08399145402046462\n",
      "3182 - Loss_train: 0.08492630482035672, Loss_test: 0.08399064926725318\n",
      "3183 - Loss_train: 0.08492531217965235, Loss_test: 0.08398984498182424\n",
      "3184 - Loss_train: 0.08492432005459759, Loss_test: 0.08398904116177158\n",
      "3185 - Loss_train: 0.08492332844474071, Loss_test: 0.08398823781028437\n",
      "3186 - Loss_train: 0.08492233734980452, Loss_test: 0.0839874349251977\n",
      "3187 - Loss_train: 0.08492134676966907, Loss_test: 0.08398663250414182\n",
      "3188 - Loss_train: 0.08492035670420642, Loss_test: 0.08398583055249273\n",
      "3189 - Loss_train: 0.08491936715321613, Loss_test: 0.08398502906504948\n",
      "3190 - Loss_train: 0.08491837811542823, Loss_test: 0.0839842280418333\n",
      "3191 - Loss_train: 0.08491738959109554, Loss_test: 0.08398342748775295\n",
      "3192 - Loss_train: 0.0849164015811951, Loss_test: 0.08398262739553004\n",
      "3193 - Loss_train: 0.08491541408281311, Loss_test: 0.08398182776785613\n",
      "3194 - Loss_train: 0.08491442709734535, Loss_test: 0.08398102860390767\n",
      "3195 - Loss_train: 0.08491344062535312, Loss_test: 0.08398022990881786\n",
      "3196 - Loss_train: 0.0849124546642018, Loss_test: 0.08397943167124909\n",
      "3197 - Loss_train: 0.08491146921512079, Loss_test: 0.08397863390173081\n",
      "3198 - Loss_train: 0.08491048427756623, Loss_test: 0.08397783659748494\n",
      "3199 - Loss_train: 0.08490949985145836, Loss_test: 0.08397703974873441\n",
      "3200 - Loss_train: 0.08490851593675723, Loss_test: 0.0839762433685269\n",
      "3201 - Loss_train: 0.08490753253344809, Loss_test: 0.08397544744998882\n",
      "3202 - Loss_train: 0.08490654964043332, Loss_test: 0.08397465199396477\n",
      "3203 - Loss_train: 0.08490556725748813, Loss_test: 0.08397385699793383\n",
      "3204 - Loss_train: 0.08490458538497318, Loss_test: 0.08397306246765533\n",
      "3205 - Loss_train: 0.08490360402252654, Loss_test: 0.0839722683962944\n",
      "3206 - Loss_train: 0.08490262316912554, Loss_test: 0.08397147478766762\n",
      "3207 - Loss_train: 0.0849016428262438, Loss_test: 0.08397068164115701\n",
      "3208 - Loss_train: 0.0849006629913391, Loss_test: 0.0839698889524451\n",
      "3209 - Loss_train: 0.08489968366535802, Loss_test: 0.08396909672712996\n",
      "3210 - Loss_train: 0.08489870484781312, Loss_test: 0.08396830496183472\n",
      "3211 - Loss_train: 0.08489772653939306, Loss_test: 0.08396751365765746\n",
      "3212 - Loss_train: 0.08489674873780897, Loss_test: 0.0839667228097037\n",
      "3213 - Loss_train: 0.08489577144412544, Loss_test: 0.08396593242550421\n",
      "3214 - Loss_train: 0.08489479465765509, Loss_test: 0.0839651424983074\n",
      "3215 - Loss_train: 0.08489381837957173, Loss_test: 0.08396435302981806\n",
      "3216 - Loss_train: 0.08489284260743848, Loss_test: 0.08396356402290292\n",
      "3217 - Loss_train: 0.08489186734187948, Loss_test: 0.08396277547172737\n",
      "3218 - Loss_train: 0.08489089258351815, Loss_test: 0.0839619873807816\n",
      "3219 - Loss_train: 0.08488991833150754, Loss_test: 0.08396119974857534\n",
      "3220 - Loss_train: 0.08488894458442812, Loss_test: 0.08396041257316739\n",
      "3221 - Loss_train: 0.08488797134362709, Loss_test: 0.08395962585646402\n",
      "3222 - Loss_train: 0.08488699860839251, Loss_test: 0.08395883959635132\n",
      "3223 - Loss_train: 0.08488602637740797, Loss_test: 0.08395805379260203\n",
      "3224 - Loss_train: 0.08488505465117387, Loss_test: 0.08395726844668136\n",
      "3225 - Loss_train: 0.08488408342947236, Loss_test: 0.08395648355543524\n",
      "3226 - Loss_train: 0.08488311271253296, Loss_test: 0.083955699123384\n",
      "3227 - Loss_train: 0.08488214250025966, Loss_test: 0.08395491514691664\n",
      "3228 - Loss_train: 0.08488117279098617, Loss_test: 0.08395413162370473\n",
      "3229 - Loss_train: 0.08488020358508258, Loss_test: 0.08395334856039521\n",
      "3230 - Loss_train: 0.08487923488269469, Loss_test: 0.08395256594728139\n",
      "3231 - Loss_train: 0.08487826668399569, Loss_test: 0.08395178379754435\n",
      "3232 - Loss_train: 0.08487729898694736, Loss_test: 0.08395100209498096\n",
      "3233 - Loss_train: 0.08487633179238133, Loss_test: 0.08395022084946807\n",
      "3234 - Loss_train: 0.08487536509996903, Loss_test: 0.08394944005780917\n",
      "3235 - Loss_train: 0.08487439890957321, Loss_test: 0.08394865972096481\n",
      "3236 - Loss_train: 0.08487343322103844, Loss_test: 0.0839478798394156\n",
      "3237 - Loss_train: 0.08487246803496425, Loss_test: 0.08394710041020303\n",
      "3238 - Loss_train: 0.08487150334933449, Loss_test: 0.0839463214340647\n",
      "3239 - Loss_train: 0.08487053916428779, Loss_test: 0.08394554291123815\n",
      "3240 - Loss_train: 0.08486957547988615, Loss_test: 0.0839447648422693\n",
      "3241 - Loss_train: 0.08486861229560627, Loss_test: 0.08394398722648169\n",
      "3242 - Loss_train: 0.08486764961183087, Loss_test: 0.08394321005974138\n",
      "3243 - Loss_train: 0.08486668742876889, Loss_test: 0.08394243335068359\n",
      "3244 - Loss_train: 0.08486572574448287, Loss_test: 0.0839416570875464\n",
      "3245 - Loss_train: 0.08486476455952516, Loss_test: 0.08394088127985184\n",
      "3246 - Loss_train: 0.08486380387463245, Loss_test: 0.0839401059238091\n",
      "3247 - Loss_train: 0.08486284368725173, Loss_test: 0.0839393310162222\n",
      "3248 - Loss_train: 0.08486188399906061, Loss_test: 0.0839385565605593\n",
      "3249 - Loss_train: 0.08486092481013947, Loss_test: 0.08393778256018208\n",
      "3250 - Loss_train: 0.08485996611816744, Loss_test: 0.08393700900413857\n",
      "3251 - Loss_train: 0.08485900792379486, Loss_test: 0.08393623590138445\n",
      "3252 - Loss_train: 0.0848580502274466, Loss_test: 0.08393546324736215\n",
      "3253 - Loss_train: 0.084857093028439, Loss_test: 0.08393469104253186\n",
      "3254 - Loss_train: 0.08485613632714899, Loss_test: 0.08393391928968004\n",
      "3255 - Loss_train: 0.08485518012126199, Loss_test: 0.0839331479837818\n",
      "3256 - Loss_train: 0.08485422441218307, Loss_test: 0.08393237712793289\n",
      "3257 - Loss_train: 0.08485326919945682, Loss_test: 0.08393160671918495\n",
      "3258 - Loss_train: 0.08485231448259453, Loss_test: 0.08393083676137432\n",
      "3259 - Loss_train: 0.08485136026223496, Loss_test: 0.08393006725106508\n",
      "3260 - Loss_train: 0.08485040653683656, Loss_test: 0.08392929818738722\n",
      "3261 - Loss_train: 0.08484945330760274, Loss_test: 0.08392852957344553\n",
      "3262 - Loss_train: 0.08484850057335677, Loss_test: 0.08392776140600167\n",
      "3263 - Loss_train: 0.08484754833246486, Loss_test: 0.08392699368535564\n",
      "3264 - Loss_train: 0.08484659658768551, Loss_test: 0.08392622641318688\n",
      "3265 - Loss_train: 0.08484564533551349, Loss_test: 0.08392545958905163\n",
      "3266 - Loss_train: 0.08484469457767062, Loss_test: 0.0839246932051579\n",
      "3267 - Loss_train: 0.08484374431404185, Loss_test: 0.08392392727487813\n",
      "3268 - Loss_train: 0.08484279454412033, Loss_test: 0.08392316178800083\n",
      "3269 - Loss_train: 0.08484184526667585, Loss_test: 0.08392239674617204\n",
      "3270 - Loss_train: 0.08484089648259804, Loss_test: 0.08392163214902397\n",
      "3271 - Loss_train: 0.08483994819101587, Loss_test: 0.0839208680013592\n",
      "3272 - Loss_train: 0.0848390003912625, Loss_test: 0.08392010429204666\n",
      "3273 - Loss_train: 0.08483805308507945, Loss_test: 0.08391934103314241\n",
      "3274 - Loss_train: 0.08483710627054661, Loss_test: 0.08391857821801313\n",
      "3275 - Loss_train: 0.08483615994676837, Loss_test: 0.08391781584601024\n",
      "3276 - Loss_train: 0.08483521411450143, Loss_test: 0.08391705391822976\n",
      "3277 - Loss_train: 0.08483426877293682, Loss_test: 0.08391629243447224\n",
      "3278 - Loss_train: 0.0848333239232188, Loss_test: 0.08391553139077552\n",
      "3279 - Loss_train: 0.08483237956436417, Loss_test: 0.08391477079684707\n",
      "3280 - Loss_train: 0.08483143569467566, Loss_test: 0.08391401064418008\n",
      "3281 - Loss_train: 0.08483049231560531, Loss_test: 0.08391325093180746\n",
      "3282 - Loss_train: 0.08482954942631386, Loss_test: 0.08391249166357172\n",
      "3283 - Loss_train: 0.08482860702764333, Loss_test: 0.08391173283969935\n",
      "3284 - Loss_train: 0.08482766511752073, Loss_test: 0.08391097445398427\n",
      "3285 - Loss_train: 0.08482672369631221, Loss_test: 0.08391021651354982\n",
      "3286 - Loss_train: 0.08482578276361569, Loss_test: 0.08390945901314699\n",
      "3287 - Loss_train: 0.08482484231983815, Loss_test: 0.08390870195363957\n",
      "3288 - Loss_train: 0.08482390236437107, Loss_test: 0.08390794533651769\n",
      "3289 - Loss_train: 0.08482296289832787, Loss_test: 0.08390718916011128\n",
      "3290 - Loss_train: 0.08482202391890574, Loss_test: 0.08390643342700473\n",
      "3291 - Loss_train: 0.08482108542804802, Loss_test: 0.08390567813163131\n",
      "3292 - Loss_train: 0.08482014742447756, Loss_test: 0.08390492327643645\n",
      "3293 - Loss_train: 0.08481920990764209, Loss_test: 0.08390416886207061\n",
      "3294 - Loss_train: 0.08481827287800237, Loss_test: 0.0839034148876738\n",
      "3295 - Loss_train: 0.08481733633372958, Loss_test: 0.08390266135118507\n",
      "3296 - Loss_train: 0.08481640027613963, Loss_test: 0.08390190825259473\n",
      "3297 - Loss_train: 0.08481546470609201, Loss_test: 0.08390115559907474\n",
      "3298 - Loss_train: 0.0848145296202378, Loss_test: 0.08390040338001352\n",
      "3299 - Loss_train: 0.08481359502154162, Loss_test: 0.08389965160017623\n",
      "3300 - Loss_train: 0.08481266090764518, Loss_test: 0.08389890026128653\n",
      "3301 - Loss_train: 0.08481172727787771, Loss_test: 0.08389814935690246\n",
      "3302 - Loss_train: 0.08481079413348873, Loss_test: 0.08389739889067412\n",
      "3303 - Loss_train: 0.08480986147450376, Loss_test: 0.08389664886513544\n",
      "3304 - Loss_train: 0.08480892929821669, Loss_test: 0.0838958992731589\n",
      "3305 - Loss_train: 0.08480799760664849, Loss_test: 0.08389515011928202\n",
      "3306 - Loss_train: 0.08480706639840405, Loss_test: 0.08389440140367445\n",
      "3307 - Loss_train: 0.0848061356738035, Loss_test: 0.08389365312193285\n",
      "3308 - Loss_train: 0.08480520543291954, Loss_test: 0.08389290527946955\n",
      "3309 - Loss_train: 0.08480427567482103, Loss_test: 0.08389215787049288\n",
      "3310 - Loss_train: 0.08480334639937771, Loss_test: 0.08389141089932314\n",
      "3311 - Loss_train: 0.08480241760663042, Loss_test: 0.08389066436357216\n",
      "3312 - Loss_train: 0.08480148929586598, Loss_test: 0.08388991826211664\n",
      "3313 - Loss_train: 0.08480056146787095, Loss_test: 0.08388917259877589\n",
      "3314 - Loss_train: 0.08479963412248484, Loss_test: 0.08388842736711291\n",
      "3315 - Loss_train: 0.08479870725740968, Loss_test: 0.08388768257313876\n",
      "3316 - Loss_train: 0.08479778087389682, Loss_test: 0.08388693821107047\n",
      "3317 - Loss_train: 0.08479685497128185, Loss_test: 0.08388619428539507\n",
      "3318 - Loss_train: 0.08479592955077243, Loss_test: 0.0838854507917986\n",
      "3319 - Loss_train: 0.08479500460927816, Loss_test: 0.08388470773524916\n",
      "3320 - Loss_train: 0.08479408014810393, Loss_test: 0.08388396510778523\n",
      "3321 - Loss_train: 0.08479315616744816, Loss_test: 0.0838832229171375\n",
      "3322 - Loss_train: 0.0847922326674216, Loss_test: 0.08388248115737988\n",
      "3323 - Loss_train: 0.08479130964603986, Loss_test: 0.08388173983148246\n",
      "3324 - Loss_train: 0.08479038710384722, Loss_test: 0.08388099893785736\n",
      "3325 - Loss_train: 0.08478946504102676, Loss_test: 0.08388025847619175\n",
      "3326 - Loss_train: 0.08478854345697776, Loss_test: 0.08387951844653235\n",
      "3327 - Loss_train: 0.0847876223515103, Loss_test: 0.08387877885127897\n",
      "3328 - Loss_train: 0.08478670172421773, Loss_test: 0.08387803968365967\n",
      "3329 - Loss_train: 0.08478578157492006, Loss_test: 0.08387730094990627\n",
      "3330 - Loss_train: 0.08478486190409686, Loss_test: 0.08387656264704518\n",
      "3331 - Loss_train: 0.08478394271043961, Loss_test: 0.08387582477612684\n",
      "3332 - Loss_train: 0.08478302399394753, Loss_test: 0.0838750873353337\n",
      "3333 - Loss_train: 0.08478210575468043, Loss_test: 0.08387435032328924\n",
      "3334 - Loss_train: 0.08478118799216186, Loss_test: 0.08387361374456309\n",
      "3335 - Loss_train: 0.08478027070607748, Loss_test: 0.0838728775912233\n",
      "3336 - Loss_train: 0.08477935389763402, Loss_test: 0.08387214187309594\n",
      "3337 - Loss_train: 0.08477843756411432, Loss_test: 0.08387140658069413\n",
      "3338 - Loss_train: 0.08477752170747152, Loss_test: 0.08387067171886295\n",
      "3339 - Loss_train: 0.08477660632652494, Loss_test: 0.08386993728783698\n",
      "3340 - Loss_train: 0.08477569142005181, Loss_test: 0.08386920328300758\n",
      "3341 - Loss_train: 0.08477477698957428, Loss_test: 0.0838684697091849\n",
      "3342 - Loss_train: 0.08477386303298645, Loss_test: 0.08386773656179494\n",
      "3343 - Loss_train: 0.08477294955146367, Loss_test: 0.08386700384308199\n",
      "3344 - Loss_train: 0.08477203654394525, Loss_test: 0.08386627155232766\n",
      "3345 - Loss_train: 0.08477112401130789, Loss_test: 0.08386553968817193\n",
      "3346 - Loss_train: 0.08477021195334189, Loss_test: 0.0838648082543296\n",
      "3347 - Loss_train: 0.08476930036856241, Loss_test: 0.08386407724609628\n",
      "3348 - Loss_train: 0.08476838925606343, Loss_test: 0.08386334666521929\n",
      "3349 - Loss_train: 0.08476747861802213, Loss_test: 0.08386261651118322\n",
      "3350 - Loss_train: 0.08476656845192876, Loss_test: 0.08386188678364606\n",
      "3351 - Loss_train: 0.08476565875840467, Loss_test: 0.08386115747981239\n",
      "3352 - Loss_train: 0.08476474953790895, Loss_test: 0.08386042860477413\n",
      "3353 - Loss_train: 0.08476384078870283, Loss_test: 0.08385970015296587\n",
      "3354 - Loss_train: 0.08476293251145468, Loss_test: 0.08385897213059573\n",
      "3355 - Loss_train: 0.08476202470682496, Loss_test: 0.08385824453009995\n",
      "3356 - Loss_train: 0.08476111737405678, Loss_test: 0.0838575173568463\n",
      "3357 - Loss_train: 0.08476021051073755, Loss_test: 0.08385679060721368\n",
      "3358 - Loss_train: 0.08475930411875827, Loss_test: 0.08385606428138849\n",
      "3359 - Loss_train: 0.08475839819749915, Loss_test: 0.08385533838258123\n",
      "3360 - Loss_train: 0.08475749274783877, Loss_test: 0.083854612905053\n",
      "3361 - Loss_train: 0.08475658776834415, Loss_test: 0.08385388785332913\n",
      "3362 - Loss_train: 0.08475568325815624, Loss_test: 0.08385316322672813\n",
      "3363 - Loss_train: 0.08475477921733864, Loss_test: 0.08385243902029638\n",
      "3364 - Loss_train: 0.08475387564724558, Loss_test: 0.08385171523853688\n",
      "3365 - Loss_train: 0.08475297254532733, Loss_test: 0.08385099187940313\n",
      "3366 - Loss_train: 0.08475206991347094, Loss_test: 0.0838502689456273\n",
      "3367 - Loss_train: 0.08475116775001193, Loss_test: 0.08384954643065508\n",
      "3368 - Loss_train: 0.08475026605409963, Loss_test: 0.08384882434187575\n",
      "3369 - Loss_train: 0.08474936482783414, Loss_test: 0.08384810267027376\n",
      "3370 - Loss_train: 0.08474846406829391, Loss_test: 0.08384738142608435\n",
      "3371 - Loss_train: 0.08474756377738579, Loss_test: 0.08384666059866054\n",
      "3372 - Loss_train: 0.08474666395307306, Loss_test: 0.0838459401958039\n",
      "3373 - Loss_train: 0.08474576459726245, Loss_test: 0.08384522021116256\n",
      "3374 - Loss_train: 0.08474486570731127, Loss_test: 0.08384450064939142\n",
      "3375 - Loss_train: 0.08474396728524997, Loss_test: 0.08384378150917886\n",
      "3376 - Loss_train: 0.08474306932909136, Loss_test: 0.08384306278775783\n",
      "3377 - Loss_train: 0.08474217183923552, Loss_test: 0.08384234448824478\n",
      "3378 - Loss_train: 0.08474127481519433, Loss_test: 0.08384162660536298\n",
      "3379 - Loss_train: 0.08474037825714208, Loss_test: 0.08384090914469411\n",
      "3380 - Loss_train: 0.08473948216474023, Loss_test: 0.0838401921037683\n",
      "3381 - Loss_train: 0.08473858653880142, Loss_test: 0.08383947548130946\n",
      "3382 - Loss_train: 0.08473769137685642, Loss_test: 0.08383875927866902\n",
      "3383 - Loss_train: 0.08473679668081413, Loss_test: 0.08383804349503667\n",
      "3384 - Loss_train: 0.08473590244871484, Loss_test: 0.08383732812900856\n",
      "3385 - Loss_train: 0.08473500868141458, Loss_test: 0.08383661318170761\n",
      "3386 - Loss_train: 0.08473411537711574, Loss_test: 0.08383589865355984\n",
      "3387 - Loss_train: 0.08473322253688714, Loss_test: 0.08383518454122134\n",
      "3388 - Loss_train: 0.08473233016158638, Loss_test: 0.0838344708494273\n",
      "3389 - Loss_train: 0.08473143824885888, Loss_test: 0.08383375757290006\n",
      "3390 - Loss_train: 0.08473054680013804, Loss_test: 0.08383304471324446\n",
      "3391 - Loss_train: 0.08472965581424152, Loss_test: 0.08383233227472583\n",
      "3392 - Loss_train: 0.08472876529013902, Loss_test: 0.08383162024697148\n",
      "3393 - Loss_train: 0.08472787522950427, Loss_test: 0.08383090864134737\n",
      "3394 - Loss_train: 0.08472698563056011, Loss_test: 0.08383019744861893\n",
      "3395 - Loss_train: 0.08472609649457219, Loss_test: 0.08382948667411802\n",
      "3396 - Loss_train: 0.08472520782004593, Loss_test: 0.08382877631481596\n",
      "3397 - Loss_train: 0.08472431960711282, Loss_test: 0.08382806637049259\n",
      "3398 - Loss_train: 0.08472343185447623, Loss_test: 0.08382735684281396\n",
      "3399 - Loss_train: 0.08472254456389967, Loss_test: 0.08382664772964267\n",
      "3400 - Loss_train: 0.08472165773401637, Loss_test: 0.08382593903280942\n",
      "3401 - Loss_train: 0.08472077136393809, Loss_test: 0.08382523074699642\n",
      "3402 - Loss_train: 0.08471988545420114, Loss_test: 0.0838245228769722\n",
      "3403 - Loss_train: 0.08471900000427889, Loss_test: 0.0838238154243294\n",
      "3404 - Loss_train: 0.08471811501415141, Loss_test: 0.08382310838017255\n",
      "3405 - Loss_train: 0.08471723048345992, Loss_test: 0.0838224017555574\n",
      "3406 - Loss_train: 0.08471634641258873, Loss_test: 0.08382169554050044\n",
      "3407 - Loss_train: 0.08471546280100031, Loss_test: 0.08382098973961381\n",
      "3408 - Loss_train: 0.08471457964924609, Loss_test: 0.08382028435406644\n",
      "3409 - Loss_train: 0.08471369695587072, Loss_test: 0.0838195793793923\n",
      "3410 - Loss_train: 0.08471281472089717, Loss_test: 0.08381887482115731\n",
      "3411 - Loss_train: 0.08471193294334962, Loss_test: 0.08381817067119592\n",
      "3412 - Loss_train: 0.0847110516231777, Loss_test: 0.0838174669338833\n",
      "3413 - Loss_train: 0.08471017076142805, Loss_test: 0.08381676360955857\n",
      "3414 - Loss_train: 0.0847092903570692, Loss_test: 0.08381606069528595\n",
      "3415 - Loss_train: 0.08470841040993239, Loss_test: 0.08381535819559122\n",
      "3416 - Loss_train: 0.08470753091970198, Loss_test: 0.08381465610354091\n",
      "3417 - Loss_train: 0.08470665188586933, Loss_test: 0.08381395442706592\n",
      "3418 - Loss_train: 0.08470577330978243, Loss_test: 0.0838132531599905\n",
      "3419 - Loss_train: 0.08470489518903791, Loss_test: 0.08381255230198749\n",
      "3420 - Loss_train: 0.08470401752405, Loss_test: 0.08381185185414497\n",
      "3421 - Loss_train: 0.08470314031502932, Loss_test: 0.08381115181985255\n",
      "3422 - Loss_train: 0.08470226356177796, Loss_test: 0.08381045219229499\n",
      "3423 - Loss_train: 0.08470138726387501, Loss_test: 0.0838097529738485\n",
      "3424 - Loss_train: 0.08470051142063412, Loss_test: 0.08380905416788687\n",
      "3425 - Loss_train: 0.08469963603316676, Loss_test: 0.08380835576920405\n",
      "3426 - Loss_train: 0.08469876110102562, Loss_test: 0.08380765778199807\n",
      "3427 - Loss_train: 0.08469788662233357, Loss_test: 0.08380696020172694\n",
      "3428 - Loss_train: 0.08469701259751145, Loss_test: 0.08380626303135369\n",
      "3429 - Loss_train: 0.08469613902671635, Loss_test: 0.08380556626656957\n",
      "3430 - Loss_train: 0.08469526591047256, Loss_test: 0.08380486991492161\n",
      "3431 - Loss_train: 0.08469439324730696, Loss_test: 0.08380417396789547\n",
      "3432 - Loss_train: 0.08469352103693985, Loss_test: 0.08380347842980702\n",
      "3433 - Loss_train: 0.08469264927997383, Loss_test: 0.08380278329714871\n",
      "3434 - Loss_train: 0.08469177797549426, Loss_test: 0.0838020885759757\n",
      "3435 - Loss_train: 0.08469090712376053, Loss_test: 0.08380139425938933\n",
      "3436 - Loss_train: 0.08469003672499853, Loss_test: 0.08380070034763791\n",
      "3437 - Loss_train: 0.08468916677725288, Loss_test: 0.08380000684566526\n",
      "3438 - Loss_train: 0.08468829728125904, Loss_test: 0.08379931375010899\n",
      "3439 - Loss_train: 0.08468742823825068, Loss_test: 0.08379862105852652\n",
      "3440 - Loss_train: 0.0846865596452012, Loss_test: 0.08379792877357359\n",
      "3441 - Loss_train: 0.08468569150411673, Loss_test: 0.08379723689661671\n",
      "3442 - Loss_train: 0.0846848238136661, Loss_test: 0.08379654542271922\n",
      "3443 - Loss_train: 0.08468395657362071, Loss_test: 0.08379585435444419\n",
      "3444 - Loss_train: 0.08468308978380824, Loss_test: 0.08379516369298735\n",
      "3445 - Loss_train: 0.08468222344422993, Loss_test: 0.08379447343401752\n",
      "3446 - Loss_train: 0.08468135755425091, Loss_test: 0.08379378358168081\n",
      "3447 - Loss_train: 0.0846804921150539, Loss_test: 0.08379309413078294\n",
      "3448 - Loss_train: 0.08467962712441496, Loss_test: 0.083792405089107\n",
      "3449 - Loss_train: 0.08467876258316402, Loss_test: 0.08379171644654607\n",
      "3450 - Loss_train: 0.08467789849182236, Loss_test: 0.08379102821078518\n",
      "3451 - Loss_train: 0.08467703484793378, Loss_test: 0.08379034037699412\n",
      "3452 - Loss_train: 0.08467617165402438, Loss_test: 0.08378965294834594\n",
      "3453 - Loss_train: 0.08467530890848496, Loss_test: 0.0837889659212032\n",
      "3454 - Loss_train: 0.08467444660971651, Loss_test: 0.08378827929732353\n",
      "3455 - Loss_train: 0.08467358475888768, Loss_test: 0.08378759307640418\n",
      "3456 - Loss_train: 0.08467272335577018, Loss_test: 0.08378690725636595\n",
      "3457 - Loss_train: 0.08467186239993753, Loss_test: 0.08378622183838276\n",
      "3458 - Loss_train: 0.08467100189126493, Loss_test: 0.08378553682543423\n",
      "3459 - Loss_train: 0.08467014182942448, Loss_test: 0.08378485221146764\n",
      "3460 - Loss_train: 0.08466928221410529, Loss_test: 0.08378416799923469\n",
      "3461 - Loss_train: 0.08466842304579143, Loss_test: 0.08378348418829494\n",
      "3462 - Loss_train: 0.08466756432361845, Loss_test: 0.08378280078187204\n",
      "3463 - Loss_train: 0.08466670604715687, Loss_test: 0.08378211777187379\n",
      "3464 - Loss_train: 0.08466584821657788, Loss_test: 0.08378143516349876\n",
      "3465 - Loss_train: 0.08466499083243646, Loss_test: 0.08378075295795169\n",
      "3466 - Loss_train: 0.08466413389268128, Loss_test: 0.08378007115117649\n",
      "3467 - Loss_train: 0.08466327739890776, Loss_test: 0.08377938974451711\n",
      "3468 - Loss_train: 0.08466242134974125, Loss_test: 0.08377870873890488\n",
      "3469 - Loss_train: 0.08466156574471236, Loss_test: 0.08377802813126387\n",
      "3470 - Loss_train: 0.08466071058393568, Loss_test: 0.08377734792266489\n",
      "3471 - Loss_train: 0.08465985586721798, Loss_test: 0.08377666811390234\n",
      "3472 - Loss_train: 0.08465900159443461, Loss_test: 0.08377598870205982\n",
      "3473 - Loss_train: 0.08465814776576341, Loss_test: 0.08377530969390147\n",
      "3474 - Loss_train: 0.08465729438013715, Loss_test: 0.08377463108031866\n",
      "3475 - Loss_train: 0.08465644143889868, Loss_test: 0.08377395286391312\n",
      "3476 - Loss_train: 0.08465558894020254, Loss_test: 0.08377327505014942\n",
      "3477 - Loss_train: 0.08465473688443043, Loss_test: 0.08377259763212108\n",
      "3478 - Loss_train: 0.08465388527149449, Loss_test: 0.08377192061047026\n",
      "3479 - Loss_train: 0.08465303410088584, Loss_test: 0.0837712439892744\n",
      "3480 - Loss_train: 0.08465218337233485, Loss_test: 0.08377056776290244\n",
      "3481 - Loss_train: 0.08465133308535006, Loss_test: 0.0837698919346118\n",
      "3482 - Loss_train: 0.08465048323926166, Loss_test: 0.08376921650090986\n",
      "3483 - Loss_train: 0.08464963383501779, Loss_test: 0.08376854146630093\n",
      "3484 - Loss_train: 0.08464878487235483, Loss_test: 0.0837678668251877\n",
      "3485 - Loss_train: 0.08464793635082495, Loss_test: 0.08376719258185476\n",
      "3486 - Loss_train: 0.08464708826874312, Loss_test: 0.08376651873458392\n",
      "3487 - Loss_train: 0.08464624062705839, Loss_test: 0.08376584528212476\n",
      "3488 - Loss_train: 0.08464539342690201, Loss_test: 0.08376517222578805\n",
      "3489 - Loss_train: 0.08464454666565024, Loss_test: 0.08376449956333767\n",
      "3490 - Loss_train: 0.08464370034455403, Loss_test: 0.08376382729796093\n",
      "3491 - Loss_train: 0.08464285446365434, Loss_test: 0.08376315542595957\n",
      "3492 - Loss_train: 0.08464200902114115, Loss_test: 0.08376248394831405\n",
      "3493 - Loss_train: 0.08464116401768047, Loss_test: 0.08376181286717849\n",
      "3494 - Loss_train: 0.08464031945324316, Loss_test: 0.08376114217590065\n",
      "3495 - Loss_train: 0.0846394753274702, Loss_test: 0.08376047188119566\n",
      "3496 - Loss_train: 0.08463863164073908, Loss_test: 0.08375980198032011\n",
      "3497 - Loss_train: 0.08463778839140441, Loss_test: 0.0837591324739439\n",
      "3498 - Loss_train: 0.08463694557995108, Loss_test: 0.08375846335756634\n",
      "3499 - Loss_train: 0.0846361032064209, Loss_test: 0.08375779463549726\n",
      "3500 - Loss_train: 0.08463526127037717, Loss_test: 0.0837571263079989\n",
      "3501 - Loss_train: 0.08463441977198399, Loss_test: 0.0837564583725058\n",
      "3502 - Loss_train: 0.08463357871045875, Loss_test: 0.08375579082756299\n",
      "3503 - Loss_train: 0.08463273808550373, Loss_test: 0.08375512367730703\n",
      "3504 - Loss_train: 0.0846318978981489, Loss_test: 0.08375445691828352\n",
      "3505 - Loss_train: 0.08463105814660114, Loss_test: 0.08375379054949651\n",
      "3506 - Loss_train: 0.08463021883176876, Loss_test: 0.08375312457417197\n",
      "3507 - Loss_train: 0.08462937995294176, Loss_test: 0.08375245899183976\n",
      "3508 - Loss_train: 0.08462854150888448, Loss_test: 0.08375179379746153\n",
      "3509 - Loss_train: 0.08462770350033079, Loss_test: 0.08375112899265602\n",
      "3510 - Loss_train: 0.08462686592723062, Loss_test: 0.0837504645819116\n",
      "3511 - Loss_train: 0.084626028789409, Loss_test: 0.08374980055897435\n",
      "3512 - Loss_train: 0.08462519208629135, Loss_test: 0.08374913693016388\n",
      "3513 - Loss_train: 0.08462435581749174, Loss_test: 0.08374847368754376\n",
      "3514 - Loss_train: 0.08462351998303566, Loss_test: 0.08374781083658513\n",
      "3515 - Loss_train: 0.08462268458358356, Loss_test: 0.08374714837564545\n",
      "3516 - Loss_train: 0.08462184961723793, Loss_test: 0.08374648630298649\n",
      "3517 - Loss_train: 0.08462101508423998, Loss_test: 0.08374582461988306\n",
      "3518 - Loss_train: 0.08462018098483783, Loss_test: 0.08374516332442611\n",
      "3519 - Loss_train: 0.08461934732008952, Loss_test: 0.08374450241932338\n",
      "3520 - Loss_train: 0.08461851408714546, Loss_test: 0.08374384190280414\n",
      "3521 - Loss_train: 0.08461768128713806, Loss_test: 0.08374318177619168\n",
      "3522 - Loss_train: 0.08461684891926273, Loss_test: 0.08374252203452884\n",
      "3523 - Loss_train: 0.08461601698389705, Loss_test: 0.08374186268006743\n",
      "3524 - Loss_train: 0.08461518548077768, Loss_test: 0.08374120371722259\n",
      "3525 - Loss_train: 0.08461435440956643, Loss_test: 0.08374054514023599\n",
      "3526 - Loss_train: 0.08461352376956222, Loss_test: 0.08373988695038805\n",
      "3527 - Loss_train: 0.08461269356124404, Loss_test: 0.08373922914588172\n",
      "3528 - Loss_train: 0.08461186378400168, Loss_test: 0.08373857172976781\n",
      "3529 - Loss_train: 0.08461103443790953, Loss_test: 0.08373791469952825\n",
      "3530 - Loss_train: 0.08461020552271412, Loss_test: 0.08373725805761681\n",
      "3531 - Loss_train: 0.084609377038016, Loss_test: 0.08373660180087475\n",
      "3532 - Loss_train: 0.08460854898361438, Loss_test: 0.08373594592787906\n",
      "3533 - Loss_train: 0.08460772136000073, Loss_test: 0.08373529044549609\n",
      "3534 - Loss_train: 0.08460689416613307, Loss_test: 0.0837346353445155\n",
      "3535 - Loss_train: 0.08460606740136814, Loss_test: 0.08373398063184129\n",
      "3536 - Loss_train: 0.08460524106581842, Loss_test: 0.08373332630147987\n",
      "3537 - Loss_train: 0.08460441515987337, Loss_test: 0.08373267235816095\n",
      "3538 - Loss_train: 0.08460358968269147, Loss_test: 0.0837320187985408\n",
      "3539 - Loss_train: 0.08460276463434702, Loss_test: 0.0837313656230907\n",
      "3540 - Loss_train: 0.08460194001465722, Loss_test: 0.08373071283358924\n",
      "3541 - Loss_train: 0.0846011158232818, Loss_test: 0.08373006042535805\n",
      "3542 - Loss_train: 0.0846002920598206, Loss_test: 0.08372940840568782\n",
      "3543 - Loss_train: 0.08459946872405383, Loss_test: 0.0837287567652497\n",
      "3544 - Loss_train: 0.08459864581626178, Loss_test: 0.08372810551042859\n",
      "3545 - Loss_train: 0.08459782333553759, Loss_test: 0.08372745463886479\n",
      "3546 - Loss_train: 0.08459700128252143, Loss_test: 0.08372680414851481\n",
      "3547 - Loss_train: 0.08459617965722362, Loss_test: 0.08372615404408952\n",
      "3548 - Loss_train: 0.0845953584585924, Loss_test: 0.08372550432080775\n",
      "3549 - Loss_train: 0.0845945376854655, Loss_test: 0.08372485498074785\n",
      "3550 - Loss_train: 0.0845937173396731, Loss_test: 0.08372420602062065\n",
      "3551 - Loss_train: 0.08459289741928216, Loss_test: 0.08372355744455688\n",
      "3552 - Loss_train: 0.08459207792466839, Loss_test: 0.0837229092489894\n",
      "3553 - Loss_train: 0.08459125885559822, Loss_test: 0.0837222614356519\n",
      "3554 - Loss_train: 0.08459044021211262, Loss_test: 0.08372161400338118\n",
      "3555 - Loss_train: 0.08458962199486553, Loss_test: 0.08372096695200944\n",
      "3556 - Loss_train: 0.08458880420132361, Loss_test: 0.08372032028420985\n",
      "3557 - Loss_train: 0.08458798683390806, Loss_test: 0.08371967399239492\n",
      "3558 - Loss_train: 0.08458716989009597, Loss_test: 0.08371902808325968\n",
      "3559 - Loss_train: 0.08458635337055675, Loss_test: 0.08371838255695174\n",
      "3560 - Loss_train: 0.08458553727574097, Loss_test: 0.08371773740829153\n",
      "3561 - Loss_train: 0.08458472160459694, Loss_test: 0.0837170926396734\n",
      "3562 - Loss_train: 0.08458390635629055, Loss_test: 0.08371644825225127\n",
      "3563 - Loss_train: 0.08458309153183198, Loss_test: 0.0837158042415776\n",
      "3564 - Loss_train: 0.08458227713138586, Loss_test: 0.08371516061273324\n",
      "3565 - Loss_train: 0.08458146315407224, Loss_test: 0.08371451736352992\n",
      "3566 - Loss_train: 0.08458064959851101, Loss_test: 0.08371387449078592\n",
      "3567 - Loss_train: 0.08457983646671498, Loss_test: 0.08371323199694104\n",
      "3568 - Loss_train: 0.0845790237563383, Loss_test: 0.08371258988362565\n",
      "3569 - Loss_train: 0.08457821146816116, Loss_test: 0.08371194814427838\n",
      "3570 - Loss_train: 0.08457739960225417, Loss_test: 0.08371130678805926\n",
      "3571 - Loss_train: 0.0845765881584266, Loss_test: 0.0837106658064512\n",
      "3572 - Loss_train: 0.08457577713594523, Loss_test: 0.0837100252046624\n",
      "3573 - Loss_train: 0.08457496653409013, Loss_test: 0.0837093849776334\n",
      "3574 - Loss_train: 0.0845741563529369, Loss_test: 0.08370874512901885\n",
      "3575 - Loss_train: 0.08457334659319335, Loss_test: 0.08370810565770999\n",
      "3576 - Loss_train: 0.08457253725375219, Loss_test: 0.08370746656376489\n",
      "3577 - Loss_train: 0.08457172833471018, Loss_test: 0.08370682784441418\n",
      "3578 - Loss_train: 0.08457091983591625, Loss_test: 0.08370618950133177\n",
      "3579 - Loss_train: 0.08457011175722803, Loss_test: 0.0837055515379705\n",
      "3580 - Loss_train: 0.08456930409805469, Loss_test: 0.08370491394546112\n",
      "3581 - Loss_train: 0.08456849685930537, Loss_test: 0.08370427673347344\n",
      "3582 - Loss_train: 0.08456769003898419, Loss_test: 0.08370363989504237\n",
      "3583 - Loss_train: 0.08456688363775842, Loss_test: 0.08370300343186486\n",
      "3584 - Loss_train: 0.08456607765617137, Loss_test: 0.08370236734422426\n",
      "3585 - Loss_train: 0.08456527209235226, Loss_test: 0.08370173163316795\n",
      "3586 - Loss_train: 0.08456446694708725, Loss_test: 0.08370109629275578\n",
      "3587 - Loss_train: 0.08456366222108799, Loss_test: 0.08370046133038299\n",
      "3588 - Loss_train: 0.08456285791311006, Loss_test: 0.08369982674151806\n",
      "3589 - Loss_train: 0.08456205402204856, Loss_test: 0.08369919252523338\n",
      "3590 - Loss_train: 0.08456125054854699, Loss_test: 0.08369855868308511\n",
      "3591 - Loss_train: 0.08456044749203272, Loss_test: 0.08369792521714486\n",
      "3592 - Loss_train: 0.08455964485390172, Loss_test: 0.08369729212260896\n",
      "3593 - Loss_train: 0.08455884263162906, Loss_test: 0.08369665940147172\n",
      "3594 - Loss_train: 0.08455804082585591, Loss_test: 0.0836960270532702\n",
      "3595 - Loss_train: 0.08455723943694778, Loss_test: 0.08369539507798722\n",
      "3596 - Loss_train: 0.08455643846434734, Loss_test: 0.08369476347598315\n",
      "3597 - Loss_train: 0.08455563790770121, Loss_test: 0.0836941322448052\n",
      "3598 - Loss_train: 0.0845548377676105, Loss_test: 0.08369350138900265\n",
      "3599 - Loss_train: 0.08455403804213943, Loss_test: 0.08369287090281832\n",
      "3600 - Loss_train: 0.084553238732069, Loss_test: 0.0836922407883809\n",
      "3601 - Loss_train: 0.08455243983759449, Loss_test: 0.08369161104550564\n",
      "3602 - Loss_train: 0.08455164135878294, Loss_test: 0.08369098167750455\n",
      "3603 - Loss_train: 0.0845508432946458, Loss_test: 0.08369035267758863\n",
      "3604 - Loss_train: 0.08455004564418989, Loss_test: 0.08368972404956716\n",
      "3605 - Loss_train: 0.08454924840821716, Loss_test: 0.08368909579028798\n",
      "3606 - Loss_train: 0.08454845158618995, Loss_test: 0.08368846790391424\n",
      "3607 - Loss_train: 0.08454765517856178, Loss_test: 0.08368784038689737\n",
      "3608 - Loss_train: 0.08454685918529246, Loss_test: 0.08368721324184632\n",
      "3609 - Loss_train: 0.08454606360442327, Loss_test: 0.08368658646509926\n",
      "3610 - Loss_train: 0.08454526843694109, Loss_test: 0.08368596005917017\n",
      "3611 - Loss_train: 0.08454447368341013, Loss_test: 0.0836853340230219\n",
      "3612 - Loss_train: 0.08454367934207224, Loss_test: 0.08368470835416443\n",
      "3613 - Loss_train: 0.08454288541335647, Loss_test: 0.08368408305764392\n",
      "3614 - Loss_train: 0.08454209189672555, Loss_test: 0.08368345812832541\n",
      "3615 - Loss_train: 0.0845412987923977, Loss_test: 0.08368283356752268\n",
      "3616 - Loss_train: 0.08454050609967478, Loss_test: 0.08368220937637537\n",
      "3617 - Loss_train: 0.08453971381968524, Loss_test: 0.08368158555276489\n",
      "3618 - Loss_train: 0.0845389219509226, Loss_test: 0.0836809620972109\n",
      "3619 - Loss_train: 0.08453813049305708, Loss_test: 0.08368033900977942\n",
      "3620 - Loss_train: 0.0845373394473113, Loss_test: 0.08367971628990085\n",
      "3621 - Loss_train: 0.08453654881236948, Loss_test: 0.08367909393976637\n",
      "3622 - Loss_train: 0.08453575858725258, Loss_test: 0.08367847195431526\n",
      "3623 - Loss_train: 0.08453496877261357, Loss_test: 0.08367785033765372\n",
      "3624 - Loss_train: 0.08453417936906325, Loss_test: 0.08367722908758327\n",
      "3625 - Loss_train: 0.08453339037545268, Loss_test: 0.08367660820393057\n",
      "3626 - Loss_train: 0.08453260179234098, Loss_test: 0.08367598768694107\n",
      "3627 - Loss_train: 0.08453181361735504, Loss_test: 0.08367536753835422\n",
      "3628 - Loss_train: 0.08453102585205642, Loss_test: 0.08367474775383385\n",
      "3629 - Loss_train: 0.08453023849571911, Loss_test: 0.08367412833357737\n",
      "3630 - Loss_train: 0.08452945154856342, Loss_test: 0.08367350928090483\n",
      "3631 - Loss_train: 0.08452866501018898, Loss_test: 0.08367289059539854\n",
      "3632 - Loss_train: 0.08452787888125686, Loss_test: 0.08367227227229443\n",
      "3633 - Loss_train: 0.08452709315949758, Loss_test: 0.08367165431479012\n",
      "3634 - Loss_train: 0.08452630784650085, Loss_test: 0.08367103672406016\n",
      "3635 - Loss_train: 0.0845255229410351, Loss_test: 0.08367041949753096\n",
      "3636 - Loss_train: 0.08452473844336017, Loss_test: 0.0836698026339369\n",
      "3637 - Loss_train: 0.08452395435397586, Loss_test: 0.08366918613664466\n",
      "3638 - Loss_train: 0.0845231706720065, Loss_test: 0.08366857000293487\n",
      "3639 - Loss_train: 0.08452238739592985, Loss_test: 0.08366795423392798\n",
      "3640 - Loss_train: 0.08452160452666847, Loss_test: 0.08366733882639597\n",
      "3641 - Loss_train: 0.08452082206530745, Loss_test: 0.08366672378520114\n",
      "3642 - Loss_train: 0.08452004000954672, Loss_test: 0.08366610910423336\n",
      "3643 - Loss_train: 0.08451925835997502, Loss_test: 0.08366549478892839\n",
      "3644 - Loss_train: 0.08451847711642652, Loss_test: 0.08366488083499973\n",
      "3645 - Loss_train: 0.08451769627960352, Loss_test: 0.0836642672453648\n",
      "3646 - Loss_train: 0.08451691584847537, Loss_test: 0.08366365401823593\n",
      "3647 - Loss_train: 0.08451613582183529, Loss_test: 0.08366304115269907\n",
      "3648 - Loss_train: 0.08451535620032373, Loss_test: 0.08366242864979265\n",
      "3649 - Loss_train: 0.08451457698395567, Loss_test: 0.08366181650810851\n",
      "3650 - Loss_train: 0.08451379817302984, Loss_test: 0.0836612047294007\n",
      "3651 - Loss_train: 0.08451301976622519, Loss_test: 0.08366059330950283\n",
      "3652 - Loss_train: 0.08451224176376168, Loss_test: 0.08365998225442532\n",
      "3653 - Loss_train: 0.08451146416527071, Loss_test: 0.08365937155883946\n",
      "3654 - Loss_train: 0.08451068697159558, Loss_test: 0.08365876122579309\n",
      "3655 - Loss_train: 0.0845099101805645, Loss_test: 0.08365815125117908\n",
      "3656 - Loss_train: 0.08450913379307576, Loss_test: 0.08365754163918186\n",
      "3657 - Loss_train: 0.08450835780896623, Loss_test: 0.08365693238423122\n",
      "3658 - Loss_train: 0.08450758222883607, Loss_test: 0.08365632349256792\n",
      "3659 - Loss_train: 0.0845068070513752, Loss_test: 0.08365571496126804\n",
      "3660 - Loss_train: 0.08450603227715224, Loss_test: 0.08365510678889702\n",
      "3661 - Loss_train: 0.0845052579040925, Loss_test: 0.0836544989769563\n",
      "3662 - Loss_train: 0.08450448393407511, Loss_test: 0.08365389152186127\n",
      "3663 - Loss_train: 0.08450371036553606, Loss_test: 0.08365328442722908\n",
      "3664 - Loss_train: 0.08450293719843661, Loss_test: 0.08365267769247786\n",
      "3665 - Loss_train: 0.08450216443348092, Loss_test: 0.08365207131513756\n",
      "3666 - Loss_train: 0.08450139206986777, Loss_test: 0.0836514652964389\n",
      "3667 - Loss_train: 0.0845006201071162, Loss_test: 0.08365085963618325\n",
      "3668 - Loss_train: 0.08449984854560541, Loss_test: 0.08365025433272827\n",
      "3669 - Loss_train: 0.08449907738498809, Loss_test: 0.08364964939058314\n",
      "3670 - Loss_train: 0.08449830662584297, Loss_test: 0.08364904480599247\n",
      "3671 - Loss_train: 0.08449753626705003, Loss_test: 0.08364844057798838\n",
      "3672 - Loss_train: 0.08449676630800489, Loss_test: 0.08364783670672868\n",
      "3673 - Loss_train: 0.08449599674936274, Loss_test: 0.08364723319469285\n",
      "3674 - Loss_train: 0.08449522758990184, Loss_test: 0.08364663003606147\n",
      "3675 - Loss_train: 0.08449445882968216, Loss_test: 0.08364602723793985\n",
      "3676 - Loss_train: 0.08449369046894112, Loss_test: 0.08364542479288833\n",
      "3677 - Loss_train: 0.08449292250728854, Loss_test: 0.08364482270788662\n",
      "3678 - Loss_train: 0.08449215494406558, Loss_test: 0.0836442209766994\n",
      "3679 - Loss_train: 0.08449138778014387, Loss_test: 0.0836436196016305\n",
      "3680 - Loss_train: 0.08449062101513943, Loss_test: 0.08364301858489862\n",
      "3681 - Loss_train: 0.08448985464762625, Loss_test: 0.08364241792022392\n",
      "3682 - Loss_train: 0.08448908867844686, Loss_test: 0.08364181761331331\n",
      "3683 - Loss_train: 0.08448832310832217, Loss_test: 0.0836412176598434\n",
      "3684 - Loss_train: 0.08448755793558138, Loss_test: 0.08364061806529877\n",
      "3685 - Loss_train: 0.08448679316044866, Loss_test: 0.08364001882076003\n",
      "3686 - Loss_train: 0.08448602878185961, Loss_test: 0.0836394199350796\n",
      "3687 - Loss_train: 0.08448526480096911, Loss_test: 0.08363882140119346\n",
      "3688 - Loss_train: 0.08448450121762993, Loss_test: 0.0836382232228416\n",
      "3689 - Loss_train: 0.08448373802977745, Loss_test: 0.08363762539782885\n",
      "3690 - Loss_train: 0.0844829752390076, Loss_test: 0.08363702792939416\n",
      "3691 - Loss_train: 0.08448221284370998, Loss_test: 0.0836364308106045\n",
      "3692 - Loss_train: 0.08448145084527896, Loss_test: 0.0836358340490736\n",
      "3693 - Loss_train: 0.08448068924211362, Loss_test: 0.08363523763730936\n",
      "3694 - Loss_train: 0.08447992803430103, Loss_test: 0.08363464157927762\n",
      "3695 - Loss_train: 0.08447916722200682, Loss_test: 0.08363404587583675\n",
      "3696 - Loss_train: 0.0844784068051275, Loss_test: 0.08363345052293618\n",
      "3697 - Loss_train: 0.0844776467831654, Loss_test: 0.08363285552436205\n",
      "3698 - Loss_train: 0.08447688715615323, Loss_test: 0.0836322608775406\n",
      "3699 - Loss_train: 0.0844761279235559, Loss_test: 0.08363166658309495\n",
      "3700 - Loss_train: 0.08447536908610355, Loss_test: 0.0836310726421246\n",
      "3701 - Loss_train: 0.08447461064207074, Loss_test: 0.083630479049082\n",
      "3702 - Loss_train: 0.08447385259202593, Loss_test: 0.08362988581027674\n",
      "3703 - Loss_train: 0.08447309493573883, Loss_test: 0.08362929292152424\n",
      "3704 - Loss_train: 0.08447233767305408, Loss_test: 0.08362870038419724\n",
      "3705 - Loss_train: 0.08447158080386227, Loss_test: 0.08362810819909157\n",
      "3706 - Loss_train: 0.08447082432786943, Loss_test: 0.08362751636309536\n",
      "3707 - Loss_train: 0.08447006824499394, Loss_test: 0.08362692487875856\n",
      "3708 - Loss_train: 0.08446931255457356, Loss_test: 0.08362633374600258\n",
      "3709 - Loss_train: 0.08446855725703549, Loss_test: 0.08362574295905414\n",
      "3710 - Loss_train: 0.08446780235235414, Loss_test: 0.08362515252741175\n",
      "3711 - Loss_train: 0.08446704783921906, Loss_test: 0.08362456244269374\n",
      "3712 - Loss_train: 0.08446629371879986, Loss_test: 0.0836239727086631\n",
      "3713 - Loss_train: 0.08446553999021673, Loss_test: 0.08362338332398515\n",
      "3714 - Loss_train: 0.08446478665230202, Loss_test: 0.08362279428916025\n",
      "3715 - Loss_train: 0.08446403370578091, Loss_test: 0.08362220560092237\n",
      "3716 - Loss_train: 0.08446328115074624, Loss_test: 0.08362161726413918\n",
      "3717 - Loss_train: 0.0844625289862852, Loss_test: 0.08362102927490757\n",
      "3718 - Loss_train: 0.08446177721337851, Loss_test: 0.08362044163324453\n",
      "3719 - Loss_train: 0.08446102583033371, Loss_test: 0.08361985434226234\n",
      "3720 - Loss_train: 0.08446027483718896, Loss_test: 0.08361926739825415\n",
      "3721 - Loss_train: 0.08445952423452696, Loss_test: 0.08361868079776841\n",
      "3722 - Loss_train: 0.08445877402181474, Loss_test: 0.08361809455327271\n",
      "3723 - Loss_train: 0.0844580241986506, Loss_test: 0.08361750864970019\n",
      "3724 - Loss_train: 0.08445727476606461, Loss_test: 0.08361692309666358\n",
      "3725 - Loss_train: 0.08445652572209132, Loss_test: 0.08361633788926784\n",
      "3726 - Loss_train: 0.08445577706744864, Loss_test: 0.08361575302880915\n",
      "3727 - Loss_train: 0.0844550288015379, Loss_test: 0.08361516851647206\n",
      "3728 - Loss_train: 0.08445428092444493, Loss_test: 0.0836145843498695\n",
      "3729 - Loss_train: 0.0844535334355858, Loss_test: 0.0836140005297053\n",
      "3730 - Loss_train: 0.08445278633507212, Loss_test: 0.08361341705559064\n",
      "3731 - Loss_train: 0.08445203962352739, Loss_test: 0.08361283392752707\n",
      "3732 - Loss_train: 0.08445129330014342, Loss_test: 0.08361225114640522\n",
      "3733 - Loss_train: 0.08445054736344662, Loss_test: 0.08361166871015498\n",
      "3734 - Loss_train: 0.0844498018143651, Loss_test: 0.0836110866190507\n",
      "3735 - Loss_train: 0.08444905665294766, Loss_test: 0.08361050487268686\n",
      "3736 - Loss_train: 0.0844483118782903, Loss_test: 0.08360992347221732\n",
      "3737 - Loss_train: 0.08444756749027808, Loss_test: 0.08360934241558184\n",
      "3738 - Loss_train: 0.08444682348937206, Loss_test: 0.08360876170245181\n",
      "3739 - Loss_train: 0.08444607987550176, Loss_test: 0.08360818133784134\n",
      "3740 - Loss_train: 0.08444533664812927, Loss_test: 0.08360760131505676\n",
      "3741 - Loss_train: 0.08444459380665825, Loss_test: 0.08360702163759082\n",
      "3742 - Loss_train: 0.08444385135140019, Loss_test: 0.08360644230287662\n",
      "3743 - Loss_train: 0.08444310928062325, Loss_test: 0.08360586331225048\n",
      "3744 - Loss_train: 0.08444236759653002, Loss_test: 0.08360528466497942\n",
      "3745 - Loss_train: 0.08444162629797858, Loss_test: 0.08360470635966952\n",
      "3746 - Loss_train: 0.0844408853832584, Loss_test: 0.08360412840174415\n",
      "3747 - Loss_train: 0.08444014485348524, Loss_test: 0.08360355078303074\n",
      "3748 - Loss_train: 0.0844394047092903, Loss_test: 0.08360297350773294\n",
      "3749 - Loss_train: 0.08443866494889399, Loss_test: 0.08360239657478913\n",
      "3750 - Loss_train: 0.08443792557278021, Loss_test: 0.08360181998535753\n",
      "3751 - Loss_train: 0.0844371865806016, Loss_test: 0.08360124373406624\n",
      "3752 - Loss_train: 0.08443644797235444, Loss_test: 0.0836006678315865\n",
      "3753 - Loss_train: 0.08443570974780998, Loss_test: 0.08360009226462227\n",
      "3754 - Loss_train: 0.08443497190823451, Loss_test: 0.08359951704513989\n",
      "3755 - Loss_train: 0.08443423445110587, Loss_test: 0.08359894216017626\n",
      "3756 - Loss_train: 0.08443349737744507, Loss_test: 0.08359836762331788\n",
      "3757 - Loss_train: 0.0844327606861013, Loss_test: 0.08359779342412449\n",
      "3758 - Loss_train: 0.08443202437821554, Loss_test: 0.08359721956652569\n",
      "3759 - Loss_train: 0.0844312884518057, Loss_test: 0.0835966460486962\n",
      "3760 - Loss_train: 0.08443055290889316, Loss_test: 0.08359607287324258\n",
      "3761 - Loss_train: 0.08442981774716526, Loss_test: 0.0835955000369627\n",
      "3762 - Loss_train: 0.0844290829681385, Loss_test: 0.08359492753984629\n",
      "3763 - Loss_train: 0.08442834857117848, Loss_test: 0.08359435538554078\n",
      "3764 - Loss_train: 0.08442761455461686, Loss_test: 0.08359378356724215\n",
      "3765 - Loss_train: 0.08442688091938276, Loss_test: 0.08359321208945132\n",
      "3766 - Loss_train: 0.08442614766601782, Loss_test: 0.08359264095389025\n",
      "3767 - Loss_train: 0.08442541479363959, Loss_test: 0.0835920701548554\n",
      "3768 - Loss_train: 0.08442468230115531, Loss_test: 0.08359149969508985\n",
      "3769 - Loss_train: 0.08442395018944326, Loss_test: 0.0835909295738559\n",
      "3770 - Loss_train: 0.0844232184579342, Loss_test: 0.08359035979396706\n",
      "3771 - Loss_train: 0.084422487107282, Loss_test: 0.08358979034953389\n",
      "3772 - Loss_train: 0.08442175613674835, Loss_test: 0.08358922124386858\n",
      "3773 - Loss_train: 0.08442102554536902, Loss_test: 0.08358865247475683\n",
      "3774 - Loss_train: 0.0844202953332678, Loss_test: 0.08358808404747224\n",
      "3775 - Loss_train: 0.08441956550051079, Loss_test: 0.08358751595480368\n",
      "3776 - Loss_train: 0.08441883604782827, Loss_test: 0.08358694820189376\n",
      "3777 - Loss_train: 0.08441810697325397, Loss_test: 0.08358638078531781\n",
      "3778 - Loss_train: 0.0844173782777219, Loss_test: 0.08358581370463569\n",
      "3779 - Loss_train: 0.084416649961322, Loss_test: 0.08358524696363163\n",
      "3780 - Loss_train: 0.08441592202243807, Loss_test: 0.08358468055750853\n",
      "3781 - Loss_train: 0.08441519446204668, Loss_test: 0.08358411448660802\n",
      "3782 - Loss_train: 0.0844144672793224, Loss_test: 0.08358354875382255\n",
      "3783 - Loss_train: 0.08441374047444819, Loss_test: 0.08358298335795374\n",
      "3784 - Loss_train: 0.0844130140479702, Loss_test: 0.08358241829726114\n",
      "3785 - Loss_train: 0.08441228799802517, Loss_test: 0.08358185357225802\n",
      "3786 - Loss_train: 0.08441156232522153, Loss_test: 0.08358128918194667\n",
      "3787 - Loss_train: 0.08441083702934309, Loss_test: 0.08358072512662715\n",
      "3788 - Loss_train: 0.0844101121112363, Loss_test: 0.08358016141092155\n",
      "3789 - Loss_train: 0.08440938756986903, Loss_test: 0.0835795980270761\n",
      "3790 - Loss_train: 0.08440866340411417, Loss_test: 0.083579034979926\n",
      "3791 - Loss_train: 0.08440793961493419, Loss_test: 0.08357847226497782\n",
      "3792 - Loss_train: 0.0844072162014433, Loss_test: 0.08357790988529142\n",
      "3793 - Loss_train: 0.08440649316357324, Loss_test: 0.08357734784048658\n",
      "3794 - Loss_train: 0.0844057705021204, Loss_test: 0.08357678612926972\n",
      "3795 - Loss_train: 0.08440504821626653, Loss_test: 0.08357622475408918\n",
      "3796 - Loss_train: 0.08440432630491448, Loss_test: 0.08357566371049845\n",
      "3797 - Loss_train: 0.08440360476879863, Loss_test: 0.08357510300112841\n",
      "3798 - Loss_train: 0.08440288360724935, Loss_test: 0.08357454262798876\n",
      "3799 - Loss_train: 0.08440216282068938, Loss_test: 0.08357398258128367\n",
      "3800 - Loss_train: 0.08440144240841518, Loss_test: 0.08357342287443019\n",
      "3801 - Loss_train: 0.08440072237138531, Loss_test: 0.08357286349787471\n",
      "3802 - Loss_train: 0.08440000270821675, Loss_test: 0.08357230445404285\n",
      "3803 - Loss_train: 0.08439928341902025, Loss_test: 0.08357174574297757\n",
      "3804 - Loss_train: 0.08439856450272576, Loss_test: 0.08357118736428928\n",
      "3805 - Loss_train: 0.08439784595998397, Loss_test: 0.08357062931915869\n",
      "3806 - Loss_train: 0.08439712779044577, Loss_test: 0.08357007160212851\n",
      "3807 - Loss_train: 0.08439640999403118, Loss_test: 0.08356951422006598\n",
      "3808 - Loss_train: 0.08439569257061223, Loss_test: 0.08356895716557049\n",
      "3809 - Loss_train: 0.08439497551966697, Loss_test: 0.08356840044826053\n",
      "3810 - Loss_train: 0.08439425884230344, Loss_test: 0.08356784405809853\n",
      "3811 - Loss_train: 0.08439354253730384, Loss_test: 0.08356728800177646\n",
      "3812 - Loss_train: 0.08439282660346743, Loss_test: 0.08356673227556895\n",
      "3813 - Loss_train: 0.08439211104143053, Loss_test: 0.08356617687805222\n",
      "3814 - Loss_train: 0.08439139585159057, Loss_test: 0.08356562181134859\n",
      "3815 - Loss_train: 0.08439068103313012, Loss_test: 0.08356506707676502\n",
      "3816 - Loss_train: 0.08438996658606414, Loss_test: 0.08356451267222792\n",
      "3817 - Loss_train: 0.0843892525109476, Loss_test: 0.08356395859573143\n",
      "3818 - Loss_train: 0.08438853880610192, Loss_test: 0.08356340485169837\n",
      "3819 - Loss_train: 0.08438782547199287, Loss_test: 0.08356285143419989\n",
      "3820 - Loss_train: 0.0843871125086415, Loss_test: 0.08356229834874165\n",
      "3821 - Loss_train: 0.08438639991559667, Loss_test: 0.0835617455906155\n",
      "3822 - Loss_train: 0.08438568769288325, Loss_test: 0.08356119316155036\n",
      "3823 - Loss_train: 0.08438497584003432, Loss_test: 0.08356064106315361\n",
      "3824 - Loss_train: 0.08438426435705669, Loss_test: 0.08356008929198244\n",
      "3825 - Loss_train: 0.08438355324456265, Loss_test: 0.08355953785256437\n",
      "3826 - Loss_train: 0.0843828425007091, Loss_test: 0.08355898673691059\n",
      "3827 - Loss_train: 0.08438213212690109, Loss_test: 0.08355843595184713\n",
      "3828 - Loss_train: 0.08438142212170896, Loss_test: 0.08355788549478786\n",
      "3829 - Loss_train: 0.08438071248543401, Loss_test: 0.08355733536458469\n",
      "3830 - Loss_train: 0.08438000321793693, Loss_test: 0.08355678556189455\n",
      "3831 - Loss_train: 0.08437929431977752, Loss_test: 0.08355623608947724\n",
      "3832 - Loss_train: 0.0843785857898581, Loss_test: 0.08355568694219209\n",
      "3833 - Loss_train: 0.08437787762701855, Loss_test: 0.08355513812101203\n",
      "3834 - Loss_train: 0.08437716983284886, Loss_test: 0.08355458962797456\n",
      "3835 - Loss_train: 0.0843764624055019, Loss_test: 0.08355404146120089\n",
      "3836 - Loss_train: 0.08437575534638424, Loss_test: 0.08355349362033508\n",
      "3837 - Loss_train: 0.08437504865514549, Loss_test: 0.08355294610759335\n",
      "3838 - Loss_train: 0.08437434233002068, Loss_test: 0.08355239891873519\n",
      "3839 - Loss_train: 0.08437363637257964, Loss_test: 0.08355185205814926\n",
      "3840 - Loss_train: 0.08437293078214404, Loss_test: 0.0835513055221399\n",
      "3841 - Loss_train: 0.08437222555717111, Loss_test: 0.08355075931256824\n",
      "3842 - Loss_train: 0.08437152069874942, Loss_test: 0.08355021342766557\n",
      "3843 - Loss_train: 0.08437081620632529, Loss_test: 0.08354966786735812\n",
      "3844 - Loss_train: 0.08437011207978301, Loss_test: 0.08354912263198926\n",
      "3845 - Loss_train: 0.08436940831936036, Loss_test: 0.08354857772182764\n",
      "3846 - Loss_train: 0.08436870492447034, Loss_test: 0.08354803313731372\n",
      "3847 - Loss_train: 0.08436800189510903, Loss_test: 0.08354748887590192\n",
      "3848 - Loss_train: 0.08436729923190078, Loss_test: 0.08354694493956143\n",
      "3849 - Loss_train: 0.08436659693406494, Loss_test: 0.083546401328658\n",
      "3850 - Loss_train: 0.08436589499990899, Loss_test: 0.0835458580395367\n",
      "3851 - Loss_train: 0.0843651934306459, Loss_test: 0.08354531507712318\n",
      "3852 - Loss_train: 0.084364492225611, Loss_test: 0.08354477243665599\n",
      "3853 - Loss_train: 0.0843637913850966, Loss_test: 0.08354423011943608\n",
      "3854 - Loss_train: 0.08436309090888876, Loss_test: 0.08354368812577286\n",
      "3855 - Loss_train: 0.0843623907967385, Loss_test: 0.08354314645407754\n",
      "3856 - Loss_train: 0.08436169104883628, Loss_test: 0.08354260510791196\n",
      "3857 - Loss_train: 0.08436099166358516, Loss_test: 0.08354206408159975\n",
      "3858 - Loss_train: 0.0843602926418879, Loss_test: 0.08354152338181756\n",
      "3859 - Loss_train: 0.08435959398409362, Loss_test: 0.0835409830004855\n",
      "3860 - Loss_train: 0.08435889568852699, Loss_test: 0.08354044294252347\n",
      "3861 - Loss_train: 0.08435819775573837, Loss_test: 0.08353990320706321\n",
      "3862 - Loss_train: 0.0843575001854429, Loss_test: 0.08353936379296052\n",
      "3863 - Loss_train: 0.08435680297762221, Loss_test: 0.08353882470073293\n",
      "3864 - Loss_train: 0.08435610613211718, Loss_test: 0.08353828593083792\n",
      "3865 - Loss_train: 0.08435540964860516, Loss_test: 0.08353774748112726\n",
      "3866 - Loss_train: 0.08435471352717364, Loss_test: 0.083537209352808\n",
      "3867 - Loss_train: 0.08435401776738616, Loss_test: 0.08353667154616214\n",
      "3868 - Loss_train: 0.08435332236902063, Loss_test: 0.0835361340592008\n",
      "3869 - Loss_train: 0.08435262733182008, Loss_test: 0.08353559689370503\n",
      "3870 - Loss_train: 0.08435193265554224, Loss_test: 0.08353506004865839\n",
      "3871 - Loss_train: 0.08435123834134271, Loss_test: 0.08353452352374414\n",
      "3872 - Loss_train: 0.08435054438696112, Loss_test: 0.08353398731922164\n",
      "3873 - Loss_train: 0.08434985079348166, Loss_test: 0.08353345143350815\n",
      "3874 - Loss_train: 0.08434915755971996, Loss_test: 0.08353291586884284\n",
      "3875 - Loss_train: 0.08434846468656661, Loss_test: 0.08353238062203834\n",
      "3876 - Loss_train: 0.08434777217315508, Loss_test: 0.08353184569615375\n",
      "3877 - Loss_train: 0.08434708002047052, Loss_test: 0.08353131108966885\n",
      "3878 - Loss_train: 0.08434638822663466, Loss_test: 0.08353077680094755\n",
      "3879 - Loss_train: 0.08434569679316314, Loss_test: 0.08353024283021776\n",
      "3880 - Loss_train: 0.08434500571808215, Loss_test: 0.08352970918293595\n",
      "3881 - Loss_train: 0.08434431500217102, Loss_test: 0.08352917584969506\n",
      "3882 - Loss_train: 0.0843436246451115, Loss_test: 0.0835286428360577\n",
      "3883 - Loss_train: 0.08434293464655956, Loss_test: 0.08352811014172676\n",
      "3884 - Loss_train: 0.08434224500661194, Loss_test: 0.08352757776293451\n",
      "3885 - Loss_train: 0.08434155572570902, Loss_test: 0.08352704570432026\n",
      "3886 - Loss_train: 0.08434086680252802, Loss_test: 0.08352651396061113\n",
      "3887 - Loss_train: 0.08434017823696638, Loss_test: 0.08352598253760667\n",
      "3888 - Loss_train: 0.08433949002938787, Loss_test: 0.08352545142861406\n",
      "3889 - Loss_train: 0.08433880217906835, Loss_test: 0.08352492064015575\n",
      "3890 - Loss_train: 0.08433811468622518, Loss_test: 0.08352439016524112\n",
      "3891 - Loss_train: 0.08433742755075123, Loss_test: 0.08352386000970452\n",
      "3892 - Loss_train: 0.08433674077296252, Loss_test: 0.08352333016898066\n",
      "3893 - Loss_train: 0.08433605435107483, Loss_test: 0.0835228006468176\n",
      "3894 - Loss_train: 0.08433536828612981, Loss_test: 0.08352227143927628\n",
      "3895 - Loss_train: 0.08433468257735652, Loss_test: 0.08352174254596734\n",
      "3896 - Loss_train: 0.08433399722492897, Loss_test: 0.08352121397336565\n",
      "3897 - Loss_train: 0.08433331222869231, Loss_test: 0.08352068571170307\n",
      "3898 - Loss_train: 0.08433262758842976, Loss_test: 0.08352015776822014\n",
      "3899 - Loss_train: 0.08433194330460864, Loss_test: 0.08351963013949527\n",
      "3900 - Loss_train: 0.08433125937645065, Loss_test: 0.0835191028254978\n",
      "3901 - Loss_train: 0.08433057580267958, Loss_test: 0.08351857582845537\n",
      "3902 - Loss_train: 0.08432989258438021, Loss_test: 0.08351804914430778\n",
      "3903 - Loss_train: 0.08432920972089815, Loss_test: 0.08351752277478312\n",
      "3904 - Loss_train: 0.08432852721210958, Loss_test: 0.08351699672102332\n",
      "3905 - Loss_train: 0.08432784505800239, Loss_test: 0.08351647097808054\n",
      "3906 - Loss_train: 0.08432716325936954, Loss_test: 0.08351594555492284\n",
      "3907 - Loss_train: 0.0843264818142381, Loss_test: 0.08351542044099498\n",
      "3908 - Loss_train: 0.08432580072294586, Loss_test: 0.08351489564371274\n",
      "3909 - Loss_train: 0.08432511998576167, Loss_test: 0.0835143711602286\n",
      "3910 - Loss_train: 0.08432443960264796, Loss_test: 0.08351384698855217\n",
      "3911 - Loss_train: 0.08432375957239689, Loss_test: 0.08351332312918851\n",
      "3912 - Loss_train: 0.08432307989622445, Loss_test: 0.0835127995864679\n",
      "3913 - Loss_train: 0.08432240057319362, Loss_test: 0.08351227635313656\n",
      "3914 - Loss_train: 0.08432172160227334, Loss_test: 0.08351175343300261\n",
      "3915 - Loss_train: 0.08432104298420663, Loss_test: 0.08351123082802492\n",
      "3916 - Loss_train: 0.08432036471900449, Loss_test: 0.08351070853353174\n",
      "3917 - Loss_train: 0.08431968680598374, Loss_test: 0.08351018655098277\n",
      "3918 - Loss_train: 0.0843190092453135, Loss_test: 0.08350966488109622\n",
      "3919 - Loss_train: 0.08431833203738717, Loss_test: 0.08350914352192618\n",
      "3920 - Loss_train: 0.08431765518065654, Loss_test: 0.0835086224765887\n",
      "3921 - Loss_train: 0.08431697867567711, Loss_test: 0.08350810174200231\n",
      "3922 - Loss_train: 0.08431630252204116, Loss_test: 0.08350758131742902\n",
      "3923 - Loss_train: 0.08431562671974761, Loss_test: 0.08350706120633065\n",
      "3924 - Loss_train: 0.08431495126880424, Loss_test: 0.08350654140417163\n",
      "3925 - Loss_train: 0.08431427616935991, Loss_test: 0.08350602191421141\n",
      "3926 - Loss_train: 0.0843136014198617, Loss_test: 0.08350550273388273\n",
      "3927 - Loss_train: 0.08431292702107036, Loss_test: 0.08350498386604398\n",
      "3928 - Loss_train: 0.08431225297360945, Loss_test: 0.08350446530559395\n",
      "3929 - Loss_train: 0.0843115792754475, Loss_test: 0.08350394705846968\n",
      "3930 - Loss_train: 0.08431090592737611, Loss_test: 0.08350342911787163\n",
      "3931 - Loss_train: 0.08431023292899732, Loss_test: 0.08350291149042599\n",
      "3932 - Loss_train: 0.08430956028126077, Loss_test: 0.08350239417095379\n",
      "3933 - Loss_train: 0.0843088879830223, Loss_test: 0.08350187716250668\n",
      "3934 - Loss_train: 0.08430821603371687, Loss_test: 0.0835013604600527\n",
      "3935 - Loss_train: 0.08430754443319172, Loss_test: 0.08350084406985597\n",
      "3936 - Loss_train: 0.08430687318180799, Loss_test: 0.08350032798851635\n",
      "3937 - Loss_train: 0.08430620227920887, Loss_test: 0.08349981221465852\n",
      "3938 - Loss_train: 0.08430553172518275, Loss_test: 0.08349929675030043\n",
      "3939 - Loss_train: 0.08430486151947683, Loss_test: 0.08349878159340679\n",
      "3940 - Loss_train: 0.08430419166195505, Loss_test: 0.08349826674433286\n",
      "3941 - Loss_train: 0.08430352215318657, Loss_test: 0.08349775220661866\n",
      "3942 - Loss_train: 0.084302852991642, Loss_test: 0.0834972379734867\n",
      "3943 - Loss_train: 0.08430218417779871, Loss_test: 0.08349672404976904\n",
      "3944 - Loss_train: 0.08430151571101858, Loss_test: 0.08349621043218533\n",
      "3945 - Loss_train: 0.08430084759197723, Loss_test: 0.08349569712250453\n",
      "3946 - Loss_train: 0.08430017982085942, Loss_test: 0.08349518412163413\n",
      "3947 - Loss_train: 0.0842995123959593, Loss_test: 0.08349467142627905\n",
      "3948 - Loss_train: 0.08429884531788252, Loss_test: 0.08349415903826608\n",
      "3949 - Loss_train: 0.08429817858648325, Loss_test: 0.08349364695574323\n",
      "3950 - Loss_train: 0.08429751220135534, Loss_test: 0.08349313518318348\n",
      "3951 - Loss_train: 0.08429684616237013, Loss_test: 0.08349262371326717\n",
      "3952 - Loss_train: 0.08429618046968738, Loss_test: 0.08349211255251379\n",
      "3953 - Loss_train: 0.0842955151235848, Loss_test: 0.08349160169584029\n",
      "3954 - Loss_train: 0.08429485012267966, Loss_test: 0.08349109114595042\n",
      "3955 - Loss_train: 0.08429418546812138, Loss_test: 0.08349058090237287\n",
      "3956 - Loss_train: 0.08429352115788942, Loss_test: 0.08349007096386069\n",
      "3957 - Loss_train: 0.08429285719298056, Loss_test: 0.08348956133042618\n",
      "3958 - Loss_train: 0.08429219357281177, Loss_test: 0.08348905200318107\n",
      "3959 - Loss_train: 0.08429153029765185, Loss_test: 0.08348854297937672\n",
      "3960 - Loss_train: 0.08429086736803872, Loss_test: 0.08348803426199382\n",
      "3961 - Loss_train: 0.08429020478211867, Loss_test: 0.08348752584874462\n",
      "3962 - Loss_train: 0.08428954254049655, Loss_test: 0.08348701773906828\n",
      "3963 - Loss_train: 0.08428888064304196, Loss_test: 0.08348650993660911\n",
      "3964 - Loss_train: 0.08428821908932915, Loss_test: 0.08348600243569974\n",
      "3965 - Loss_train: 0.08428755788004551, Loss_test: 0.08348549523987292\n",
      "3966 - Loss_train: 0.08428689701357209, Loss_test: 0.08348498834895386\n",
      "3967 - Loss_train: 0.08428623649116919, Loss_test: 0.08348448175843552\n",
      "3968 - Loss_train: 0.0842855763114527, Loss_test: 0.08348397547361391\n",
      "3969 - Loss_train: 0.0842849164746562, Loss_test: 0.08348346949398817\n",
      "3970 - Loss_train: 0.08428425698062537, Loss_test: 0.08348296381399586\n",
      "3971 - Loss_train: 0.08428359782954276, Loss_test: 0.08348245843969708\n",
      "3972 - Loss_train: 0.08428293902134423, Loss_test: 0.08348195336691444\n",
      "3973 - Loss_train: 0.08428228055509904, Loss_test: 0.08348144859781151\n",
      "3974 - Loss_train: 0.0842816224315987, Loss_test: 0.08348094413042711\n",
      "3975 - Loss_train: 0.08428096464964271, Loss_test: 0.08348043996409635\n",
      "3976 - Loss_train: 0.08428030720913243, Loss_test: 0.08347993610391759\n",
      "3977 - Loss_train: 0.08427965011013852, Loss_test: 0.08347943254303454\n",
      "3978 - Loss_train: 0.08427899335280842, Loss_test: 0.08347892928411305\n",
      "3979 - Loss_train: 0.0842783369367866, Loss_test: 0.08347842632721933\n",
      "3980 - Loss_train: 0.08427768086235278, Loss_test: 0.08347792367174149\n",
      "3981 - Loss_train: 0.08427702512806202, Loss_test: 0.08347742131675792\n",
      "3982 - Loss_train: 0.08427636973461522, Loss_test: 0.0834769192648073\n",
      "3983 - Loss_train: 0.08427571468176466, Loss_test: 0.08347641751275482\n",
      "3984 - Loss_train: 0.084275059969989, Loss_test: 0.08347591606249583\n",
      "3985 - Loss_train: 0.08427440559754298, Loss_test: 0.08347541491051233\n",
      "3986 - Loss_train: 0.08427375156594365, Loss_test: 0.08347491406267393\n",
      "3987 - Loss_train: 0.08427309787336439, Loss_test: 0.08347441351306502\n",
      "3988 - Loss_train: 0.08427244452094296, Loss_test: 0.0834739132643485\n",
      "3989 - Loss_train: 0.0842717915076462, Loss_test: 0.08347341331516146\n",
      "3990 - Loss_train: 0.08427113883345197, Loss_test: 0.08347291366494648\n",
      "3991 - Loss_train: 0.0842704864986027, Loss_test: 0.08347241431549965\n",
      "3992 - Loss_train: 0.08426983450305299, Loss_test: 0.08347191526546122\n",
      "3993 - Loss_train: 0.08426918284602974, Loss_test: 0.0834714165130503\n",
      "3994 - Loss_train: 0.08426853152751079, Loss_test: 0.08347091806359594\n",
      "3995 - Loss_train: 0.08426788054846711, Loss_test: 0.08347041991106054\n",
      "3996 - Loss_train: 0.08426722990752546, Loss_test: 0.08346992205833678\n",
      "3997 - Loss_train: 0.08426657960383624, Loss_test: 0.08346942450316717\n",
      "3998 - Loss_train: 0.08426592963807439, Loss_test: 0.08346892724831576\n",
      "3999 - Loss_train: 0.08426528000993631, Loss_test: 0.08346843028904809\n",
      "4000 - Loss_train: 0.08426463071957775, Loss_test: 0.08346793362904961\n",
      "4001 - Loss_train: 0.08426398176741198, Loss_test: 0.08346743726987202\n",
      "4002 - Loss_train: 0.08426333315153613, Loss_test: 0.08346694120402856\n",
      "4003 - Loss_train: 0.08426268487263053, Loss_test: 0.08346644544050864\n",
      "4004 - Loss_train: 0.08426203693087218, Loss_test: 0.08346594997004374\n",
      "4005 - Loss_train: 0.08426138932576248, Loss_test: 0.08346545479992441\n",
      "4006 - Loss_train: 0.08426074205741743, Loss_test: 0.0834649599264372\n",
      "4007 - Loss_train: 0.08426009512599321, Loss_test: 0.08346446534885266\n",
      "4008 - Loss_train: 0.0842594485297217, Loss_test: 0.08346397106861868\n",
      "4009 - Loss_train: 0.08425880227056506, Loss_test: 0.08346347708876917\n",
      "4010 - Loss_train: 0.08425815634688415, Loss_test: 0.0834629834014511\n",
      "4011 - Loss_train: 0.08425751075887034, Loss_test: 0.08346249001233444\n",
      "4012 - Loss_train: 0.08425686550658841, Loss_test: 0.08346199691861353\n",
      "4013 - Loss_train: 0.08425622058858787, Loss_test: 0.08346150412049312\n",
      "4014 - Loss_train: 0.08425557600638245, Loss_test: 0.08346101162091371\n",
      "4015 - Loss_train: 0.08425493175851434, Loss_test: 0.08346051941296276\n",
      "4016 - Loss_train: 0.0842542878453843, Loss_test: 0.0834600275015983\n",
      "4017 - Loss_train: 0.08425364426693771, Loss_test: 0.08345953588773429\n",
      "4018 - Loss_train: 0.08425300102306461, Loss_test: 0.08345904456760941\n",
      "4019 - Loss_train: 0.0842523581138191, Loss_test: 0.08345855354500882\n",
      "4020 - Loss_train: 0.08425171553800197, Loss_test: 0.08345806281256272\n",
      "4021 - Loss_train: 0.08425107329618305, Loss_test: 0.08345757237981945\n",
      "4022 - Loss_train: 0.08425043138880739, Loss_test: 0.08345708223817573\n",
      "4023 - Loss_train: 0.0842497898149047, Loss_test: 0.08345659239400723\n",
      "4024 - Loss_train: 0.084249148573766, Loss_test: 0.0834561028400975\n",
      "4025 - Loss_train: 0.08424850766560597, Loss_test: 0.08345561358548383\n",
      "4026 - Loss_train: 0.08424786709087659, Loss_test: 0.08345512461964454\n",
      "4027 - Loss_train: 0.08424722684860675, Loss_test: 0.08345463595129238\n",
      "4028 - Loss_train: 0.08424658693905783, Loss_test: 0.08345414757239657\n",
      "4029 - Loss_train: 0.08424594736201936, Loss_test: 0.08345365949300351\n",
      "4030 - Loss_train: 0.08424530811820197, Loss_test: 0.08345317170118358\n",
      "4031 - Loss_train: 0.08424466920576147, Loss_test: 0.08345268420575219\n",
      "4032 - Loss_train: 0.0842440306264517, Loss_test: 0.08345219700239466\n",
      "4033 - Loss_train: 0.08424339237857899, Loss_test: 0.08345171009243973\n",
      "4034 - Loss_train: 0.08424275446175485, Loss_test: 0.08345122347508124\n",
      "4035 - Loss_train: 0.08424211687648274, Loss_test: 0.08345073714913752\n",
      "4036 - Loss_train: 0.08424147962258477, Loss_test: 0.08345025111590865\n",
      "4037 - Loss_train: 0.08424084270035737, Loss_test: 0.08344976537521268\n",
      "4038 - Loss_train: 0.08424020610825815, Loss_test: 0.08344927992818454\n",
      "4039 - Loss_train: 0.08423956984722568, Loss_test: 0.08344879476928367\n",
      "4040 - Loss_train: 0.08423893391679595, Loss_test: 0.08344830990509834\n",
      "4041 - Loss_train: 0.08423829831766455, Loss_test: 0.08344782532987477\n",
      "4042 - Loss_train: 0.08423766304810718, Loss_test: 0.08344734105144187\n",
      "4043 - Loss_train: 0.08423702810921721, Loss_test: 0.0834468570591497\n",
      "4044 - Loss_train: 0.08423639349951638, Loss_test: 0.08344637335845073\n",
      "4045 - Loss_train: 0.08423575921970171, Loss_test: 0.08344588994738433\n",
      "4046 - Loss_train: 0.08423512526921438, Loss_test: 0.08344540683145486\n",
      "4047 - Loss_train: 0.08423449164862462, Loss_test: 0.08344492400089204\n",
      "4048 - Loss_train: 0.08423385835803757, Loss_test: 0.08344444146493726\n",
      "4049 - Loss_train: 0.08423322539640886, Loss_test: 0.08344395921871729\n",
      "4050 - Loss_train: 0.08423259276275949, Loss_test: 0.08344347726039837\n",
      "4051 - Loss_train: 0.08423196045780168, Loss_test: 0.08344299559385845\n",
      "4052 - Loss_train: 0.08423132848168746, Loss_test: 0.08344251421456134\n",
      "4053 - Loss_train: 0.08423069683472702, Loss_test: 0.08344203312923194\n",
      "4054 - Loss_train: 0.08423006551582893, Loss_test: 0.0834415523293517\n",
      "4055 - Loss_train: 0.08422943452508583, Loss_test: 0.08344107182067372\n",
      "4056 - Loss_train: 0.08422880386164874, Loss_test: 0.08344059159999695\n",
      "4057 - Loss_train: 0.0842281735261615, Loss_test: 0.08344011166974745\n",
      "4058 - Loss_train: 0.08422754351788347, Loss_test: 0.08343963202681563\n",
      "4059 - Loss_train: 0.08422691383689646, Loss_test: 0.08343915267380217\n",
      "4060 - Loss_train: 0.0842262844838283, Loss_test: 0.08343867360775165\n",
      "4061 - Loss_train: 0.08422565545694782, Loss_test: 0.08343819483081322\n",
      "4062 - Loss_train: 0.08422502675726078, Loss_test: 0.08343771634162722\n",
      "4063 - Loss_train: 0.08422439838493134, Loss_test: 0.08343723814162807\n",
      "4064 - Loss_train: 0.08422377033821296, Loss_test: 0.08343676022715246\n",
      "4065 - Loss_train: 0.0842231426186651, Loss_test: 0.08343628260229706\n",
      "4066 - Loss_train: 0.08422251522538725, Loss_test: 0.08343580526502067\n",
      "4067 - Loss_train: 0.08422188815745132, Loss_test: 0.08343532821488034\n",
      "4068 - Loss_train: 0.0842212614153766, Loss_test: 0.08343485145040129\n",
      "4069 - Loss_train: 0.08422063499906514, Loss_test: 0.08343437497403595\n",
      "4070 - Loss_train: 0.08422000890882882, Loss_test: 0.08343389878639249\n",
      "4071 - Loss_train: 0.0842193831440313, Loss_test: 0.08343342288247106\n",
      "4072 - Loss_train: 0.08421875770355328, Loss_test: 0.0834329472678335\n",
      "4073 - Loss_train: 0.0842181325882065, Loss_test: 0.08343247193589157\n",
      "4074 - Loss_train: 0.08421750779768984, Loss_test: 0.0834319968937101\n",
      "4075 - Loss_train: 0.08421688333177847, Loss_test: 0.08343152213414855\n",
      "4076 - Loss_train: 0.08421625919046388, Loss_test: 0.08343104766269885\n",
      "4077 - Loss_train: 0.08421563537352793, Loss_test: 0.08343057347907552\n",
      "4078 - Loss_train: 0.08421501188081211, Loss_test: 0.08343009957688705\n",
      "4079 - Loss_train: 0.0842143887120384, Loss_test: 0.08342962596263344\n",
      "4080 - Loss_train: 0.08421376586781168, Loss_test: 0.08342915263362378\n",
      "4081 - Loss_train: 0.08421314334688271, Loss_test: 0.08342867958933166\n",
      "4082 - Loss_train: 0.08421252114929584, Loss_test: 0.08342820683014425\n",
      "4083 - Loss_train: 0.08421189927559063, Loss_test: 0.08342773435652098\n",
      "4084 - Loss_train: 0.0842112777243339, Loss_test: 0.08342726216615688\n",
      "4085 - Loss_train: 0.08421065649664017, Loss_test: 0.08342679026036899\n",
      "4086 - Loss_train: 0.08421003559118204, Loss_test: 0.08342631863819411\n",
      "4087 - Loss_train: 0.08420941500823957, Loss_test: 0.08342584730268247\n",
      "4088 - Loss_train: 0.08420879474771614, Loss_test: 0.08342537624991453\n",
      "4089 - Loss_train: 0.08420817480976875, Loss_test: 0.08342490547968055\n",
      "4090 - Loss_train: 0.08420755519466848, Loss_test: 0.08342443499538925\n",
      "4091 - Loss_train: 0.08420693590148938, Loss_test: 0.08342396479311055\n",
      "4092 - Loss_train: 0.08420631692942276, Loss_test: 0.08342349487535068\n",
      "4093 - Loss_train: 0.08420569827910701, Loss_test: 0.08342302523880328\n",
      "4094 - Loss_train: 0.08420507995003988, Loss_test: 0.08342255588686967\n",
      "4095 - Loss_train: 0.08420446194244717, Loss_test: 0.08342208681844338\n",
      "4096 - Loss_train: 0.08420384425612369, Loss_test: 0.08342161803113372\n",
      "4097 - Loss_train: 0.08420322689148141, Loss_test: 0.08342114952725078\n",
      "4098 - Loss_train: 0.0842026098470952, Loss_test: 0.08342068130613091\n",
      "4099 - Loss_train: 0.08420199312322495, Loss_test: 0.08342021336654538\n",
      "4100 - Loss_train: 0.08420137671980804, Loss_test: 0.08341974571003614\n",
      "4101 - Loss_train: 0.08420076063669729, Loss_test: 0.08341927833565424\n",
      "4102 - Loss_train: 0.08420014487453463, Loss_test: 0.08341881124237553\n",
      "4103 - Loss_train: 0.08419952943157452, Loss_test: 0.08341834443069969\n",
      "4104 - Loss_train: 0.08419891430854957, Loss_test: 0.08341787790063136\n",
      "4105 - Loss_train: 0.08419829950510414, Loss_test: 0.08341741165242895\n",
      "4106 - Loss_train: 0.08419768502108853, Loss_test: 0.08341694568467944\n",
      "4107 - Loss_train: 0.08419707085656813, Loss_test: 0.08341647999908292\n",
      "4108 - Loss_train: 0.08419645701106548, Loss_test: 0.08341601459215703\n",
      "4109 - Loss_train: 0.08419584348473523, Loss_test: 0.08341554946647436\n",
      "4110 - Loss_train: 0.0841952302772874, Loss_test: 0.08341508462383732\n",
      "4111 - Loss_train: 0.08419461738848445, Loss_test: 0.08341462005878761\n",
      "4112 - Loss_train: 0.08419400481863394, Loss_test: 0.08341415577757455\n",
      "4113 - Loss_train: 0.08419339256658227, Loss_test: 0.08341369177208774\n",
      "4114 - Loss_train: 0.08419278063272177, Loss_test: 0.08341322805140587\n",
      "4115 - Loss_train: 0.08419216901749244, Loss_test: 0.08341276460502402\n",
      "4116 - Loss_train: 0.08419155771971532, Loss_test: 0.08341230144308015\n",
      "4117 - Loss_train: 0.08419094673929249, Loss_test: 0.08341183855606311\n",
      "4118 - Loss_train: 0.0841903360765817, Loss_test: 0.08341137595349195\n",
      "4119 - Loss_train: 0.08418972573104774, Loss_test: 0.08341091362735054\n",
      "4120 - Loss_train: 0.08418911570335624, Loss_test: 0.08341045157981342\n",
      "4121 - Loss_train: 0.08418850599197454, Loss_test: 0.08340998981269138\n",
      "4122 - Loss_train: 0.08418789659753209, Loss_test: 0.0834095283226565\n",
      "4123 - Loss_train: 0.08418728752062782, Loss_test: 0.08340906711146528\n",
      "4124 - Loss_train: 0.08418667876018403, Loss_test: 0.08340860618119104\n",
      "4125 - Loss_train: 0.08418607031582455, Loss_test: 0.08340814552586481\n",
      "4126 - Loss_train: 0.08418546218802901, Loss_test: 0.0834076851510742\n",
      "4127 - Loss_train: 0.08418485437512473, Loss_test: 0.08340722505180104\n",
      "4128 - Loss_train: 0.08418424687930219, Loss_test: 0.08340676523305719\n",
      "4129 - Loss_train: 0.08418363969854901, Loss_test: 0.08340630569038998\n",
      "4130 - Loss_train: 0.08418303283363107, Loss_test: 0.08340584642736239\n",
      "4131 - Loss_train: 0.08418242628329493, Loss_test: 0.08340538743678201\n",
      "4132 - Loss_train: 0.08418182004899138, Loss_test: 0.0834049287283551\n",
      "4133 - Loss_train: 0.084181214129101, Loss_test: 0.08340447029222854\n",
      "4134 - Loss_train: 0.08418060852406412, Loss_test: 0.08340401213622328\n",
      "4135 - Loss_train: 0.08418000323377738, Loss_test: 0.08340355425553751\n",
      "4136 - Loss_train: 0.08417939825796145, Loss_test: 0.08340309665147012\n",
      "4137 - Loss_train: 0.08417879359663143, Loss_test: 0.08340263932441512\n",
      "4138 - Loss_train: 0.08417818924960765, Loss_test: 0.08340218227408673\n",
      "4139 - Loss_train: 0.08417758521683882, Loss_test: 0.08340172549752518\n",
      "4140 - Loss_train: 0.084176981497695, Loss_test: 0.08340126900136566\n",
      "4141 - Loss_train: 0.08417637809307693, Loss_test: 0.08340081277636187\n",
      "4142 - Loss_train: 0.08417577500221007, Loss_test: 0.08340035683172244\n",
      "4143 - Loss_train: 0.08417517222378733, Loss_test: 0.08339990115916496\n",
      "4144 - Loss_train: 0.0841745697594064, Loss_test: 0.08339944576286566\n",
      "4145 - Loss_train: 0.08417396760739515, Loss_test: 0.08339899063977872\n",
      "4146 - Loss_train: 0.08417336576885799, Loss_test: 0.08339853579362683\n",
      "4147 - Loss_train: 0.08417276424253954, Loss_test: 0.08339808122171566\n",
      "4148 - Loss_train: 0.08417216302879624, Loss_test: 0.08339762692541224\n",
      "4149 - Loss_train: 0.08417156212849215, Loss_test: 0.08339717290302268\n",
      "4150 - Loss_train: 0.08417096153933508, Loss_test: 0.0833967191578525\n",
      "4151 - Loss_train: 0.08417036126248871, Loss_test: 0.08339626568077578\n",
      "4152 - Loss_train: 0.08416976129747972, Loss_test: 0.08339581248254357\n",
      "4153 - Loss_train: 0.08416916164528361, Loss_test: 0.0833953595560694\n",
      "4154 - Loss_train: 0.08416856230466092, Loss_test: 0.0833949069072779\n",
      "4155 - Loss_train: 0.0841679632754048, Loss_test: 0.0833944545281447\n",
      "4156 - Loss_train: 0.08416736455669827, Loss_test: 0.08339400242416005\n",
      "4157 - Loss_train: 0.08416676614931473, Loss_test: 0.08339355059171842\n",
      "4158 - Loss_train: 0.0841661680528744, Loss_test: 0.08339309903421943\n",
      "4159 - Loss_train: 0.0841655702677317, Loss_test: 0.08339264775028161\n",
      "4160 - Loss_train: 0.08416497279262904, Loss_test: 0.08339219673619662\n",
      "4161 - Loss_train: 0.08416437562786415, Loss_test: 0.0833917459985066\n",
      "4162 - Loss_train: 0.08416377877445795, Loss_test: 0.08339129552928248\n",
      "4163 - Loss_train: 0.08416318223148284, Loss_test: 0.08339084533678068\n",
      "4164 - Loss_train: 0.08416258599750602, Loss_test: 0.08339039541614789\n",
      "4165 - Loss_train: 0.08416199007345151, Loss_test: 0.08338994576484768\n",
      "4166 - Loss_train: 0.08416139445946023, Loss_test: 0.08338949638886926\n",
      "4167 - Loss_train: 0.08416079915536911, Loss_test: 0.0833890472820406\n",
      "4168 - Loss_train: 0.08416020415989542, Loss_test: 0.0833885984459405\n",
      "4169 - Loss_train: 0.08415960947355478, Loss_test: 0.08338814988519473\n",
      "4170 - Loss_train: 0.08415901509686642, Loss_test: 0.08338770159291636\n",
      "4171 - Loss_train: 0.08415842102881822, Loss_test: 0.08338725357294122\n",
      "4172 - Loss_train: 0.08415782726937955, Loss_test: 0.0833868058225485\n",
      "4173 - Loss_train: 0.08415723381910152, Loss_test: 0.08338635834643877\n",
      "4174 - Loss_train: 0.08415664067639612, Loss_test: 0.08338591113873377\n",
      "4175 - Loss_train: 0.08415604784196182, Loss_test: 0.08338546420142838\n",
      "4176 - Loss_train: 0.08415545531578005, Loss_test: 0.08338501753426274\n",
      "4177 - Loss_train: 0.08415486309835962, Loss_test: 0.08338457113812427\n",
      "4178 - Loss_train: 0.08415427118787826, Loss_test: 0.08338412501431483\n",
      "4179 - Loss_train: 0.08415367958584422, Loss_test: 0.08338367915734853\n",
      "4180 - Loss_train: 0.08415308829034598, Loss_test: 0.08338323357098713\n",
      "4181 - Loss_train: 0.08415249730220947, Loss_test: 0.08338278825560874\n",
      "4182 - Loss_train: 0.08415190662103943, Loss_test: 0.08338234320805962\n",
      "4183 - Loss_train: 0.08415131624698924, Loss_test: 0.08338189843273201\n",
      "4184 - Loss_train: 0.08415072617981606, Loss_test: 0.08338145392217627\n",
      "4185 - Loss_train: 0.08415013641996225, Loss_test: 0.0833810096843382\n",
      "4186 - Loss_train: 0.08414954696588468, Loss_test: 0.08338056571559105\n",
      "4187 - Loss_train: 0.08414895781820032, Loss_test: 0.08338012201305854\n",
      "4188 - Loss_train: 0.08414836897734089, Loss_test: 0.08337967858172798\n",
      "4189 - Loss_train: 0.08414778044179227, Loss_test: 0.08337923541751505\n",
      "4190 - Loss_train: 0.08414719221227776, Loss_test: 0.08337879252214632\n",
      "4191 - Loss_train: 0.08414660428843945, Loss_test: 0.08337834989388207\n",
      "4192 - Loss_train: 0.08414601667078836, Loss_test: 0.08337790753718796\n",
      "4193 - Loss_train: 0.08414542935795603, Loss_test: 0.0833774654440964\n",
      "4194 - Loss_train: 0.08414484235034701, Loss_test: 0.0833770236220228\n",
      "4195 - Loss_train: 0.08414425564785828, Loss_test: 0.08337658206587846\n",
      "4196 - Loss_train: 0.08414366925006483, Loss_test: 0.08337614077768278\n",
      "4197 - Loss_train: 0.08414308315713662, Loss_test: 0.08337569975628499\n",
      "4198 - Loss_train: 0.08414249736867062, Loss_test: 0.0833752590010717\n",
      "4199 - Loss_train: 0.08414191188483694, Loss_test: 0.08337481851492516\n",
      "4200 - Loss_train: 0.08414132670535761, Loss_test: 0.08337437829593555\n",
      "4201 - Loss_train: 0.08414074182980955, Loss_test: 0.08337393834324444\n",
      "4202 - Loss_train: 0.08414015725846427, Loss_test: 0.08337349865562764\n",
      "4203 - Loss_train: 0.08413957299087638, Loss_test: 0.08337305923508649\n",
      "4204 - Loss_train: 0.08413898902685518, Loss_test: 0.08337262008277964\n",
      "4205 - Loss_train: 0.08413840536622855, Loss_test: 0.08337218119476707\n",
      "4206 - Loss_train: 0.08413782200908775, Loss_test: 0.08337174257183327\n",
      "4207 - Loss_train: 0.08413723895564608, Loss_test: 0.0833713042179229\n",
      "4208 - Loss_train: 0.08413665620458137, Loss_test: 0.08337086612714165\n",
      "4209 - Loss_train: 0.08413607375737958, Loss_test: 0.08337042830302199\n",
      "4210 - Loss_train: 0.08413549161183308, Loss_test: 0.08336999074461292\n",
      "4211 - Loss_train: 0.08413490976889385, Loss_test: 0.08336955345145053\n",
      "4212 - Loss_train: 0.08413432822840605, Loss_test: 0.08336911642212472\n",
      "4213 - Loss_train: 0.08413374699013798, Loss_test: 0.08336867965779166\n",
      "4214 - Loss_train: 0.08413316605475313, Loss_test: 0.08336824316073826\n",
      "4215 - Loss_train: 0.08413258542040573, Loss_test: 0.08336780692647244\n",
      "4216 - Loss_train: 0.08413200508794771, Loss_test: 0.08336737095536899\n",
      "4217 - Loss_train: 0.08413142505701904, Loss_test: 0.08336693525136334\n",
      "4218 - Loss_train: 0.08413084532782575, Loss_test: 0.08336649980988015\n",
      "4219 - Loss_train: 0.08413026589965442, Loss_test: 0.08336606463188038\n",
      "4220 - Loss_train: 0.0841296867725787, Loss_test: 0.08336562972007985\n",
      "4221 - Loss_train: 0.08412910794722642, Loss_test: 0.08336519507137935\n",
      "4222 - Loss_train: 0.08412852942206421, Loss_test: 0.08336476068449519\n",
      "4223 - Loss_train: 0.08412795119742646, Loss_test: 0.08336432656280676\n",
      "4224 - Loss_train: 0.08412737327317482, Loss_test: 0.08336389270403302\n",
      "4225 - Loss_train: 0.08412679564931196, Loss_test: 0.0833634591094042\n",
      "4226 - Loss_train: 0.084126218326171, Loss_test: 0.0833630257755359\n",
      "4227 - Loss_train: 0.08412564130335907, Loss_test: 0.0833625927068651\n",
      "4228 - Loss_train: 0.08412506457979685, Loss_test: 0.08336215990019252\n",
      "4229 - Loss_train: 0.08412448815625674, Loss_test: 0.08336172735627874\n",
      "4230 - Loss_train: 0.08412391203281572, Loss_test: 0.08336129507519772\n",
      "4231 - Loss_train: 0.0841233362089604, Loss_test: 0.08336086305517808\n",
      "4232 - Loss_train: 0.08412276068390338, Loss_test: 0.0833604312998505\n",
      "4233 - Loss_train: 0.08412218545830401, Loss_test: 0.08335999980289513\n",
      "4234 - Loss_train: 0.08412161053147252, Loss_test: 0.08335956857139121\n",
      "4235 - Loss_train: 0.08412103590350821, Loss_test: 0.08335913759975246\n",
      "4236 - Loss_train: 0.08412046157371886, Loss_test: 0.08335870688994274\n",
      "4237 - Loss_train: 0.08411988754305567, Loss_test: 0.08335827644369947\n",
      "4238 - Loss_train: 0.08411931380999124, Loss_test: 0.08335784625580885\n",
      "4239 - Loss_train: 0.08411874037497331, Loss_test: 0.08335741632901386\n",
      "4240 - Loss_train: 0.0841181672382037, Loss_test: 0.08335698666458526\n",
      "4241 - Loss_train: 0.08411759439995063, Loss_test: 0.08335655726058462\n",
      "4242 - Loss_train: 0.08411702185925204, Loss_test: 0.08335612811715978\n",
      "4243 - Loss_train: 0.08411644961552632, Loss_test: 0.08335569923661654\n",
      "4244 - Loss_train: 0.08411587766984481, Loss_test: 0.08335527061278533\n",
      "4245 - Loss_train: 0.08411530602072737, Loss_test: 0.08335484225345272\n",
      "4246 - Loss_train: 0.08411473466859094, Loss_test: 0.08335441414927318\n",
      "4247 - Loss_train: 0.08411416361364905, Loss_test: 0.08335398630721605\n",
      "4248 - Loss_train: 0.08411359285530894, Loss_test: 0.08335355872420011\n",
      "4249 - Loss_train: 0.08411302239362035, Loss_test: 0.08335313140238908\n",
      "4250 - Loss_train: 0.08411245222837758, Loss_test: 0.08335270434132251\n",
      "4251 - Loss_train: 0.08411188235947586, Loss_test: 0.08335227753678386\n",
      "4252 - Loss_train: 0.08411131278759214, Loss_test: 0.08335185099225259\n",
      "4253 - Loss_train: 0.08411074351097365, Loss_test: 0.08335142470791446\n",
      "4254 - Loss_train: 0.08411017453023116, Loss_test: 0.08335099868217215\n",
      "4255 - Loss_train: 0.08410960584609528, Loss_test: 0.08335057291620597\n",
      "4256 - Loss_train: 0.08410903745736528, Loss_test: 0.08335014740657266\n",
      "4257 - Loss_train: 0.08410846936387899, Loss_test: 0.08334972215733626\n",
      "4258 - Loss_train: 0.08410790156600854, Loss_test: 0.0833492971678633\n",
      "4259 - Loss_train: 0.08410733406246966, Loss_test: 0.08334887243323039\n",
      "4260 - Loss_train: 0.08410676685370803, Loss_test: 0.08334844795961108\n",
      "4261 - Loss_train: 0.0841061999406265, Loss_test: 0.08334802374070718\n",
      "4262 - Loss_train: 0.08410563332165216, Loss_test: 0.08334759978511803\n",
      "4263 - Loss_train: 0.0841050669968475, Loss_test: 0.08334717608257099\n",
      "4264 - Loss_train: 0.08410450096728114, Loss_test: 0.0833467526389697\n",
      "4265 - Loss_train: 0.08410393523164887, Loss_test: 0.08334632945408514\n",
      "4266 - Loss_train: 0.08410336978960854, Loss_test: 0.08334590652382537\n",
      "4267 - Loss_train: 0.08410280464215932, Loss_test: 0.08334548385120276\n",
      "4268 - Loss_train: 0.08410223978755813, Loss_test: 0.08334506143734387\n",
      "4269 - Loss_train: 0.08410167522731107, Loss_test: 0.0833446392797489\n",
      "4270 - Loss_train: 0.08410111096035021, Loss_test: 0.08334421737946612\n",
      "4271 - Loss_train: 0.08410054698601584, Loss_test: 0.08334379573440963\n",
      "4272 - Loss_train: 0.08409998330493522, Loss_test: 0.08334337434464252\n",
      "4273 - Loss_train: 0.08409941991679235, Loss_test: 0.08334295321290872\n",
      "4274 - Loss_train: 0.08409885682172594, Loss_test: 0.08334253233618293\n",
      "4275 - Loss_train: 0.08409829401916435, Loss_test: 0.0833421117155861\n",
      "4276 - Loss_train: 0.08409773151002244, Loss_test: 0.08334169135166324\n",
      "4277 - Loss_train: 0.08409716929216228, Loss_test: 0.08334127124524769\n",
      "4278 - Loss_train: 0.08409660736670788, Loss_test: 0.08334085139076844\n",
      "4279 - Loss_train: 0.08409604573349008, Loss_test: 0.08334043179310785\n",
      "4280 - Loss_train: 0.0840954843919229, Loss_test: 0.0833400124533225\n",
      "4281 - Loss_train: 0.08409492334270517, Loss_test: 0.08333959336429579\n",
      "4282 - Loss_train: 0.08409436258451661, Loss_test: 0.08333917453330555\n",
      "4283 - Loss_train: 0.08409380211835187, Loss_test: 0.08333875595377437\n",
      "4284 - Loss_train: 0.08409324194295444, Loss_test: 0.08333833763293114\n",
      "4285 - Loss_train: 0.0840926820585678, Loss_test: 0.08333791956455272\n",
      "4286 - Loss_train: 0.0840921224658332, Loss_test: 0.08333750175095392\n",
      "4287 - Loss_train: 0.08409156316350139, Loss_test: 0.08333708419181578\n",
      "4288 - Loss_train: 0.08409100415192916, Loss_test: 0.08333666688568973\n",
      "4289 - Loss_train: 0.08409044543073751, Loss_test: 0.08333624983496081\n",
      "4290 - Loss_train: 0.08408988700073156, Loss_test: 0.08333583303872806\n",
      "4291 - Loss_train: 0.08408932885987715, Loss_test: 0.083335416494914\n",
      "4292 - Loss_train: 0.08408877100958842, Loss_test: 0.08333500020394753\n",
      "4293 - Loss_train: 0.08408821344899584, Loss_test: 0.0833345841684941\n",
      "4294 - Loss_train: 0.08408765617780445, Loss_test: 0.08333416838432046\n",
      "4295 - Loss_train: 0.08408709919662405, Loss_test: 0.08333375285390732\n",
      "4296 - Loss_train: 0.08408654250571888, Loss_test: 0.08333333757667695\n",
      "4297 - Loss_train: 0.08408598610319484, Loss_test: 0.08333292255161437\n",
      "4298 - Loss_train: 0.08408542999082515, Loss_test: 0.08333250778054134\n",
      "4299 - Loss_train: 0.0840848741667447, Loss_test: 0.0833320932611817\n",
      "4300 - Loss_train: 0.08408431863153382, Loss_test: 0.0833316789958047\n",
      "4301 - Loss_train: 0.08408376338601158, Loss_test: 0.08333126498031705\n",
      "4302 - Loss_train: 0.0840832084288583, Loss_test: 0.08333085121989352\n",
      "4303 - Loss_train: 0.08408265376027724, Loss_test: 0.08333043770973735\n",
      "4304 - Loss_train: 0.0840820993792011, Loss_test: 0.08333002445245215\n",
      "4305 - Loss_train: 0.08408154528680568, Loss_test: 0.08332961144487272\n",
      "4306 - Loss_train: 0.08408099148260464, Loss_test: 0.0833291986916783\n",
      "4307 - Loss_train: 0.08408043796610883, Loss_test: 0.08332878618848133\n",
      "4308 - Loss_train: 0.084079884736402, Loss_test: 0.08332837393546169\n",
      "4309 - Loss_train: 0.08407933179483684, Loss_test: 0.0833279619346153\n",
      "4310 - Loss_train: 0.08407877914104003, Loss_test: 0.08332755018612109\n",
      "4311 - Loss_train: 0.08407822677384022, Loss_test: 0.08332713868523477\n",
      "4312 - Loss_train: 0.08407767469350244, Loss_test: 0.08332672743794232\n",
      "4313 - Loss_train: 0.0840771229003784, Loss_test: 0.08332631643936675\n",
      "4314 - Loss_train: 0.08407657139407626, Loss_test: 0.08332590569351701\n",
      "4315 - Loss_train: 0.08407602017432027, Loss_test: 0.08332549519499821\n",
      "4316 - Loss_train: 0.08407546924105212, Loss_test: 0.08332508494913045\n",
      "4317 - Loss_train: 0.08407491859416297, Loss_test: 0.08332467495269288\n",
      "4318 - Loss_train: 0.08407436823397307, Loss_test: 0.08332426520514137\n",
      "4319 - Loss_train: 0.08407381815937022, Loss_test: 0.08332385570856715\n",
      "4320 - Loss_train: 0.08407326837070138, Loss_test: 0.08332344646243414\n",
      "4321 - Loss_train: 0.0840727188686074, Loss_test: 0.08332303746462319\n",
      "4322 - Loss_train: 0.08407216965130135, Loss_test: 0.08332262871684536\n",
      "4323 - Loss_train: 0.08407162072011211, Loss_test: 0.08332222021763598\n",
      "4324 - Loss_train: 0.08407107207413106, Loss_test: 0.08332181196798095\n",
      "4325 - Loss_train: 0.08407052371270222, Loss_test: 0.08332140396632783\n",
      "4326 - Loss_train: 0.08406997563633647, Loss_test: 0.08332099621498013\n",
      "4327 - Loss_train: 0.08406942784481929, Loss_test: 0.08332058871185337\n",
      "4328 - Loss_train: 0.08406888033818057, Loss_test: 0.08332018145649316\n",
      "4329 - Loss_train: 0.08406833311669581, Loss_test: 0.08331977444962654\n",
      "4330 - Loss_train: 0.08406778617875431, Loss_test: 0.08331936769259153\n",
      "4331 - Loss_train: 0.08406723952584229, Loss_test: 0.08331896118231018\n",
      "4332 - Loss_train: 0.08406669315658932, Loss_test: 0.08331855491804331\n",
      "4333 - Loss_train: 0.08406614707117546, Loss_test: 0.083318148905147\n",
      "4334 - Loss_train: 0.08406560126986092, Loss_test: 0.08331774313818728\n",
      "4335 - Loss_train: 0.08406505575212996, Loss_test: 0.08331733761842929\n",
      "4336 - Loss_train: 0.08406451051835256, Loss_test: 0.08331693234487636\n",
      "4337 - Loss_train: 0.08406396556780509, Loss_test: 0.0833165273239614\n",
      "4338 - Loss_train: 0.08406342090020487, Loss_test: 0.08331612254537957\n",
      "4339 - Loss_train: 0.08406287651591031, Loss_test: 0.0833157180166837\n",
      "4340 - Loss_train: 0.084062332414389, Loss_test: 0.0833153137309486\n",
      "4341 - Loss_train: 0.08406178859547624, Loss_test: 0.08331490969553546\n",
      "4342 - Loss_train: 0.08406124505991201, Loss_test: 0.08331450590524225\n",
      "4343 - Loss_train: 0.08406070180623146, Loss_test: 0.08331410236166116\n",
      "4344 - Loss_train: 0.08406015883479022, Loss_test: 0.08331369906477362\n",
      "4345 - Loss_train: 0.08405961614558952, Loss_test: 0.08331329601491408\n",
      "4346 - Loss_train: 0.08405907373922074, Loss_test: 0.08331289320843786\n",
      "4347 - Loss_train: 0.08405853161429554, Loss_test: 0.08331249064906249\n",
      "4348 - Loss_train: 0.0840579897718905, Loss_test: 0.08331208833659287\n",
      "4349 - Loss_train: 0.08405744821081004, Loss_test: 0.08331168627022316\n",
      "4350 - Loss_train: 0.08405690693077068, Loss_test: 0.08331128444720919\n",
      "4351 - Loss_train: 0.08405636593222635, Loss_test: 0.08331088287062142\n",
      "4352 - Loss_train: 0.0840558252146995, Loss_test: 0.08331048153719672\n",
      "4353 - Loss_train: 0.08405528477813329, Loss_test: 0.08331008045246835\n",
      "4354 - Loss_train: 0.08405474462256843, Loss_test: 0.08330967961109066\n",
      "4355 - Loss_train: 0.08405420474791231, Loss_test: 0.08330927901238817\n",
      "4356 - Loss_train: 0.0840536651536077, Loss_test: 0.0833088786622778\n",
      "4357 - Loss_train: 0.0840531258398434, Loss_test: 0.08330847855455609\n",
      "4358 - Loss_train: 0.0840525868067119, Loss_test: 0.08330807869186295\n",
      "4359 - Loss_train: 0.08405204805340385, Loss_test: 0.08330767907074901\n",
      "4360 - Loss_train: 0.08405150958027204, Loss_test: 0.08330727969697926\n",
      "4361 - Loss_train: 0.08405097138674192, Loss_test: 0.08330688056615426\n",
      "4362 - Loss_train: 0.08405043347317008, Loss_test: 0.08330648167956138\n",
      "4363 - Loss_train: 0.08404989583917652, Loss_test: 0.08330608303759354\n",
      "4364 - Loss_train: 0.08404935848498439, Loss_test: 0.08330568463633926\n",
      "4365 - Loss_train: 0.08404882140973816, Loss_test: 0.08330528648285943\n",
      "4366 - Loss_train: 0.08404828461369422, Loss_test: 0.08330488856922659\n",
      "4367 - Loss_train: 0.08404774809668836, Loss_test: 0.08330449089903927\n",
      "4368 - Loss_train: 0.0840472118588246, Loss_test: 0.08330409347259915\n",
      "4369 - Loss_train: 0.08404667589948922, Loss_test: 0.0833036962899975\n",
      "4370 - Loss_train: 0.08404614021852846, Loss_test: 0.0833032993475183\n",
      "4371 - Loss_train: 0.08404560481622916, Loss_test: 0.08330290264977339\n",
      "4372 - Loss_train: 0.08404506969215751, Loss_test: 0.08330250619291789\n",
      "4373 - Loss_train: 0.08404453484622529, Loss_test: 0.08330210998274475\n",
      "4374 - Loss_train: 0.08404400027815638, Loss_test: 0.08330171401061962\n",
      "4375 - Loss_train: 0.08404346598805262, Loss_test: 0.08330131828231702\n",
      "4376 - Loss_train: 0.08404293197630379, Loss_test: 0.08330092279375893\n",
      "4377 - Loss_train: 0.0840423982421135, Loss_test: 0.08330052755040863\n",
      "4378 - Loss_train: 0.084041864784575, Loss_test: 0.08330013254766114\n",
      "4379 - Loss_train: 0.08404133160466677, Loss_test: 0.08329973778519387\n",
      "4380 - Loss_train: 0.08404079870232548, Loss_test: 0.08329934326498105\n",
      "4381 - Loss_train: 0.0840402660768983, Loss_test: 0.08329894898545728\n",
      "4382 - Loss_train: 0.08403973372767934, Loss_test: 0.08329855494835961\n",
      "4383 - Loss_train: 0.08403920165603843, Loss_test: 0.08329816115087439\n",
      "4384 - Loss_train: 0.08403866986079805, Loss_test: 0.08329776759572387\n",
      "4385 - Loss_train: 0.08403813834193616, Loss_test: 0.08329737428110569\n",
      "4386 - Loss_train: 0.08403760709877693, Loss_test: 0.0832969812048883\n",
      "4387 - Loss_train: 0.0840370761317108, Loss_test: 0.08329658837283202\n",
      "4388 - Loss_train: 0.0840365454411697, Loss_test: 0.08329619577746339\n",
      "4389 - Loss_train: 0.08403601502688222, Loss_test: 0.08329580342596339\n",
      "4390 - Loss_train: 0.08403548488712755, Loss_test: 0.0832954113120657\n",
      "4391 - Loss_train: 0.08403495502386774, Loss_test: 0.08329501943810705\n",
      "4392 - Loss_train: 0.08403442543593291, Loss_test: 0.08329462780435493\n",
      "4393 - Loss_train: 0.08403389612272827, Loss_test: 0.08329423641063563\n",
      "4394 - Loss_train: 0.08403336708522466, Loss_test: 0.08329384525649787\n",
      "4395 - Loss_train: 0.08403283832213976, Loss_test: 0.08329345434173378\n",
      "4396 - Loss_train: 0.08403230983462293, Loss_test: 0.08329306366602282\n",
      "4397 - Loss_train: 0.08403178162106636, Loss_test: 0.08329267323057839\n",
      "4398 - Loss_train: 0.08403125368217058, Loss_test: 0.08329228303197331\n",
      "4399 - Loss_train: 0.08403072601758176, Loss_test: 0.08329189307430458\n",
      "4400 - Loss_train: 0.08403019862735074, Loss_test: 0.08329150335212004\n",
      "4401 - Loss_train: 0.0840296715109635, Loss_test: 0.08329111387330004\n",
      "4402 - Loss_train: 0.0840291446687313, Loss_test: 0.0832907246282117\n",
      "4403 - Loss_train: 0.08402861810104947, Loss_test: 0.08329033562477761\n",
      "4404 - Loss_train: 0.08402809180672544, Loss_test: 0.08328994685810084\n",
      "4405 - Loss_train: 0.08402756578561363, Loss_test: 0.08328955832961549\n",
      "4406 - Loss_train: 0.08402704003792422, Loss_test: 0.08328917004017496\n",
      "4407 - Loss_train: 0.0840265145636106, Loss_test: 0.08328878198664781\n",
      "4408 - Loss_train: 0.08402598936249826, Loss_test: 0.08328839417169397\n",
      "4409 - Loss_train: 0.0840254644342814, Loss_test: 0.08328800659523244\n",
      "4410 - Loss_train: 0.08402493977893226, Loss_test: 0.08328761925472271\n",
      "4411 - Loss_train: 0.08402441539685403, Loss_test: 0.08328723215201371\n",
      "4412 - Loss_train: 0.08402389128677576, Loss_test: 0.08328684528686277\n",
      "4413 - Loss_train: 0.0840233674491888, Loss_test: 0.0832864586578472\n",
      "4414 - Loss_train: 0.08402284388450544, Loss_test: 0.08328607226636393\n",
      "4415 - Loss_train: 0.0840223205919448, Loss_test: 0.0832856861156853\n",
      "4416 - Loss_train: 0.08402179757132114, Loss_test: 0.08328530019377997\n",
      "4417 - Loss_train: 0.08402127482208067, Loss_test: 0.08328491451261054\n",
      "4418 - Loss_train: 0.0840207523451166, Loss_test: 0.08328452906960598\n",
      "4419 - Loss_train: 0.08402023013929476, Loss_test: 0.08328414385848922\n",
      "4420 - Loss_train: 0.08401970820479278, Loss_test: 0.083283758887354\n",
      "4421 - Loss_train: 0.08401918654212644, Loss_test: 0.08328337414833648\n",
      "4422 - Loss_train: 0.08401866515087679, Loss_test: 0.083282989648421\n",
      "4423 - Loss_train: 0.08401814403015659, Loss_test: 0.08328260538213547\n",
      "4424 - Loss_train: 0.08401762317981827, Loss_test: 0.0832822213551586\n",
      "4425 - Loss_train: 0.08401710260002063, Loss_test: 0.0832818375584909\n",
      "4426 - Loss_train: 0.0840165822910259, Loss_test: 0.0832814540011798\n",
      "4427 - Loss_train: 0.08401606225335452, Loss_test: 0.08328107067568201\n",
      "4428 - Loss_train: 0.08401554248580759, Loss_test: 0.08328068758752598\n",
      "4429 - Loss_train: 0.08401502298809782, Loss_test: 0.0832803047329061\n",
      "4430 - Loss_train: 0.08401450376013683, Loss_test: 0.08327992211427471\n",
      "4431 - Loss_train: 0.084013984802796, Loss_test: 0.08327953972890857\n",
      "4432 - Loss_train: 0.08401346611456982, Loss_test: 0.08327915758038544\n",
      "4433 - Loss_train: 0.08401294769595799, Loss_test: 0.08327877566386471\n",
      "4434 - Loss_train: 0.08401242954684608, Loss_test: 0.08327839398202531\n",
      "4435 - Loss_train: 0.08401191166707195, Loss_test: 0.0832780125347393\n",
      "4436 - Loss_train: 0.08401139405657493, Loss_test: 0.0832776313218524\n",
      "4437 - Loss_train: 0.08401087671503973, Loss_test: 0.08327725034255772\n",
      "4438 - Loss_train: 0.08401035964251233, Loss_test: 0.08327686959574127\n",
      "4439 - Loss_train: 0.08400984283886659, Loss_test: 0.08327648908287746\n",
      "4440 - Loss_train: 0.08400932630386132, Loss_test: 0.08327610880534443\n",
      "4441 - Loss_train: 0.08400881003716629, Loss_test: 0.08327572875982178\n",
      "4442 - Loss_train: 0.08400829403958593, Loss_test: 0.08327534894772921\n",
      "4443 - Loss_train: 0.08400777830949491, Loss_test: 0.08327496936933332\n",
      "4444 - Loss_train: 0.0840072628482469, Loss_test: 0.08327459002372979\n",
      "4445 - Loss_train: 0.08400674765418314, Loss_test: 0.08327421090976483\n",
      "4446 - Loss_train: 0.08400623272791699, Loss_test: 0.08327383202870992\n",
      "4447 - Loss_train: 0.08400571806937492, Loss_test: 0.08327345338231312\n",
      "4448 - Loss_train: 0.08400520367820932, Loss_test: 0.08327307496538608\n",
      "4449 - Loss_train: 0.0840046895552317, Loss_test: 0.08327269678414977\n",
      "4450 - Loss_train: 0.08400417569909475, Loss_test: 0.08327231883098682\n",
      "4451 - Loss_train: 0.0840036621097825, Loss_test: 0.08327194111440092\n",
      "4452 - Loss_train: 0.08400314878738695, Loss_test: 0.08327156362542829\n",
      "4453 - Loss_train: 0.08400263573247331, Loss_test: 0.08327118637037272\n",
      "4454 - Loss_train: 0.08400212294422636, Loss_test: 0.08327080934720854\n",
      "4455 - Loss_train: 0.08400161042189368, Loss_test: 0.08327043255489025\n",
      "4456 - Loss_train: 0.08400109816624965, Loss_test: 0.08327005599393736\n",
      "4457 - Loss_train: 0.08400058617671319, Loss_test: 0.08326967966426473\n",
      "4458 - Loss_train: 0.08400007445359851, Loss_test: 0.08326930356638525\n",
      "4459 - Loss_train: 0.08399956299627573, Loss_test: 0.08326892769902536\n",
      "4460 - Loss_train: 0.08399905180487262, Loss_test: 0.08326855206250616\n",
      "4461 - Loss_train: 0.08399854087921156, Loss_test: 0.08326817665600922\n",
      "4462 - Loss_train: 0.0839980302190163, Loss_test: 0.08326780148182747\n",
      "4463 - Loss_train: 0.08399751982443425, Loss_test: 0.08326742653589338\n",
      "4464 - Loss_train: 0.08399700969559612, Loss_test: 0.08326705182117149\n",
      "4465 - Loss_train: 0.08399649983208757, Loss_test: 0.08326667734011067\n",
      "4466 - Loss_train: 0.0839959902330776, Loss_test: 0.08326630308441758\n",
      "4467 - Loss_train: 0.08399548089964189, Loss_test: 0.08326592906176157\n",
      "4468 - Loss_train: 0.08399497183032201, Loss_test: 0.08326555526751234\n",
      "4469 - Loss_train: 0.08399446302539036, Loss_test: 0.08326518170416253\n",
      "4470 - Loss_train: 0.08399395448492972, Loss_test: 0.08326480836672884\n",
      "4471 - Loss_train: 0.08399344620875691, Loss_test: 0.08326443526237788\n",
      "4472 - Loss_train: 0.08399293819688865, Loss_test: 0.083264062387687\n",
      "4473 - Loss_train: 0.08399243044897464, Loss_test: 0.08326368973947454\n",
      "4474 - Loss_train: 0.08399192296512037, Loss_test: 0.08326331732021172\n",
      "4475 - Loss_train: 0.0839914157455853, Loss_test: 0.08326294513475108\n",
      "4476 - Loss_train: 0.08399090878916654, Loss_test: 0.08326257317336422\n",
      "4477 - Loss_train: 0.08399040209627592, Loss_test: 0.08326220144202978\n",
      "4478 - Loss_train: 0.08398989566724335, Loss_test: 0.08326182993910282\n",
      "4479 - Loss_train: 0.08398938950102314, Loss_test: 0.08326145866616363\n",
      "4480 - Loss_train: 0.08398888359782329, Loss_test: 0.08326108761884336\n",
      "4481 - Loss_train: 0.08398837795770951, Loss_test: 0.08326071680047756\n",
      "4482 - Loss_train: 0.083987872580423, Loss_test: 0.0832603462114916\n",
      "4483 - Loss_train: 0.08398736746668375, Loss_test: 0.08325997584931004\n",
      "4484 - Loss_train: 0.08398686261491826, Loss_test: 0.083259605716594\n",
      "4485 - Loss_train: 0.08398635802532338, Loss_test: 0.08325923580795563\n",
      "4486 - Loss_train: 0.08398585369865341, Loss_test: 0.0832588661297362\n",
      "4487 - Loss_train: 0.08398534963358993, Loss_test: 0.0832584966791778\n",
      "4488 - Loss_train: 0.08398484583051515, Loss_test: 0.08325812745436903\n",
      "4489 - Loss_train: 0.08398434228932213, Loss_test: 0.08325775845816272\n",
      "4490 - Loss_train: 0.0839838390101085, Loss_test: 0.08325738968701266\n",
      "4491 - Loss_train: 0.08398333599233544, Loss_test: 0.0832570211450899\n",
      "4492 - Loss_train: 0.08398283323579557, Loss_test: 0.08325665282845698\n",
      "4493 - Loss_train: 0.08398233074064668, Loss_test: 0.08325628473842764\n",
      "4494 - Loss_train: 0.08398182850711981, Loss_test: 0.08325591687514661\n",
      "4495 - Loss_train: 0.08398132653501912, Loss_test: 0.08325554924018566\n",
      "4496 - Loss_train: 0.0839808248241139, Loss_test: 0.08325518182887637\n",
      "4497 - Loss_train: 0.08398032337307863, Loss_test: 0.08325481464327864\n",
      "4498 - Loss_train: 0.08397982218284365, Loss_test: 0.08325444768690037\n",
      "4499 - Loss_train: 0.08397932125290002, Loss_test: 0.08325408095459157\n",
      "4500 - Loss_train: 0.08397882058418384, Loss_test: 0.08325371444793554\n",
      "4501 - Loss_train: 0.08397832017514008, Loss_test: 0.08325334816622783\n",
      "4502 - Loss_train: 0.08397782002618608, Loss_test: 0.08325298211086078\n",
      "4503 - Loss_train: 0.08397732013739383, Loss_test: 0.08325261628072206\n",
      "4504 - Loss_train: 0.08397682050854208, Loss_test: 0.08325225067676477\n",
      "4505 - Loss_train: 0.08397632113973655, Loss_test: 0.0832518852969399\n",
      "4506 - Loss_train: 0.08397582202995757, Loss_test: 0.08325152014316134\n",
      "4507 - Loss_train: 0.08397532317957132, Loss_test: 0.08325115521346663\n",
      "4508 - Loss_train: 0.08397482458870581, Loss_test: 0.08325079050714389\n",
      "4509 - Loss_train: 0.08397432625678038, Loss_test: 0.08325042602742136\n",
      "4510 - Loss_train: 0.08397382818408557, Loss_test: 0.08325006177053687\n",
      "4511 - Loss_train: 0.08397333037032652, Loss_test: 0.08324969774001865\n",
      "4512 - Loss_train: 0.08397283281529253, Loss_test: 0.08324933393167043\n",
      "4513 - Loss_train: 0.08397233551895555, Loss_test: 0.08324897034858146\n",
      "4514 - Loss_train: 0.08397183848099107, Loss_test: 0.08324860698964028\n",
      "4515 - Loss_train: 0.0839713417020663, Loss_test: 0.08324824385457304\n",
      "4516 - Loss_train: 0.08397084518076676, Loss_test: 0.08324788094428694\n",
      "4517 - Loss_train: 0.08397034891855014, Loss_test: 0.08324751825609761\n",
      "4518 - Loss_train: 0.08396985291363188, Loss_test: 0.08324715579210774\n",
      "4519 - Loss_train: 0.08396935716643886, Loss_test: 0.0832467935508222\n",
      "4520 - Loss_train: 0.0839688616767175, Loss_test: 0.08324643153360425\n",
      "4521 - Loss_train: 0.08396836644527962, Loss_test: 0.08324606973818631\n",
      "4522 - Loss_train: 0.08396787147066227, Loss_test: 0.08324570816683421\n",
      "4523 - Loss_train: 0.08396737675335803, Loss_test: 0.08324534681787395\n",
      "4524 - Loss_train: 0.08396688229329509, Loss_test: 0.08324498569283388\n",
      "4525 - Loss_train: 0.08396638809020628, Loss_test: 0.08324462478905925\n",
      "4526 - Loss_train: 0.08396589414417549, Loss_test: 0.08324426410819422\n",
      "4527 - Loss_train: 0.083965400455324, Loss_test: 0.08324390364979807\n",
      "4528 - Loss_train: 0.08396490702261022, Loss_test: 0.08324354341253193\n",
      "4529 - Loss_train: 0.08396441384627949, Loss_test: 0.08324318339956958\n",
      "4530 - Loss_train: 0.08396392092696688, Loss_test: 0.08324282360771476\n",
      "4531 - Loss_train: 0.08396342826333626, Loss_test: 0.08324246403854191\n",
      "4532 - Loss_train: 0.08396293585638048, Loss_test: 0.08324210469029145\n",
      "4533 - Loss_train: 0.08396244370466718, Loss_test: 0.0832417455634052\n",
      "4534 - Loss_train: 0.08396195180898952, Loss_test: 0.08324138665753962\n",
      "4535 - Loss_train: 0.08396146016958161, Loss_test: 0.08324102797343441\n",
      "4536 - Loss_train: 0.0839609687851236, Loss_test: 0.08324066951275191\n",
      "4537 - Loss_train: 0.08396047765618617, Loss_test: 0.08324031127008062\n",
      "4538 - Loss_train: 0.08395998678256542, Loss_test: 0.08323995324874493\n",
      "4539 - Loss_train: 0.08395949616401079, Loss_test: 0.08323959544928874\n",
      "4540 - Loss_train: 0.08395900580058906, Loss_test: 0.08323923787007476\n",
      "4541 - Loss_train: 0.08395851569273582, Loss_test: 0.08323888051245658\n",
      "4542 - Loss_train: 0.08395802583977281, Loss_test: 0.08323852337488535\n",
      "4543 - Loss_train: 0.08395753624073506, Loss_test: 0.08323816645919961\n",
      "4544 - Loss_train: 0.08395704689631307, Loss_test: 0.0832378097611607\n",
      "4545 - Loss_train: 0.08395655780631422, Loss_test: 0.08323745328541968\n",
      "4546 - Loss_train: 0.08395606897045393, Loss_test: 0.08323709702766958\n",
      "4547 - Loss_train: 0.08395558038874167, Loss_test: 0.08323674099041062\n",
      "4548 - Loss_train: 0.08395509206109555, Loss_test: 0.08323638517213985\n",
      "4549 - Loss_train: 0.08395460398760056, Loss_test: 0.08323602957702048\n",
      "4550 - Loss_train: 0.08395411616746112, Loss_test: 0.0832356741984241\n",
      "4551 - Loss_train: 0.08395362860176704, Loss_test: 0.08323531904026873\n",
      "4552 - Loss_train: 0.08395314128858196, Loss_test: 0.08323496410169627\n",
      "4553 - Loss_train: 0.08395265422887922, Loss_test: 0.08323460938011486\n",
      "4554 - Loss_train: 0.08395216742280548, Loss_test: 0.0832342548818284\n",
      "4555 - Loss_train: 0.08395168086926412, Loss_test: 0.0832339005966089\n",
      "4556 - Loss_train: 0.08395119456908806, Loss_test: 0.08323354653687064\n",
      "4557 - Loss_train: 0.08395070852124002, Loss_test: 0.08323319268973689\n",
      "4558 - Loss_train: 0.08395022272662678, Loss_test: 0.08323283906475001\n",
      "4559 - Loss_train: 0.08394973718401982, Loss_test: 0.083232485656688\n",
      "4560 - Loss_train: 0.08394925189412218, Loss_test: 0.08323213246675241\n",
      "4561 - Loss_train: 0.08394876685646252, Loss_test: 0.08323177949651858\n",
      "4562 - Loss_train: 0.08394828207123292, Loss_test: 0.08323142674392103\n",
      "4563 - Loss_train: 0.08394779753731257, Loss_test: 0.08323107420776242\n",
      "4564 - Loss_train: 0.0839473132555342, Loss_test: 0.08323072189061079\n",
      "4565 - Loss_train: 0.08394682922604922, Loss_test: 0.0832303697908464\n",
      "4566 - Loss_train: 0.0839463454476396, Loss_test: 0.08323001791004656\n",
      "4567 - Loss_train: 0.08394586192139025, Loss_test: 0.08322966624556538\n",
      "4568 - Loss_train: 0.08394537864586572, Loss_test: 0.08322931479709855\n",
      "4569 - Loss_train: 0.08394489562211027, Loss_test: 0.08322896356775386\n",
      "4570 - Loss_train: 0.0839444128489711, Loss_test: 0.08322861255476575\n",
      "4571 - Loss_train: 0.08394393032674666, Loss_test: 0.08322826175880252\n",
      "4572 - Loss_train: 0.08394344805580696, Loss_test: 0.08322791117908494\n",
      "4573 - Loss_train: 0.08394296603503162, Loss_test: 0.0832275608179015\n",
      "4574 - Loss_train: 0.08394248426495908, Loss_test: 0.08322721067188787\n",
      "4575 - Loss_train: 0.08394200274577067, Loss_test: 0.08322686074362946\n",
      "4576 - Loss_train: 0.0839415214762462, Loss_test: 0.08322651102910802\n",
      "4577 - Loss_train: 0.08394104045706142, Loss_test: 0.08322616153259164\n",
      "4578 - Loss_train: 0.08394055968845865, Loss_test: 0.08322581225163386\n",
      "4579 - Loss_train: 0.08394007916964175, Loss_test: 0.08322546318756363\n",
      "4580 - Loss_train: 0.08393959890057749, Loss_test: 0.08322511433700229\n",
      "4581 - Loss_train: 0.08393911888091542, Loss_test: 0.083224765705449\n",
      "4582 - Loss_train: 0.08393863911161822, Loss_test: 0.0832244172876625\n",
      "4583 - Loss_train: 0.08393815959098715, Loss_test: 0.08322406908697473\n",
      "4584 - Loss_train: 0.08393768032010951, Loss_test: 0.08322372110040344\n",
      "4585 - Loss_train: 0.08393720129797531, Loss_test: 0.08322337332840717\n",
      "4586 - Loss_train: 0.08393672252478725, Loss_test: 0.08322302577360571\n",
      "4587 - Loss_train: 0.0839362440005002, Loss_test: 0.08322267843062864\n",
      "4588 - Loss_train: 0.08393576572474556, Loss_test: 0.08322233130588397\n",
      "4589 - Loss_train: 0.08393528769786213, Loss_test: 0.08322198439376825\n",
      "4590 - Loss_train: 0.08393480991951814, Loss_test: 0.0832216376966851\n",
      "4591 - Loss_train: 0.08393433238951103, Loss_test: 0.08322129121431661\n",
      "4592 - Loss_train: 0.08393385510772072, Loss_test: 0.0832209449481774\n",
      "4593 - Loss_train: 0.08393337807421701, Loss_test: 0.0832205988926002\n",
      "4594 - Loss_train: 0.08393290128859544, Loss_test: 0.08322025305466761\n",
      "4595 - Loss_train: 0.08393242475095761, Loss_test: 0.08321990742810326\n",
      "4596 - Loss_train: 0.08393194846137655, Loss_test: 0.08321956201781433\n",
      "4597 - Loss_train: 0.08393147241878043, Loss_test: 0.08321921681977958\n",
      "4598 - Loss_train: 0.08393099662477511, Loss_test: 0.08321887183661349\n",
      "4599 - Loss_train: 0.0839305210772394, Loss_test: 0.0832185270658217\n",
      "4600 - Loss_train: 0.08393004577677421, Loss_test: 0.08321818251064307\n",
      "4601 - Loss_train: 0.08392957072420247, Loss_test: 0.08321783816575125\n",
      "4602 - Loss_train: 0.08392909591830994, Loss_test: 0.08321749403714566\n",
      "4603 - Loss_train: 0.08392862135873336, Loss_test: 0.08321715011949475\n",
      "4604 - Loss_train: 0.08392814704612807, Loss_test: 0.08321680641595676\n",
      "4605 - Loss_train: 0.08392767298016947, Loss_test: 0.08321646292442784\n",
      "4606 - Loss_train: 0.08392719916106196, Loss_test: 0.08321611964670761\n",
      "4607 - Loss_train: 0.0839267255878384, Loss_test: 0.0832157765817133\n",
      "4608 - Loss_train: 0.08392625226128869, Loss_test: 0.08321543372822106\n",
      "4609 - Loss_train: 0.0839257791802508, Loss_test: 0.08321509108730256\n",
      "4610 - Loss_train: 0.0839253063449976, Loss_test: 0.08321474865793704\n",
      "4611 - Loss_train: 0.083924833756351, Loss_test: 0.08321440644173449\n",
      "4612 - Loss_train: 0.08392436141331047, Loss_test: 0.0832140644386233\n",
      "4613 - Loss_train: 0.08392388931581783, Loss_test: 0.0832137226456673\n",
      "4614 - Loss_train: 0.08392341746308901, Loss_test: 0.08321338106567273\n",
      "4615 - Loss_train: 0.08392294585591538, Loss_test: 0.08321303969550592\n",
      "4616 - Loss_train: 0.08392247449369596, Loss_test: 0.08321269853922338\n",
      "4617 - Loss_train: 0.08392200337708844, Loss_test: 0.08321235759011336\n",
      "4618 - Loss_train: 0.08392153250472449, Loss_test: 0.08321201685954022\n",
      "4619 - Loss_train: 0.08392106187787836, Loss_test: 0.08321167633526201\n",
      "4620 - Loss_train: 0.08392059149534166, Loss_test: 0.08321133602072828\n",
      "4621 - Loss_train: 0.08392012135761169, Loss_test: 0.0832109959218908\n",
      "4622 - Loss_train: 0.08391965146371233, Loss_test: 0.08321065603033087\n",
      "4623 - Loss_train: 0.08391918181423223, Loss_test: 0.08321031634963434\n",
      "4624 - Loss_train: 0.08391871240899614, Loss_test: 0.08320997687945333\n",
      "4625 - Loss_train: 0.08391824324754434, Loss_test: 0.08320963762176489\n",
      "4626 - Loss_train: 0.0839177743308366, Loss_test: 0.0832092985742841\n",
      "4627 - Loss_train: 0.08391730565718701, Loss_test: 0.08320895973616108\n",
      "4628 - Loss_train: 0.08391683722706585, Loss_test: 0.08320862110659578\n",
      "4629 - Loss_train: 0.08391636904073478, Loss_test: 0.08320828268968017\n",
      "4630 - Loss_train: 0.08391590109775225, Loss_test: 0.0832079444790181\n",
      "4631 - Loss_train: 0.08391543339802439, Loss_test: 0.08320760648250257\n",
      "4632 - Loss_train: 0.08391496594160502, Loss_test: 0.08320726869258076\n",
      "4633 - Loss_train: 0.08391449872873329, Loss_test: 0.08320693111539115\n",
      "4634 - Loss_train: 0.08391403175807097, Loss_test: 0.08320659374584499\n",
      "4635 - Loss_train: 0.083913565030171, Loss_test: 0.08320625658407271\n",
      "4636 - Loss_train: 0.08391309854474094, Loss_test: 0.0832059196333305\n",
      "4637 - Loss_train: 0.08391263230233978, Loss_test: 0.08320558289089842\n",
      "4638 - Loss_train: 0.08391216630198887, Loss_test: 0.08320524635909586\n",
      "4639 - Loss_train: 0.0839117005437094, Loss_test: 0.08320491003250025\n",
      "4640 - Loss_train: 0.08391123502829517, Loss_test: 0.08320457391904429\n",
      "4641 - Loss_train: 0.08391076975423122, Loss_test: 0.08320423801101531\n",
      "4642 - Loss_train: 0.08391030472262472, Loss_test: 0.08320390231417836\n",
      "4643 - Loss_train: 0.08390983993286845, Loss_test: 0.08320356682247479\n",
      "4644 - Loss_train: 0.08390937538413266, Loss_test: 0.0832032315405379\n",
      "4645 - Loss_train: 0.08390891107701745, Loss_test: 0.08320289646776226\n",
      "4646 - Loss_train: 0.0839084470112623, Loss_test: 0.08320256160059189\n",
      "4647 - Loss_train: 0.08390798318707043, Loss_test: 0.08320222694420076\n",
      "4648 - Loss_train: 0.08390751960359714, Loss_test: 0.08320189249314826\n",
      "4649 - Loss_train: 0.08390705626135404, Loss_test: 0.08320155825035595\n",
      "4650 - Loss_train: 0.0839065931605428, Loss_test: 0.0832012242171039\n",
      "4651 - Loss_train: 0.0839061302999685, Loss_test: 0.08320089038782465\n",
      "4652 - Loss_train: 0.08390566768060698, Loss_test: 0.08320055676890029\n",
      "4653 - Loss_train: 0.08390520530203183, Loss_test: 0.08320022335661197\n",
      "4654 - Loss_train: 0.08390474316347146, Loss_test: 0.08319989015050933\n",
      "4655 - Loss_train: 0.08390428126457335, Loss_test: 0.08319955715264789\n",
      "4656 - Loss_train: 0.0839038196058924, Loss_test: 0.08319922436053932\n",
      "4657 - Loss_train: 0.08390335818759655, Loss_test: 0.0831988917771577\n",
      "4658 - Loss_train: 0.08390289700895383, Loss_test: 0.08319855939745222\n",
      "4659 - Loss_train: 0.08390243606996857, Loss_test: 0.08319822722578621\n",
      "4660 - Loss_train: 0.08390197537100198, Loss_test: 0.0831978952595386\n",
      "4661 - Loss_train: 0.083901514911071, Loss_test: 0.08319756350066496\n",
      "4662 - Loss_train: 0.08390105469074974, Loss_test: 0.08319723194813626\n",
      "4663 - Loss_train: 0.0839005947101374, Loss_test: 0.0831969006003159\n",
      "4664 - Loss_train: 0.08390013496805361, Loss_test: 0.08319656945850215\n",
      "4665 - Loss_train: 0.08389967546487971, Loss_test: 0.08319623852431624\n",
      "4666 - Loss_train: 0.08389921620071741, Loss_test: 0.08319590779433829\n",
      "4667 - Loss_train: 0.0838987571759426, Loss_test: 0.08319557726912932\n",
      "4668 - Loss_train: 0.08389829838923611, Loss_test: 0.08319524695155922\n",
      "4669 - Loss_train: 0.08389783984093795, Loss_test: 0.08319491683777894\n",
      "4670 - Loss_train: 0.0838973815317164, Loss_test: 0.0831945869299687\n",
      "4671 - Loss_train: 0.08389692346015486, Loss_test: 0.08319425722652897\n",
      "4672 - Loss_train: 0.08389646562659255, Loss_test: 0.08319392772946524\n",
      "4673 - Loss_train: 0.08389600803096584, Loss_test: 0.0831935984362171\n",
      "4674 - Loss_train: 0.08389555067351746, Loss_test: 0.08319326934789442\n",
      "4675 - Loss_train: 0.08389509355414829, Loss_test: 0.08319294046417028\n",
      "4676 - Loss_train: 0.0838946366725745, Loss_test: 0.08319261178592903\n",
      "4677 - Loss_train: 0.08389418002792579, Loss_test: 0.08319228331062632\n",
      "4678 - Loss_train: 0.08389372362104486, Loss_test: 0.0831919550408013\n",
      "4679 - Loss_train: 0.08389326745154312, Loss_test: 0.08319162697488915\n",
      "4680 - Loss_train: 0.08389281151966128, Loss_test: 0.0831912991134708\n",
      "4681 - Loss_train: 0.08389235582410512, Loss_test: 0.08319097145429559\n",
      "4682 - Loss_train: 0.08389190036545519, Loss_test: 0.08319064399887512\n",
      "4683 - Loss_train: 0.0838914451440079, Loss_test: 0.08319031675198922\n",
      "4684 - Loss_train: 0.08389099015951747, Loss_test: 0.08318998970165344\n",
      "4685 - Loss_train: 0.08389053541150858, Loss_test: 0.08318966286206997\n",
      "4686 - Loss_train: 0.08389008089919693, Loss_test: 0.0831893362211525\n",
      "4687 - Loss_train: 0.08388962662303445, Loss_test: 0.08318900978336999\n",
      "4688 - Loss_train: 0.08388917258371319, Loss_test: 0.0831886835513696\n",
      "4689 - Loss_train: 0.08388871878001174, Loss_test: 0.08318835751958734\n",
      "4690 - Loss_train: 0.08388826521240192, Loss_test: 0.08318803169333142\n",
      "4691 - Loss_train: 0.08388781188050164, Loss_test: 0.08318770606766338\n",
      "4692 - Loss_train: 0.08388735878424358, Loss_test: 0.08318738064773999\n",
      "4693 - Loss_train: 0.08388690592359242, Loss_test: 0.08318705542670603\n",
      "4694 - Loss_train: 0.08388645329835358, Loss_test: 0.08318673041080578\n",
      "4695 - Loss_train: 0.08388600090828295, Loss_test: 0.08318640559583189\n",
      "4696 - Loss_train: 0.08388554875363742, Loss_test: 0.08318608098325356\n",
      "4697 - Loss_train: 0.08388509683395816, Loss_test: 0.08318575657419254\n",
      "4698 - Loss_train: 0.08388464514938496, Loss_test: 0.08318543236367065\n",
      "4699 - Loss_train: 0.0838841937000424, Loss_test: 0.08318510836032485\n",
      "4700 - Loss_train: 0.08388374248547004, Loss_test: 0.08318478455599088\n",
      "4701 - Loss_train: 0.08388329150503915, Loss_test: 0.08318446095209066\n",
      "4702 - Loss_train: 0.0838828407589421, Loss_test: 0.0831841375538266\n",
      "4703 - Loss_train: 0.0838823902471834, Loss_test: 0.08318381435211701\n",
      "4704 - Loss_train: 0.0838819399701639, Loss_test: 0.08318349135500812\n",
      "4705 - Loss_train: 0.08388148992683925, Loss_test: 0.08318316855615132\n",
      "4706 - Loss_train: 0.08388104011744164, Loss_test: 0.08318284596195692\n",
      "4707 - Loss_train: 0.08388059054190021, Loss_test: 0.08318252356639766\n",
      "4708 - Loss_train: 0.08388014120020794, Loss_test: 0.08318220137284749\n",
      "4709 - Loss_train: 0.08387969209260529, Loss_test: 0.08318187937961861\n",
      "4710 - Loss_train: 0.08387924321785901, Loss_test: 0.08318155758663154\n",
      "4711 - Loss_train: 0.08387879457644781, Loss_test: 0.08318123599586086\n",
      "4712 - Loss_train: 0.08387834616845322, Loss_test: 0.0831809146036434\n",
      "4713 - Loss_train: 0.08387789799354496, Loss_test: 0.08318059341430155\n",
      "4714 - Loss_train: 0.08387745005162146, Loss_test: 0.0831802724198212\n",
      "4715 - Loss_train: 0.08387700234255684, Loss_test: 0.08317995163262913\n",
      "4716 - Loss_train: 0.08387655486623566, Loss_test: 0.0831796310398069\n",
      "4717 - Loss_train: 0.08387610762322313, Loss_test: 0.08317931064961842\n",
      "4718 - Loss_train: 0.0838756606121545, Loss_test: 0.08317899045966712\n",
      "4719 - Loss_train: 0.08387521383349988, Loss_test: 0.0831786704666428\n",
      "4720 - Loss_train: 0.08387476728713428, Loss_test: 0.08317835067548993\n",
      "4721 - Loss_train: 0.08387432097333722, Loss_test: 0.08317803108329055\n",
      "4722 - Loss_train: 0.0838738748913313, Loss_test: 0.08317771168898853\n",
      "4723 - Loss_train: 0.08387342904109753, Loss_test: 0.08317739249527728\n",
      "4724 - Loss_train: 0.08387298342305369, Loss_test: 0.08317707350337745\n",
      "4725 - Loss_train: 0.08387253803624119, Loss_test: 0.08317675470551786\n",
      "4726 - Loss_train: 0.08387209288155897, Loss_test: 0.08317643610705235\n",
      "4727 - Loss_train: 0.08387164795831123, Loss_test: 0.08317611771066896\n",
      "4728 - Loss_train: 0.0838712032660491, Loss_test: 0.08317579951023686\n",
      "4729 - Loss_train: 0.08387075880488863, Loss_test: 0.08317548150959647\n",
      "4730 - Loss_train: 0.08387031457538073, Loss_test: 0.08317516370816362\n",
      "4731 - Loss_train: 0.08386987057668091, Loss_test: 0.08317484610398865\n",
      "4732 - Loss_train: 0.08386942680825311, Loss_test: 0.08317452869863876\n",
      "4733 - Loss_train: 0.08386898327115254, Loss_test: 0.08317421148954768\n",
      "4734 - Loss_train: 0.08386853996461513, Loss_test: 0.08317389448120012\n",
      "4735 - Loss_train: 0.08386809688859669, Loss_test: 0.08317357766839142\n",
      "4736 - Loss_train: 0.08386765404295451, Loss_test: 0.08317326105336069\n",
      "4737 - Loss_train: 0.08386721142774424, Loss_test: 0.0831729446375441\n",
      "4738 - Loss_train: 0.08386676904188578, Loss_test: 0.08317262841891389\n",
      "4739 - Loss_train: 0.08386632688606481, Loss_test: 0.08317231239587827\n",
      "4740 - Loss_train: 0.08386588496021143, Loss_test: 0.08317199657262477\n",
      "4741 - Loss_train: 0.08386544326410088, Loss_test: 0.08317168094536644\n",
      "4742 - Loss_train: 0.08386500179751488, Loss_test: 0.08317136551477468\n",
      "4743 - Loss_train: 0.08386456056061128, Loss_test: 0.08317105028129304\n",
      "4744 - Loss_train: 0.08386411955295923, Loss_test: 0.08317073524452925\n",
      "4745 - Loss_train: 0.08386367877532644, Loss_test: 0.08317042040595476\n",
      "4746 - Loss_train: 0.08386323822687825, Loss_test: 0.08317010576253217\n",
      "4747 - Loss_train: 0.08386279790689638, Loss_test: 0.08316979131752732\n",
      "4748 - Loss_train: 0.08386235781631367, Loss_test: 0.08316947706610975\n",
      "4749 - Loss_train: 0.0838619179539324, Loss_test: 0.08316916301247543\n",
      "4750 - Loss_train: 0.08386147832058591, Loss_test: 0.08316884915605613\n",
      "4751 - Loss_train: 0.08386103891615151, Loss_test: 0.0831685354957431\n",
      "4752 - Loss_train: 0.0838605997398003, Loss_test: 0.08316822202942246\n",
      "4753 - Loss_train: 0.08386016079169019, Loss_test: 0.08316790876090925\n",
      "4754 - Loss_train: 0.08385972207182024, Loss_test: 0.08316759568708801\n",
      "4755 - Loss_train: 0.0838592835800194, Loss_test: 0.0831672828085038\n",
      "4756 - Loss_train: 0.08385884531609959, Loss_test: 0.08316697012607976\n",
      "4757 - Loss_train: 0.08385840728029095, Loss_test: 0.0831666576398905\n",
      "4758 - Loss_train: 0.08385796947277267, Loss_test: 0.08316634534809295\n",
      "4759 - Loss_train: 0.08385753189197222, Loss_test: 0.08316603325246179\n",
      "4760 - Loss_train: 0.08385709453942393, Loss_test: 0.08316572134947799\n",
      "4761 - Loss_train: 0.08385665741379142, Loss_test: 0.08316540964303508\n",
      "4762 - Loss_train: 0.08385622051531032, Loss_test: 0.08316509813256957\n",
      "4763 - Loss_train: 0.0838557838440268, Loss_test: 0.08316478681583803\n",
      "4764 - Loss_train: 0.08385534739998259, Loss_test: 0.08316447569350806\n",
      "4765 - Loss_train: 0.08385491118262048, Loss_test: 0.08316416476653851\n",
      "4766 - Loss_train: 0.083854475192089, Loss_test: 0.08316385403298894\n",
      "4767 - Loss_train: 0.08385403942828308, Loss_test: 0.08316354349418664\n",
      "4768 - Loss_train: 0.08385360389106677, Loss_test: 0.0831632331498286\n",
      "4769 - Loss_train: 0.08385316858026423, Loss_test: 0.08316292299983837\n",
      "4770 - Loss_train: 0.08385273349576569, Loss_test: 0.08316261304301865\n",
      "4771 - Loss_train: 0.0838522986374822, Loss_test: 0.08316230328005499\n",
      "4772 - Loss_train: 0.0838518640059522, Loss_test: 0.08316199371083512\n",
      "4773 - Loss_train: 0.08385142959970858, Loss_test: 0.0831616843375501\n",
      "4774 - Loss_train: 0.0838509954198958, Loss_test: 0.08316137515443087\n",
      "4775 - Loss_train: 0.08385056146526312, Loss_test: 0.08316106616793835\n",
      "4776 - Loss_train: 0.08385012773620415, Loss_test: 0.08316075737278797\n",
      "4777 - Loss_train: 0.08384969423274378, Loss_test: 0.0831604487706438\n",
      "4778 - Loss_train: 0.08384926095465983, Loss_test: 0.08316014036457388\n",
      "4779 - Loss_train: 0.0838488279019577, Loss_test: 0.08315983214696407\n",
      "4780 - Loss_train: 0.08384839507488644, Loss_test: 0.08315952412663753\n",
      "4781 - Loss_train: 0.08384796247235972, Loss_test: 0.08315921629569967\n",
      "4782 - Loss_train: 0.08384753009469725, Loss_test: 0.08315890866015993\n",
      "4783 - Loss_train: 0.08384709794218341, Loss_test: 0.08315860121469994\n",
      "4784 - Loss_train: 0.08384666601452632, Loss_test: 0.08315829396303355\n",
      "4785 - Loss_train: 0.08384623431145917, Loss_test: 0.08315798690496158\n",
      "4786 - Loss_train: 0.08384580283250263, Loss_test: 0.08315768003739665\n",
      "4787 - Loss_train: 0.083845371577647, Loss_test: 0.08315737336456516\n",
      "4788 - Loss_train: 0.0838449405476312, Loss_test: 0.0831570668805739\n",
      "4789 - Loss_train: 0.08384450974118322, Loss_test: 0.0831567605898492\n",
      "4790 - Loss_train: 0.08384407915890499, Loss_test: 0.08315645448950051\n",
      "4791 - Loss_train: 0.08384364880036273, Loss_test: 0.08315614858262636\n",
      "4792 - Loss_train: 0.08384321866568027, Loss_test: 0.08315584286646874\n",
      "4793 - Loss_train: 0.08384278875518898, Loss_test: 0.08315553734452126\n",
      "4794 - Loss_train: 0.08384235906762508, Loss_test: 0.08315523201115924\n",
      "4795 - Loss_train: 0.08384192960350137, Loss_test: 0.08315492686853881\n",
      "4796 - Loss_train: 0.08384150036274242, Loss_test: 0.08315462191928638\n",
      "4797 - Loss_train: 0.0838410713457885, Loss_test: 0.08315431716071775\n",
      "4798 - Loss_train: 0.08384064255120884, Loss_test: 0.08315401259114355\n",
      "4799 - Loss_train: 0.08384021397953449, Loss_test: 0.08315370821463396\n",
      "4800 - Loss_train: 0.08383978563069322, Loss_test: 0.0831534040273397\n",
      "4801 - Loss_train: 0.08383935750508094, Loss_test: 0.08315310003180178\n",
      "4802 - Loss_train: 0.08383892960176115, Loss_test: 0.08315279622715657\n",
      "4803 - Loss_train: 0.08383850192078067, Loss_test: 0.08315249261091569\n",
      "4804 - Loss_train: 0.08383807446212979, Loss_test: 0.08315218918785691\n",
      "4805 - Loss_train: 0.08383764722571217, Loss_test: 0.08315188595290819\n",
      "4806 - Loss_train: 0.08383722021146742, Loss_test: 0.08315158290944843\n",
      "4807 - Loss_train: 0.08383679341981881, Loss_test: 0.08315128005247431\n",
      "4808 - Loss_train: 0.08383636685001282, Loss_test: 0.08315097739075027\n",
      "4809 - Loss_train: 0.0838359405023607, Loss_test: 0.08315067491457101\n",
      "4810 - Loss_train: 0.08383551437525036, Loss_test: 0.08315037263263479\n",
      "4811 - Loss_train: 0.08383508846974519, Loss_test: 0.08315007053572926\n",
      "4812 - Loss_train: 0.08383466278611076, Loss_test: 0.08314976862941695\n",
      "4813 - Loss_train: 0.08383423732371534, Loss_test: 0.08314946691501718\n",
      "4814 - Loss_train: 0.08383381208262618, Loss_test: 0.08314916538559057\n",
      "4815 - Loss_train: 0.08383338706252405, Loss_test: 0.08314886404853813\n",
      "4816 - Loss_train: 0.08383296226331563, Loss_test: 0.08314856289875389\n",
      "4817 - Loss_train: 0.08383253768520506, Loss_test: 0.08314826193811052\n",
      "4818 - Loss_train: 0.08383211332813807, Loss_test: 0.08314796116682002\n",
      "4819 - Loss_train: 0.08383168919174534, Loss_test: 0.08314766058433168\n",
      "4820 - Loss_train: 0.08383126527536355, Loss_test: 0.0831473601908759\n",
      "4821 - Loss_train: 0.08383084157941621, Loss_test: 0.08314705998593909\n",
      "4822 - Loss_train: 0.08383041810429959, Loss_test: 0.08314675996912396\n",
      "4823 - Loss_train: 0.08382999484901293, Loss_test: 0.08314646013918497\n",
      "4824 - Loss_train: 0.08382957181426302, Loss_test: 0.08314616049984377\n",
      "4825 - Loss_train: 0.08382914899875307, Loss_test: 0.08314586104711373\n",
      "4826 - Loss_train: 0.08382872640333729, Loss_test: 0.08314556178388155\n",
      "4827 - Loss_train: 0.08382830402749746, Loss_test: 0.0831452627062008\n",
      "4828 - Loss_train: 0.08382788187116781, Loss_test: 0.08314496381792726\n",
      "4829 - Loss_train: 0.08382745993502248, Loss_test: 0.08314466511699123\n",
      "4830 - Loss_train: 0.08382703821771272, Loss_test: 0.0831443666021399\n",
      "4831 - Loss_train: 0.08382661672011354, Loss_test: 0.08314406827754721\n",
      "4832 - Loss_train: 0.08382619544132945, Loss_test: 0.08314377013816483\n",
      "4833 - Loss_train: 0.0838257743813853, Loss_test: 0.08314347218711467\n",
      "4834 - Loss_train: 0.08382535354045387, Loss_test: 0.08314317442109166\n",
      "4835 - Loss_train: 0.08382493291876794, Loss_test: 0.08314287684508874\n",
      "4836 - Loss_train: 0.08382451251527434, Loss_test: 0.0831425794559339\n",
      "4837 - Loss_train: 0.08382409233033113, Loss_test: 0.0831422822508411\n",
      "4838 - Loss_train: 0.0838236723639563, Loss_test: 0.08314198523567595\n",
      "4839 - Loss_train: 0.08382325261594079, Loss_test: 0.08314168840274527\n",
      "4840 - Loss_train: 0.08382283308607245, Loss_test: 0.08314139175987602\n",
      "4841 - Loss_train: 0.08382241377516803, Loss_test: 0.08314109530403227\n",
      "4842 - Loss_train: 0.08382199468170617, Loss_test: 0.08314079902924264\n",
      "4843 - Loss_train: 0.0838215758065208, Loss_test: 0.08314050294718017\n",
      "4844 - Loss_train: 0.08382115714942807, Loss_test: 0.08314020704817641\n",
      "4845 - Loss_train: 0.08382073870926222, Loss_test: 0.08313991133505302\n",
      "4846 - Loss_train: 0.08382032048667609, Loss_test: 0.08313961580805718\n",
      "4847 - Loss_train: 0.08381990248147421, Loss_test: 0.08313932046510464\n",
      "4848 - Loss_train: 0.08381948469367255, Loss_test: 0.08313902531196285\n",
      "4849 - Loss_train: 0.08381906712304803, Loss_test: 0.08313873034001534\n",
      "4850 - Loss_train: 0.08381864976957905, Loss_test: 0.0831384355566288\n",
      "4851 - Loss_train: 0.08381823263317291, Loss_test: 0.08313814095654379\n",
      "4852 - Loss_train: 0.08381781571358925, Loss_test: 0.0831378465425884\n",
      "4853 - Loss_train: 0.08381739901111726, Loss_test: 0.08313755231293057\n",
      "4854 - Loss_train: 0.08381698252567864, Loss_test: 0.08313725826954098\n",
      "4855 - Loss_train: 0.08381656625625064, Loss_test: 0.08313696441137061\n",
      "4856 - Loss_train: 0.08381615020322598, Loss_test: 0.08313667073687175\n",
      "4857 - Loss_train: 0.08381573436686456, Loss_test: 0.08313637724714651\n",
      "4858 - Loss_train: 0.08381531874697497, Loss_test: 0.08313608394415134\n",
      "4859 - Loss_train: 0.08381490334261489, Loss_test: 0.08313579082257863\n",
      "4860 - Loss_train: 0.08381448815461709, Loss_test: 0.0831354978886149\n",
      "4861 - Loss_train: 0.08381407318205977, Loss_test: 0.08313520513752794\n",
      "4862 - Loss_train: 0.0838136584253492, Loss_test: 0.08313491256851757\n",
      "4863 - Loss_train: 0.08381324388468722, Loss_test: 0.08313462018450667\n",
      "4864 - Loss_train: 0.08381282955912872, Loss_test: 0.08313432798746237\n",
      "4865 - Loss_train: 0.08381241544899062, Loss_test: 0.0831340359716615\n",
      "4866 - Loss_train: 0.08381200155442821, Loss_test: 0.08313374413957719\n",
      "4867 - Loss_train: 0.08381158787495327, Loss_test: 0.08313345249281494\n",
      "4868 - Loss_train: 0.0838111744109651, Loss_test: 0.08313316102828387\n",
      "4869 - Loss_train: 0.08381076116157381, Loss_test: 0.08313286974697627\n",
      "4870 - Loss_train: 0.0838103481276735, Loss_test: 0.08313257865013432\n",
      "4871 - Loss_train: 0.08380993530783068, Loss_test: 0.08313228773622862\n",
      "4872 - Loss_train: 0.08380952270265282, Loss_test: 0.0831319970041486\n",
      "4873 - Loss_train: 0.08380911031256957, Loss_test: 0.08313170645823374\n",
      "4874 - Loss_train: 0.08380869813697402, Loss_test: 0.0831314160916721\n",
      "4875 - Loss_train: 0.08380828617520275, Loss_test: 0.0831311259096312\n",
      "4876 - Loss_train: 0.08380787442810836, Loss_test: 0.08313083591060767\n",
      "4877 - Loss_train: 0.0838074628951129, Loss_test: 0.08313054609516894\n",
      "4878 - Loss_train: 0.08380705157558765, Loss_test: 0.08313025646078345\n",
      "4879 - Loss_train: 0.08380664047024876, Loss_test: 0.08312996700917354\n",
      "4880 - Loss_train: 0.08380622957884311, Loss_test: 0.08312967774146134\n",
      "4881 - Loss_train: 0.08380581890126505, Loss_test: 0.0831293886530482\n",
      "4882 - Loss_train: 0.08380540843658907, Loss_test: 0.0831290997472455\n",
      "4883 - Loss_train: 0.08380499818535138, Loss_test: 0.08312881102680501\n",
      "4884 - Loss_train: 0.08380458814739207, Loss_test: 0.08312852248417264\n",
      "4885 - Loss_train: 0.083804178322752, Loss_test: 0.08312823412442304\n",
      "4886 - Loss_train: 0.08380376871112373, Loss_test: 0.08312794594650702\n",
      "4887 - Loss_train: 0.08380335931247003, Loss_test: 0.08312765795243358\n",
      "4888 - Loss_train: 0.08380295012675641, Loss_test: 0.08312737013718464\n",
      "4889 - Loss_train: 0.08380254115354052, Loss_test: 0.08312708250411403\n",
      "4890 - Loss_train: 0.08380213239313368, Loss_test: 0.08312679505141551\n",
      "4891 - Loss_train: 0.08380172384579358, Loss_test: 0.0831265077816168\n",
      "4892 - Loss_train: 0.08380131551088932, Loss_test: 0.0831262206936782\n",
      "4893 - Loss_train: 0.08380090738784206, Loss_test: 0.0831259337855327\n",
      "4894 - Loss_train: 0.08380049947690454, Loss_test: 0.08312564705716972\n",
      "4895 - Loss_train: 0.08380009177810949, Loss_test: 0.08312536050940617\n",
      "4896 - Loss_train: 0.08379968429221196, Loss_test: 0.08312507414282283\n",
      "4897 - Loss_train: 0.0837992770174662, Loss_test: 0.08312478795999519\n",
      "4898 - Loss_train: 0.08379886995504529, Loss_test: 0.08312450195448698\n",
      "4899 - Loss_train: 0.08379846310348417, Loss_test: 0.08312421613108954\n",
      "4900 - Loss_train: 0.08379805646371194, Loss_test: 0.08312393048542151\n",
      "4901 - Loss_train: 0.08379765003528393, Loss_test: 0.08312364502202566\n",
      "4902 - Loss_train: 0.08379724381805104, Loss_test: 0.08312335973750384\n",
      "4903 - Loss_train: 0.08379683781215393, Loss_test: 0.08312307463374398\n",
      "4904 - Loss_train: 0.08379643201736878, Loss_test: 0.08312278970831839\n",
      "4905 - Loss_train: 0.0837960264338918, Loss_test: 0.08312250496682587\n",
      "4906 - Loss_train: 0.08379562106104892, Loss_test: 0.08312222040019683\n",
      "4907 - Loss_train: 0.08379521589906198, Loss_test: 0.08312193601589801\n",
      "4908 - Loss_train: 0.08379481094767256, Loss_test: 0.08312165181076679\n",
      "4909 - Loss_train: 0.08379440620695297, Loss_test: 0.08312136778410645\n",
      "4910 - Loss_train: 0.08379400167725055, Loss_test: 0.08312108393650962\n",
      "4911 - Loss_train: 0.08379359735740795, Loss_test: 0.08312080026775291\n",
      "4912 - Loss_train: 0.08379319324759588, Loss_test: 0.08312051678113595\n",
      "4913 - Loss_train: 0.08379278934806637, Loss_test: 0.08312023347088376\n",
      "4914 - Loss_train: 0.08379238565852105, Loss_test: 0.08311995034125176\n",
      "4915 - Loss_train: 0.08379198217937019, Loss_test: 0.08311966738824794\n",
      "4916 - Loss_train: 0.08379157890947478, Loss_test: 0.08311938461539332\n",
      "4917 - Loss_train: 0.0837911758492115, Loss_test: 0.08311910202083067\n",
      "4918 - Loss_train: 0.08379077299906974, Loss_test: 0.0831188196032805\n",
      "4919 - Loss_train: 0.08379037035806518, Loss_test: 0.08311853736860046\n",
      "4920 - Loss_train: 0.08378996792685046, Loss_test: 0.08311825530656351\n",
      "4921 - Loss_train: 0.08378956570499957, Loss_test: 0.08311797342775035\n",
      "4922 - Loss_train: 0.08378916369246685, Loss_test: 0.08311769172322311\n",
      "4923 - Loss_train: 0.08378876188818479, Loss_test: 0.08311741019821717\n",
      "4924 - Loss_train: 0.083788360292641, Loss_test: 0.08311712884893308\n",
      "4925 - Loss_train: 0.0837879589061585, Loss_test: 0.08311684767992326\n",
      "4926 - Loss_train: 0.08378755772826223, Loss_test: 0.08311656668543253\n",
      "4927 - Loss_train: 0.08378715675956196, Loss_test: 0.0831162858739228\n",
      "4928 - Loss_train: 0.08378675599884748, Loss_test: 0.08311600523654218\n",
      "4929 - Loss_train: 0.08378635544655288, Loss_test: 0.08311572477603808\n",
      "4930 - Loss_train: 0.08378595510268222, Loss_test: 0.08311544449333641\n",
      "4931 - Loss_train: 0.08378555496678587, Loss_test: 0.08311516438729218\n",
      "4932 - Loss_train: 0.08378515503960593, Loss_test: 0.08311488445978452\n",
      "4933 - Loss_train: 0.08378475531958458, Loss_test: 0.08311460470808622\n",
      "4934 - Loss_train: 0.08378435580769673, Loss_test: 0.08311432513353159\n",
      "4935 - Loss_train: 0.08378395650409672, Loss_test: 0.08311404573530581\n",
      "4936 - Loss_train: 0.08378355740791701, Loss_test: 0.08311376651610415\n",
      "4937 - Loss_train: 0.08378315851927776, Loss_test: 0.08311348746889316\n",
      "4938 - Loss_train: 0.0837827598376779, Loss_test: 0.08311320860120976\n",
      "4939 - Loss_train: 0.08378236136342158, Loss_test: 0.0831129299105277\n",
      "4940 - Loss_train: 0.08378196309624929, Loss_test: 0.0831126513957606\n",
      "4941 - Loss_train: 0.08378156503605351, Loss_test: 0.08311237305384665\n",
      "4942 - Loss_train: 0.08378116718311074, Loss_test: 0.08311209489104558\n",
      "4943 - Loss_train: 0.08378076953658506, Loss_test: 0.0831118169056776\n",
      "4944 - Loss_train: 0.08378037209716307, Loss_test: 0.08311153909077952\n",
      "4945 - Loss_train: 0.08377997486504114, Loss_test: 0.0831112614564838\n",
      "4946 - Loss_train: 0.08377957783881604, Loss_test: 0.08311098399797756\n",
      "4947 - Loss_train: 0.08377918101906966, Loss_test: 0.08311070671163213\n",
      "4948 - Loss_train: 0.08377878440583879, Loss_test: 0.08311042960262996\n",
      "4949 - Loss_train: 0.08377838799879739, Loss_test: 0.08311015266940942\n",
      "4950 - Loss_train: 0.0837779917983198, Loss_test: 0.08310987591129362\n",
      "4951 - Loss_train: 0.08377759580324154, Loss_test: 0.08310959932699193\n",
      "4952 - Loss_train: 0.08377720001431058, Loss_test: 0.08310932291988593\n",
      "4953 - Loss_train: 0.08377680443159405, Loss_test: 0.08310904668577955\n",
      "4954 - Loss_train: 0.08377640905405223, Loss_test: 0.08310877062859488\n",
      "4955 - Loss_train: 0.08377601388214609, Loss_test: 0.0831084947447443\n",
      "4956 - Loss_train: 0.08377561891622415, Loss_test: 0.08310821903523649\n",
      "4957 - Loss_train: 0.08377522415511644, Loss_test: 0.08310794350227611\n",
      "4958 - Loss_train: 0.08377482959931223, Loss_test: 0.08310766814075247\n",
      "4959 - Loss_train: 0.08377443524872778, Loss_test: 0.08310739295537417\n",
      "4960 - Loss_train: 0.08377404110317124, Loss_test: 0.08310711794460128\n",
      "4961 - Loss_train: 0.08377364716304253, Loss_test: 0.08310684310818545\n",
      "4962 - Loss_train: 0.08377325342726563, Loss_test: 0.08310656844443072\n",
      "4963 - Loss_train: 0.08377285989636184, Loss_test: 0.08310629395491026\n",
      "4964 - Loss_train: 0.08377246656995471, Loss_test: 0.08310601964005065\n",
      "4965 - Loss_train: 0.08377207344852654, Loss_test: 0.08310574550039929\n",
      "4966 - Loss_train: 0.08377168053168842, Loss_test: 0.08310547153317271\n",
      "4967 - Loss_train: 0.08377128781900914, Loss_test: 0.08310519773707722\n",
      "4968 - Loss_train: 0.08377089531014621, Loss_test: 0.08310492411639891\n",
      "4969 - Loss_train: 0.08377050300522819, Loss_test: 0.08310465066900455\n",
      "4970 - Loss_train: 0.0837701109045051, Loss_test: 0.08310437739567793\n",
      "4971 - Loss_train: 0.08376971900815432, Loss_test: 0.08310410429375947\n",
      "4972 - Loss_train: 0.0837693273151193, Loss_test: 0.08310383136734954\n",
      "4973 - Loss_train: 0.08376893582562836, Loss_test: 0.0831035586132318\n",
      "4974 - Loss_train: 0.08376854453968426, Loss_test: 0.08310328603072203\n",
      "4975 - Loss_train: 0.08376815345734474, Loss_test: 0.08310301362021132\n",
      "4976 - Loss_train: 0.08376776257880858, Loss_test: 0.08310274138583341\n",
      "4977 - Loss_train: 0.08376737190304406, Loss_test: 0.08310246932165695\n",
      "4978 - Loss_train: 0.08376698143046478, Loss_test: 0.08310219743082847\n",
      "4979 - Loss_train: 0.08376659116141298, Loss_test: 0.08310192571143726\n",
      "4980 - Loss_train: 0.08376620109485024, Loss_test: 0.0831016541666533\n",
      "4981 - Loss_train: 0.08376581123119534, Loss_test: 0.08310138279308525\n",
      "4982 - Loss_train: 0.08376542157090053, Loss_test: 0.08310111158793144\n",
      "4983 - Loss_train: 0.08376503211257562, Loss_test: 0.08310084056020375\n",
      "4984 - Loss_train: 0.08376464285671163, Loss_test: 0.08310056970099572\n",
      "4985 - Loss_train: 0.08376425380323863, Loss_test: 0.08310029901485226\n",
      "4986 - Loss_train: 0.08376386495210217, Loss_test: 0.08310002850097654\n",
      "4987 - Loss_train: 0.08376347630307193, Loss_test: 0.08309975815590277\n",
      "4988 - Loss_train: 0.08376308785631931, Loss_test: 0.08309948798487168\n",
      "4989 - Loss_train: 0.08376269961137095, Loss_test: 0.0830992179855209\n",
      "4990 - Loss_train: 0.08376231156899935, Loss_test: 0.08309894815661575\n",
      "4991 - Loss_train: 0.08376192372841879, Loss_test: 0.08309867849981126\n",
      "4992 - Loss_train: 0.08376153608946096, Loss_test: 0.08309840901235425\n",
      "4993 - Loss_train: 0.08376114865167626, Loss_test: 0.0830981396983163\n",
      "4994 - Loss_train: 0.08376076141579915, Loss_test: 0.08309787055533481\n",
      "4995 - Loss_train: 0.08376037438066111, Loss_test: 0.08309760158148737\n",
      "4996 - Loss_train: 0.0837599875475005, Loss_test: 0.08309733277907858\n",
      "4997 - Loss_train: 0.08375960091523262, Loss_test: 0.08309706414877055\n",
      "4998 - Loss_train: 0.0837592144836744, Loss_test: 0.08309679568505281\n",
      "4999 - Loss_train: 0.08375882825301191, Loss_test: 0.08309652739497961\n"
     ]
    }
   ],
   "source": [
    "W = fit(X, y,X_test, y_test, 5000, 0.00001, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d773cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08247027173989448"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_perfect  = 10 * X_test.reshape(-1) + 5\n",
    "# y_test\n",
    "np.mean((y_perfect - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e00305b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.01650666, 1.37502072],\n",
       "       [3.31511209, 0.42720289]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1cbc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = predict(X_test, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "63f83f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01121403, -0.08675618,  0.35201396,  0.2226306 ,  0.10616688,\n",
       "        0.06340828,  0.4485908 ,  0.49043686, -0.15241439, -0.25670109,\n",
       "        0.30060644, -0.43607872, -0.12938147, -0.42512792, -0.17008284,\n",
       "       -0.4201537 , -0.20807644,  0.29718975,  0.41296397,  0.29353483,\n",
       "        0.06992523,  0.12337845, -0.13488865, -0.33006568, -0.05965624,\n",
       "        0.23760675, -0.42819155, -0.47082121,  0.27370086, -0.19669162,\n",
       "        0.21137443,  0.2410099 ,  0.44453982, -0.09258294, -0.13759496,\n",
       "        0.3116875 ,  0.26465809,  0.16173915,  0.31432295,  0.14686056,\n",
       "        0.46331122, -0.15815478,  0.2494128 ,  0.45027173, -0.16404381,\n",
       "        0.11987423, -0.13524859, -0.02743525, -0.4210309 , -0.46902144])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a675565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_test, y_test, y_predict are defined\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='y_test')\n",
    "plt.scatter(X_test, y_predict, color='red', alpha=0.6, label='y_predict')\n",
    "\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Scatter: True vs. Predicted')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f57edd",
   "metadata": {},
   "source": [
    "Question1:\n",
    "\n",
    "Generate non linear data  - Just like tax slabs. Introduce activation function in predict and then train the model and plot the values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04965a91",
   "metadata": {},
   "source": [
    "Question2:\n",
    "\n",
    "Create a new neural network with n1 neuron in first and n2 neurons in second.\n",
    "Train and Test on non-linear data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dff75",
   "metadata": {},
   "source": [
    "Question3:\n",
    "\n",
    "Think about how would you optimize the computation of gradients using back propagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
